{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5754,"status":"ok","timestamp":1709126179805,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"},"user_tz":-300},"id":"y7wOUNWd5n1w","outputId":"f16dd077-ba07-46b4-f436-48e4e77fe659"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'R2GenCMN'...\n","remote: Enumerating objects: 88, done.\u001b[K\n","remote: Counting objects: 100% (34/34), done.\u001b[K\n","remote: Compressing objects: 100% (24/24), done.\u001b[K\n","remote: Total 88 (delta 14), reused 23 (delta 8), pack-reused 54\u001b[K\n","Receiving objects: 100% (88/88), 70.19 MiB | 15.55 MiB/s, done.\n","Resolving deltas: 100% (19/19), done.\n"]}],"source":["!git clone \"https://github.com/zhjohnchan/R2GenCMN.git\""]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33287,"status":"ok","timestamp":1709126230247,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"},"user_tz":-300},"id":"_TjUW_gjYGbc","outputId":"2126983e-dad2-4f59-b669-bebc786e95db"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"qvJEaV5aAp68","executionInfo":{"status":"ok","timestamp":1709126249483,"user_tz":-300,"elapsed":5037,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"}}},"outputs":[],"source":["# utils.py\n","\n","import numpy as np\n","import cv2\n","import torch\n","\n","\n","def penalty_builder(penalty_config):\n","    if penalty_config == '':\n","        return lambda x, y: y\n","    pen_type, alpha = penalty_config.split('_')\n","    alpha = float(alpha)\n","    if pen_type == 'wu':\n","        return lambda x, y: length_wu(x, y, alpha)\n","    if pen_type == 'avg':\n","        return lambda x, y: length_average(x, y, alpha)\n","\n","\n","def length_wu(length, logprobs, alpha=0.):\n","    \"\"\"\n","    NMT length re-ranking score from\n","    \"Google's Neural Machine Translation System\" :cite:`wu2016google`.\n","    \"\"\"\n","\n","    modifier = (((5 + length) ** alpha) /\n","                ((5 + 1) ** alpha))\n","    return logprobs / modifier\n","\n","\n","def length_average(length, logprobs, alpha=0.):\n","    \"\"\"\n","    Returns the average probability of tokens in a sequence.\n","    \"\"\"\n","    return logprobs / length\n","\n","\n","def split_tensors(n, x):\n","    if torch.is_tensor(x):\n","        assert x.shape[0] % n == 0\n","        x = x.reshape(x.shape[0] // n, n, *x.shape[1:]).unbind(1)\n","    elif type(x) is list or type(x) is tuple:\n","        x = [split_tensors(n, _) for _ in x]\n","    elif x is None:\n","        x = [None] * n\n","    return x\n","\n","\n","def repeat_tensors(n, x):\n","    \"\"\"\n","    For a tensor of size Bx..., we repeat it n times, and make it Bnx...\n","    For collections, do nested repeat\n","    \"\"\"\n","    if torch.is_tensor(x):\n","        x = x.unsqueeze(1)  # Bx1x...\n","        x = x.expand(-1, n, *([-1] * len(x.shape[2:])))  # Bxnx...\n","        x = x.reshape(x.shape[0] * n, *x.shape[2:])  # Bnx...\n","    elif type(x) is list or type(x) is tuple:\n","        x = [repeat_tensors(n, _) for _ in x]\n","    return x\n","\n","\n","def generate_heatmap(image, weights):\n","    image = image.transpose(1, 2, 0)\n","    height, width, _ = image.shape\n","    weights = weights.reshape(int(weights.shape[0] ** 0.5), int(weights.shape[0] ** 0.5))\n","    weights = weights - np.min(weights)\n","    weights = weights / np.max(weights)\n","    weights = cv2.resize(weights, (width, height))\n","    weights = np.uint8(255 * weights)\n","    heatmap = cv2.applyColorMap(weights, cv2.COLORMAP_JET)\n","    result = heatmap * 0.5 + image * 0.5\n","    return result"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"E4CI2IfwAkOa","executionInfo":{"status":"ok","timestamp":1709126249484,"user_tz":-300,"elapsed":5,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"}}},"outputs":[],"source":["# caption_model\n","\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# import modules.utils as utils\n","\n","\n","class CaptionModel(nn.Module):\n","    def __init__(self):\n","        super(CaptionModel, self).__init__()\n","\n","    # implements beam search\n","    # calls beam_step and returns the final set of beams\n","    # augments log-probabilities with diversity terms when number of groups > 1\n","\n","    def forward(self, *args, **kwargs):\n","        mode = kwargs.get('mode', 'forward')\n","        if 'mode' in kwargs:\n","            del kwargs['mode']\n","        return getattr(self, '_' + mode)(*args, **kwargs)\n","\n","    def beam_search(self, init_state, init_logprobs, *args, **kwargs):\n","\n","        # function computes the similarity score to be augmented\n","        def add_diversity(beam_seq_table, logprobs, t, divm, diversity_lambda, bdash):\n","            local_time = t - divm\n","            unaug_logprobs = logprobs.clone()\n","            batch_size = beam_seq_table[0].shape[0]\n","\n","            if divm > 0:\n","                change = logprobs.new_zeros(batch_size, logprobs.shape[-1])\n","                for prev_choice in range(divm):\n","                    prev_decisions = beam_seq_table[prev_choice][:, :, local_time]  # Nxb\n","                    for prev_labels in range(bdash):\n","                        change.scatter_add_(1, prev_decisions[:, prev_labels].unsqueeze(-1),\n","                                            change.new_ones(batch_size, 1))\n","\n","                if local_time == 0:\n","                    logprobs = logprobs - change * diversity_lambda\n","                else:\n","                    logprobs = logprobs - self.repeat_tensor(bdash, change) * diversity_lambda\n","\n","            return logprobs, unaug_logprobs\n","\n","        # does one step of classical beam search\n","\n","        def beam_step(logprobs, unaug_logprobs, beam_size, t, beam_seq, beam_seq_logprobs, beam_logprobs_sum, state):\n","            # INPUTS:\n","            # logprobs: probabilities augmented after diversity N*bxV\n","            # beam_size: obvious\n","            # t        : time instant\n","            # beam_seq : tensor contanining the beams\n","            # beam_seq_logprobs: tensor contanining the beam logprobs\n","            # beam_logprobs_sum: tensor contanining joint logprobs\n","            # OUPUTS:\n","            # beam_seq : tensor containing the word indices of the decoded captions Nxbxl\n","            # beam_seq_logprobs : log-probability of each decision made, NxbxlxV\n","            # beam_logprobs_sum : joint log-probability of each beam Nxb\n","\n","            batch_size = beam_logprobs_sum.shape[0]\n","            vocab_size = logprobs.shape[-1]\n","            logprobs = logprobs.reshape(batch_size, -1, vocab_size)  # NxbxV\n","            if t == 0:\n","                assert logprobs.shape[1] == 1\n","                beam_logprobs_sum = beam_logprobs_sum[:, :1]\n","            candidate_logprobs = beam_logprobs_sum.unsqueeze(-1) + logprobs  # beam_logprobs_sum Nxb logprobs is NxbxV\n","            ys, ix = torch.sort(candidate_logprobs.reshape(candidate_logprobs.shape[0], -1), -1, True)\n","            ys, ix = ys[:, :beam_size], ix[:, :beam_size]\n","            beam_ix = ix // vocab_size  # Nxb which beam\n","            selected_ix = ix % vocab_size  # Nxb # which world\n","            state_ix = (beam_ix + torch.arange(batch_size).type_as(beam_ix).unsqueeze(-1) * logprobs.shape[1]).reshape(\n","                -1)  # N*b which in Nxb beams\n","\n","            if t > 0:\n","                # gather according to beam_ix\n","                assert (beam_seq.gather(1, beam_ix.unsqueeze(-1).expand_as(beam_seq)) ==\n","                        beam_seq.reshape(-1, beam_seq.shape[-1])[state_ix].view_as(beam_seq)).all()\n","                beam_seq = beam_seq.gather(1, beam_ix.unsqueeze(-1).expand_as(beam_seq))\n","\n","                beam_seq_logprobs = beam_seq_logprobs.gather(1, beam_ix.unsqueeze(-1).unsqueeze(-1).expand_as(\n","                    beam_seq_logprobs))\n","\n","            beam_seq = torch.cat([beam_seq, selected_ix.unsqueeze(-1)], -1)  # beam_seq Nxbxl\n","            beam_logprobs_sum = beam_logprobs_sum.gather(1, beam_ix) + \\\n","                                logprobs.reshape(batch_size, -1).gather(1, ix)\n","            assert (beam_logprobs_sum == ys).all()\n","            _tmp_beam_logprobs = unaug_logprobs[state_ix].reshape(batch_size, -1, vocab_size)\n","            beam_logprobs = unaug_logprobs.reshape(batch_size, -1, vocab_size).gather(1,\n","                                                                                      beam_ix.unsqueeze(-1).expand(-1,\n","                                                                                                                   -1,\n","                                                                                                                   vocab_size))  # NxbxV\n","            assert (_tmp_beam_logprobs == beam_logprobs).all()\n","            beam_seq_logprobs = torch.cat([\n","                beam_seq_logprobs,\n","                beam_logprobs.reshape(batch_size, -1, 1, vocab_size)], 2)\n","\n","            new_state = [None for _ in state]\n","            for _ix in range(len(new_state)):\n","                #  copy over state in previous beam q to new beam at vix\n","                new_state[_ix] = state[_ix][:, state_ix]\n","            state = new_state\n","            return beam_seq, beam_seq_logprobs, beam_logprobs_sum, state\n","\n","        # Start diverse_beam_search\n","        opt = kwargs['opt']\n","        temperature = opt.get('temperature', 1)  # This should not affect beam search, but will affect dbs\n","        beam_size = opt.get('beam_size', 10)\n","        group_size = opt.get('group_size', 1)\n","        diversity_lambda = opt.get('diversity_lambda', 0.5)\n","        decoding_constraint = opt.get('decoding_constraint', 0)\n","        suppress_UNK = opt.get('suppress_UNK', 0)\n","        length_penalty = penalty_builder(opt.get('length_penalty', ''))\n","        bdash = beam_size // group_size  # beam per group\n","\n","        batch_size = init_logprobs.shape[0]\n","        device = init_logprobs.device\n","        # INITIALIZATIONS\n","        beam_seq_table = [torch.LongTensor(batch_size, bdash, 0).to(device) for _ in range(group_size)]\n","        beam_seq_logprobs_table = [torch.FloatTensor(batch_size, bdash, 0, self.vocab_size + 1).to(device) for _ in\n","                                   range(group_size)]\n","        beam_logprobs_sum_table = [torch.zeros(batch_size, bdash).to(device) for _ in range(group_size)]\n","\n","        # logprobs # logprobs predicted in last time step, shape (beam_size, vocab_size+1)\n","        done_beams_table = [[[] for __ in range(group_size)] for _ in range(batch_size)]\n","        state_table = [[_.clone() for _ in init_state] for _ in range(group_size)]\n","        logprobs_table = [init_logprobs.clone() for _ in range(group_size)]\n","        # END INIT\n","\n","        # Chunk elements in the args\n","        args = list(args)\n","        args = split_tensors(group_size, args)  # For each arg, turn (Bbg)x... to (Bb)x(g)x...\n","        if self.__class__.__name__ == 'AttEnsemble':\n","            args = [[[args[j][i][k] for i in range(len(self.models))] for j in range(len(args))] for k in\n","                    range(group_size)]  # group_name, arg_name, model_name\n","        else:\n","            args = [[args[i][j] for i in range(len(args))] for j in range(group_size)]\n","\n","        for t in range(self.max_seq_length + group_size - 1):\n","            for divm in range(group_size):\n","                if t >= divm and t <= self.max_seq_length + divm - 1:\n","                    # add diversity\n","                    logprobs = logprobs_table[divm]\n","                    # suppress previous word\n","                    if decoding_constraint and t - divm > 0:\n","                        logprobs.scatter_(1, beam_seq_table[divm][:, :, t - divm - 1].reshape(-1, 1).to(device),\n","                                          float('-inf'))\n","                    # suppress UNK tokens in the decoding\n","                    if suppress_UNK and hasattr(self, 'vocab') and self.vocab[str(logprobs.size(1) - 1)] == 'UNK':\n","                        logprobs[:, logprobs.size(1) - 1] = logprobs[:, logprobs.size(1) - 1] - 1000\n","                        # diversity is added here\n","                    # the function directly modifies the logprobs values and hence, we need to return\n","                    # the unaugmented ones for sorting the candidates in the end. # for historical\n","                    # reasons :-)\n","                    logprobs, unaug_logprobs = add_diversity(beam_seq_table, logprobs, t, divm, diversity_lambda, bdash)\n","\n","                    # infer new beams\n","                    beam_seq_table[divm], \\\n","                    beam_seq_logprobs_table[divm], \\\n","                    beam_logprobs_sum_table[divm], \\\n","                    state_table[divm] = beam_step(logprobs,\n","                                                  unaug_logprobs,\n","                                                  bdash,\n","                                                  t - divm,\n","                                                  beam_seq_table[divm],\n","                                                  beam_seq_logprobs_table[divm],\n","                                                  beam_logprobs_sum_table[divm],\n","                                                  state_table[divm])\n","\n","                    # if time's up... or if end token is reached then copy beams\n","                    for b in range(batch_size):\n","                        is_end = beam_seq_table[divm][b, :, t - divm] == self.eos_idx\n","                        assert beam_seq_table[divm].shape[-1] == t - divm + 1\n","                        if t == self.max_seq_length + divm - 1:\n","                            is_end.fill_(1)\n","                        for vix in range(bdash):\n","                            if is_end[vix]:\n","                                final_beam = {\n","                                    'seq': beam_seq_table[divm][b, vix].clone(),\n","                                    'logps': beam_seq_logprobs_table[divm][b, vix].clone(),\n","                                    'unaug_p': beam_seq_logprobs_table[divm][b, vix].sum().item(),\n","                                    'p': beam_logprobs_sum_table[divm][b, vix].item()\n","                                }\n","                                final_beam['p'] = length_penalty(t - divm + 1, final_beam['p'])\n","                                done_beams_table[b][divm].append(final_beam)\n","                        beam_logprobs_sum_table[divm][b, is_end] -= 1000\n","\n","                    # move the current group one step forward in time\n","\n","                    it = beam_seq_table[divm][:, :, t - divm].reshape(-1)\n","                    logprobs_table[divm], state_table[divm] = self.get_logprobs_state(it.cuda(), *(\n","                            args[divm] + [state_table[divm]]))\n","                    logprobs_table[divm] = F.log_softmax(logprobs_table[divm] / temperature, dim=-1)\n","\n","        # all beams are sorted by their log-probabilities\n","        done_beams_table = [[sorted(done_beams_table[b][i], key=lambda x: -x['p'])[:bdash] for i in range(group_size)]\n","                            for b in range(batch_size)]\n","        done_beams = [sum(_, []) for _ in done_beams_table]\n","        return done_beams\n","\n","    def old_beam_search(self, init_state, init_logprobs, *args, **kwargs):\n","\n","        # function computes the similarity score to be augmented\n","        def add_diversity(beam_seq_table, logprobsf, t, divm, diversity_lambda, bdash):\n","            local_time = t - divm\n","            unaug_logprobsf = logprobsf.clone()\n","            for prev_choice in range(divm):\n","                prev_decisions = beam_seq_table[prev_choice][local_time]\n","                for sub_beam in range(bdash):\n","                    for prev_labels in range(bdash):\n","                        logprobsf[sub_beam][prev_decisions[prev_labels]] = logprobsf[sub_beam][prev_decisions[\n","                            prev_labels]] - diversity_lambda\n","            return unaug_logprobsf\n","\n","        # does one step of classical beam search\n","\n","        def beam_step(logprobsf, unaug_logprobsf, beam_size, t, beam_seq, beam_seq_logprobs, beam_logprobs_sum, state):\n","            # INPUTS:\n","            # logprobsf: probabilities augmented after diversity\n","            # beam_size: obvious\n","            # t        : time instant\n","            # beam_seq : tensor contanining the beams\n","            # beam_seq_logprobs: tensor contanining the beam logprobs\n","            # beam_logprobs_sum: tensor contanining joint logprobs\n","            # OUPUTS:\n","            # beam_seq : tensor containing the word indices of the decoded captions\n","            # beam_seq_logprobs : log-probability of each decision made, same size as beam_seq\n","            # beam_logprobs_sum : joint log-probability of each beam\n","\n","            ys, ix = torch.sort(logprobsf, 1, True)\n","            candidates = []\n","            cols = min(beam_size, ys.size(1))\n","            rows = beam_size\n","            if t == 0:\n","                rows = 1\n","            for c in range(cols):  # for each column (word, essentially)\n","                for q in range(rows):  # for each beam expansion\n","                    # compute logprob of expanding beam q with word in (sorted) position c\n","                    local_logprob = ys[q, c].item()\n","                    candidate_logprob = beam_logprobs_sum[q] + local_logprob\n","                    # local_unaug_logprob = unaug_logprobsf[q,ix[q,c]]\n","                    candidates.append({'c': ix[q, c], 'q': q, 'p': candidate_logprob, 'r': unaug_logprobsf[q]})\n","            candidates = sorted(candidates, key=lambda x: -x['p'])\n","\n","            new_state = [_.clone() for _ in state]\n","            # beam_seq_prev, beam_seq_logprobs_prev\n","            if t >= 1:\n","                # we''ll need these as reference when we fork beams around\n","                beam_seq_prev = beam_seq[:t].clone()\n","                beam_seq_logprobs_prev = beam_seq_logprobs[:t].clone()\n","            for vix in range(beam_size):\n","                v = candidates[vix]\n","                # fork beam index q into index vix\n","                if t >= 1:\n","                    beam_seq[:t, vix] = beam_seq_prev[:, v['q']]\n","                    beam_seq_logprobs[:t, vix] = beam_seq_logprobs_prev[:, v['q']]\n","                # rearrange recurrent states\n","                for state_ix in range(len(new_state)):\n","                    #  copy over state in previous beam q to new beam at vix\n","                    new_state[state_ix][:, vix] = state[state_ix][:, v['q']]  # dimension one is time step\n","                # append new end terminal at the end of this beam\n","                beam_seq[t, vix] = v['c']  # c'th word is the continuation\n","                beam_seq_logprobs[t, vix] = v['r']  # the raw logprob here\n","                beam_logprobs_sum[vix] = v['p']  # the new (sum) logprob along this beam\n","            state = new_state\n","            return beam_seq, beam_seq_logprobs, beam_logprobs_sum, state, candidates\n","\n","        # Start diverse_beam_search\n","        opt = kwargs['opt']\n","        temperature = opt.get('temperature', 1)  # This should not affect beam search, but will affect dbs\n","        beam_size = opt.get('beam_size', 10)\n","        group_size = opt.get('group_size', 1)\n","        diversity_lambda = opt.get('diversity_lambda', 0.5)\n","        decoding_constraint = opt.get('decoding_constraint', 0)\n","        suppress_UNK = opt.get('suppress_UNK', 0)\n","        length_penalty = penalty_builder(opt.get('length_penalty', ''))\n","        bdash = beam_size // group_size  # beam per group\n","\n","        # INITIALIZATIONS\n","        beam_seq_table = [torch.LongTensor(self.max_seq_length, bdash).zero_() for _ in range(group_size)]\n","        beam_seq_logprobs_table = [torch.FloatTensor(self.max_seq_length, bdash, self.vocab_size + 1).zero_() for _ in\n","                                   range(group_size)]\n","        beam_logprobs_sum_table = [torch.zeros(bdash) for _ in range(group_size)]\n","\n","        # logprobs # logprobs predicted in last time step, shape (beam_size, vocab_size+1)\n","        done_beams_table = [[] for _ in range(group_size)]\n","        # state_table = [list(torch.unbind(_)) for _ in torch.stack(init_state).chunk(group_size, 2)]\n","        state_table = list(zip(*[_.chunk(group_size, 1) for _ in init_state]))\n","        logprobs_table = list(init_logprobs.chunk(group_size, 0))\n","        # END INIT\n","\n","        # Chunk elements in the args\n","        args = list(args)\n","        if self.__class__.__name__ == 'AttEnsemble':\n","            args = [[_.chunk(group_size) if _ is not None else [None] * group_size for _ in args_] for args_ in\n","                    args]  # arg_name, model_name, group_name\n","            args = [[[args[j][i][k] for i in range(len(self.models))] for j in range(len(args))] for k in\n","                    range(group_size)]  # group_name, arg_name, model_name\n","        else:\n","            args = [_.chunk(group_size) if _ is not None else [None] * group_size for _ in args]\n","            args = [[args[i][j] for i in range(len(args))] for j in range(group_size)]\n","\n","        for t in range(self.max_seq_length + group_size - 1):\n","            for divm in range(group_size):\n","                if t >= divm and t <= self.max_seq_length + divm - 1:\n","                    # add diversity\n","                    logprobsf = logprobs_table[divm].float()\n","                    # suppress previous word\n","                    if decoding_constraint and t - divm > 0:\n","                        logprobsf.scatter_(1, beam_seq_table[divm][t - divm - 1].unsqueeze(1).cuda(), float('-inf'))\n","                    # suppress UNK tokens in the decoding\n","                    if suppress_UNK and hasattr(self, 'vocab') and self.vocab[str(logprobsf.size(1) - 1)] == 'UNK':\n","                        logprobsf[:, logprobsf.size(1) - 1] = logprobsf[:, logprobsf.size(1) - 1] - 1000\n","                        # diversity is added here\n","                    # the function directly modifies the logprobsf values and hence, we need to return\n","                    # the unaugmented ones for sorting the candidates in the end. # for historical\n","                    # reasons :-)\n","                    unaug_logprobsf = add_diversity(beam_seq_table, logprobsf, t, divm, diversity_lambda, bdash)\n","\n","                    # infer new beams\n","                    beam_seq_table[divm], \\\n","                    beam_seq_logprobs_table[divm], \\\n","                    beam_logprobs_sum_table[divm], \\\n","                    state_table[divm], \\\n","                    candidates_divm = beam_step(logprobsf,\n","                                                unaug_logprobsf,\n","                                                bdash,\n","                                                t - divm,\n","                                                beam_seq_table[divm],\n","                                                beam_seq_logprobs_table[divm],\n","                                                beam_logprobs_sum_table[divm],\n","                                                state_table[divm])\n","\n","                    # if time's up... or if end token is reached then copy beams\n","                    for vix in range(bdash):\n","                        if beam_seq_table[divm][t - divm, vix] == self.eos_idx or t == self.max_seq_length + divm - 1:\n","                            final_beam = {\n","                                'seq': beam_seq_table[divm][:, vix].clone(),\n","                                'logps': beam_seq_logprobs_table[divm][:, vix].clone(),\n","                                'unaug_p': beam_seq_logprobs_table[divm][:, vix].sum().item(),\n","                                'p': beam_logprobs_sum_table[divm][vix].item()\n","                            }\n","                            final_beam['p'] = length_penalty(t - divm + 1, final_beam['p'])\n","                            done_beams_table[divm].append(final_beam)\n","                            # don't continue beams from finished sequences\n","                            beam_logprobs_sum_table[divm][vix] = -1000\n","\n","                    # move the current group one step forward in time\n","\n","                    it = beam_seq_table[divm][t - divm]\n","                    logprobs_table[divm], state_table[divm] = self.get_logprobs_state(it.cuda(), *(\n","                            args[divm] + [state_table[divm]]))\n","                    logprobs_table[divm] = F.log_softmax(logprobs_table[divm] / temperature, dim=-1)\n","\n","        # all beams are sorted by their log-probabilities\n","        done_beams_table = [sorted(done_beams_table[i], key=lambda x: -x['p'])[:bdash] for i in range(group_size)]\n","        done_beams = sum(done_beams_table, [])\n","        return done_beams\n","\n","    def sample_next_word(self, logprobs, sample_method, temperature):\n","        if sample_method == 'greedy':\n","            sampleLogprobs, it = torch.max(logprobs.data, 1)\n","            it = it.view(-1).long()\n","        elif sample_method == 'gumbel':  # gumbel softmax\n","            def sample_gumbel(shape, eps=1e-20):\n","                U = torch.rand(shape).cuda()\n","                return -torch.log(-torch.log(U + eps) + eps)\n","\n","            def gumbel_softmax_sample(logits, temperature):\n","                y = logits + sample_gumbel(logits.size())\n","                return F.log_softmax(y / temperature, dim=-1)\n","\n","            _logprobs = gumbel_softmax_sample(logprobs, temperature)\n","            _, it = torch.max(_logprobs.data, 1)\n","            sampleLogprobs = logprobs.gather(1, it.unsqueeze(1))  # gather the logprobs at sampled positions\n","        else:\n","            logprobs = logprobs / temperature\n","            if sample_method.startswith('top'):  # topk sampling\n","                top_num = float(sample_method[3:])\n","                if 0 < top_num < 1:\n","                    # nucleus sampling from # The Curious Case of Neural Text Degeneration\n","                    probs = F.softmax(logprobs, dim=1)\n","                    sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=1)\n","                    _cumsum = sorted_probs.cumsum(1)\n","                    mask = _cumsum < top_num\n","                    mask = torch.cat([torch.ones_like(mask[:, :1]), mask[:, :-1]], 1)\n","                    sorted_probs = sorted_probs * mask.float()\n","                    sorted_probs = sorted_probs / sorted_probs.sum(1, keepdim=True)\n","                    logprobs.scatter_(1, sorted_indices, sorted_probs.log())\n","                else:\n","                    the_k = int(top_num)\n","                    tmp = torch.empty_like(logprobs).fill_(float('-inf'))\n","                    topk, indices = torch.topk(logprobs, the_k, dim=1)\n","                    tmp = tmp.scatter(1, indices, topk)\n","                    logprobs = tmp\n","            it = torch.distributions.Categorical(logits=logprobs.detach()).sample()\n","            sampleLogprobs = logprobs.gather(1, it.unsqueeze(1))  # gather the logprobs at sampled positions\n","        return it, sampleLogprobs\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"LxFdy7YRARQm","executionInfo":{"status":"ok","timestamp":1709126249484,"user_tz":-300,"elapsed":4,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"}}},"outputs":[],"source":["# att_model\n","\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import PackedSequence, pack_padded_sequence, pad_packed_sequence\n","\n","# import modules.utils as utils\n","# from modules.caption_model import CaptionModel\n","\n","\n","def sort_pack_padded_sequence(input, lengths):\n","    sorted_lengths, indices = torch.sort(lengths, descending=True)\n","    tmp = pack_padded_sequence(input[indices], sorted_lengths.cpu(), batch_first=True)\n","    inv_ix = indices.clone()\n","    inv_ix[indices] = torch.arange(0, len(indices)).type_as(inv_ix)\n","    return tmp, inv_ix\n","\n","\n","def pad_unsort_packed_sequence(input, inv_ix):\n","    tmp, _ = pad_packed_sequence(input, batch_first=True)\n","    tmp = tmp[inv_ix]\n","    return tmp\n","\n","\n","def pack_wrapper(module, att_feats, att_masks):\n","    if att_masks is not None:\n","        packed, inv_ix = sort_pack_padded_sequence(att_feats, att_masks.data.long().sum(1))\n","        return pad_unsort_packed_sequence(PackedSequence(module(packed[0]), packed[1]), inv_ix)\n","    else:\n","        return module(att_feats)\n","\n","\n","class AttModel(CaptionModel):\n","    def __init__(self, args, tokenizer):\n","        super(AttModel, self).__init__()\n","        self.args = args\n","        self.tokenizer = tokenizer\n","        self.vocab_size = len(tokenizer.idx2token)\n","        self.input_encoding_size = args['d_model']\n","        self.rnn_size = args['d_ff']\n","        self.num_layers = args['num_layers']\n","        self.drop_prob_lm = args['drop_prob_lm']\n","        self.max_seq_length = args['max_seq_length']\n","        self.att_feat_size = args['d_vf']\n","        self.att_hid_size = args['d_model']\n","\n","        self.bos_idx = args['bos_idx']\n","        self.eos_idx = args['eos_idx']\n","        self.pad_idx = args['pad_idx']\n","\n","        self.use_bn = args['use_bn']\n","\n","        self.embed = lambda x: x\n","        self.fc_embed = lambda x: x\n","        self.att_embed = nn.Sequential(*(\n","                ((nn.BatchNorm1d(self.att_feat_size),) if self.use_bn else ()) +\n","                (nn.Linear(self.att_feat_size, self.input_encoding_size),\n","                 nn.ReLU(),\n","                 nn.Dropout(self.drop_prob_lm)) +\n","                ((nn.BatchNorm1d(self.input_encoding_size),) if self.use_bn == 2 else ())))\n","\n","    def clip_att(self, att_feats, att_masks):\n","        # Clip the length of att_masks and att_feats to the maximum length\n","        if att_masks is not None:\n","            max_len = att_masks.data.long().sum(1).max()\n","            att_feats = att_feats[:, :max_len].contiguous()\n","            att_masks = att_masks[:, :max_len].contiguous()\n","        return att_feats, att_masks\n","\n","    def _prepare_feature(self, fc_feats, att_feats, att_masks):\n","        att_feats, att_masks = self.clip_att(att_feats, att_masks)\n","\n","        # embed fc and att feats\n","        fc_feats = self.fc_embed(fc_feats)\n","        att_feats = pack_wrapper(self.att_embed, att_feats, att_masks)\n","\n","        # Project the attention feats first to reduce memory and computation comsumptions.\n","        p_att_feats = self.ctx2att(att_feats)\n","\n","        return fc_feats, att_feats, p_att_feats, att_masks\n","\n","    def get_logprobs_state(self, it, fc_feats, att_feats, p_att_feats, att_masks, state, output_logsoftmax=1):\n","        # 'it' contains a word index\n","        xt = self.embed(it)\n","\n","        output, state = self.core(xt, fc_feats, att_feats, p_att_feats, state, att_masks)\n","        if output_logsoftmax:\n","            logprobs = F.log_softmax(self.logit(output), dim=1)\n","        else:\n","            logprobs = self.logit(output)\n","\n","        return logprobs, state\n","\n","    def _sample_beam(self, fc_feats, att_feats, att_masks=None, opt={}):\n","        beam_size = opt.get('beam_size', 10)\n","        group_size = opt.get('group_size', 1)\n","        sample_n = opt.get('sample_n', 10)\n","        # when sample_n == beam_size then each beam is a sample.\n","        assert sample_n == 1 or sample_n == beam_size // group_size, 'when beam search, sample_n == 1 or beam search'\n","        batch_size = fc_feats.size(0)\n","\n","        p_fc_feats, p_att_feats, pp_att_feats, p_att_masks = self._prepare_feature(fc_feats, att_feats, att_masks)\n","\n","        assert beam_size <= self.vocab_size + 1, 'lets assume this for now, otherwise this corner case causes a few headaches down the road. can be dealt with in future if needed'\n","        seq = fc_feats.new_full((batch_size * sample_n, self.max_seq_length), self.pad_idx, dtype=torch.long)\n","        seqLogprobs = fc_feats.new_zeros(batch_size * sample_n, self.max_seq_length, self.vocab_size + 1)\n","        # lets process every image independently for now, for simplicity\n","\n","        self.done_beams = [[] for _ in range(batch_size)]\n","\n","        state = self.init_hidden(batch_size)\n","\n","        # first step, feed bos\n","        it = fc_feats.new_full([batch_size], self.bos_idx, dtype=torch.long)\n","        logprobs, state = self.get_logprobs_state(it, p_fc_feats, p_att_feats, pp_att_feats, p_att_masks, state)\n","\n","        p_fc_feats, p_att_feats, pp_att_feats, p_att_masks =repeat_tensors(beam_size,\n","                                                                                  [p_fc_feats, p_att_feats,\n","                                                                                   pp_att_feats, p_att_masks]\n","                                                                                  )\n","        self.done_beams = self.beam_search(state, logprobs, p_fc_feats, p_att_feats, pp_att_feats, p_att_masks, opt=opt)\n","        for k in range(batch_size):\n","            if sample_n == beam_size:\n","                for _n in range(sample_n):\n","                    seq_len = self.done_beams[k][_n]['seq'].shape[0]\n","                    seq[k * sample_n + _n, :seq_len] = self.done_beams[k][_n]['seq']\n","                    seqLogprobs[k * sample_n + _n, :seq_len] = self.done_beams[k][_n]['logps']\n","            else:\n","                seq_len = self.done_beams[k][0]['seq'].shape[0]\n","                seq[k, :seq_len] = self.done_beams[k][0]['seq']  # the first beam has highest cumulative score\n","                seqLogprobs[k, :seq_len] = self.done_beams[k][0]['logps']\n","        # return the samples and their log likelihoods\n","        return seq, seqLogprobs\n","\n","    def _sample(self, fc_feats, att_feats, att_masks=None, update_opts={}):\n","        opt = self.args\n","        opt.update(**update_opts)\n","\n","        sample_method = opt.get('sample_method', 'greedy')\n","        beam_size = opt.get('beam_size', 1)\n","        temperature = opt.get('temperature', 1.0)\n","        sample_n = int(opt.get('sample_n', 1))\n","        group_size = opt.get('group_size', 1)\n","        output_logsoftmax = opt.get('output_logsoftmax', 1)\n","        decoding_constraint = opt.get('decoding_constraint', 0)\n","        block_trigrams = opt.get('block_trigrams', 0)\n","        if beam_size > 1 and sample_method in ['greedy', 'beam_search']:\n","            return self._sample_beam(fc_feats, att_feats, att_masks, opt)\n","        if group_size > 1:\n","            return self._diverse_sample(fc_feats, att_feats, att_masks, opt)\n","\n","        batch_size = fc_feats.size(0)\n","        state = self.init_hidden(batch_size * sample_n)\n","\n","        p_fc_feats, p_att_feats, pp_att_feats, p_att_masks = self._prepare_feature(fc_feats, att_feats, att_masks)\n","\n","        if sample_n > 1:\n","            p_fc_feats, p_att_feats, pp_att_feats, p_att_masks = repeat_tensors(sample_n,\n","                                                                                      [p_fc_feats, p_att_feats,\n","                                                                                       pp_att_feats, p_att_masks]\n","                                                                                      )\n","\n","        trigrams = []  # will be a list of batch_size dictionaries\n","\n","        seq = fc_feats.new_full((batch_size * sample_n, self.max_seq_length), self.pad_idx, dtype=torch.long)\n","        seqLogprobs = fc_feats.new_zeros(batch_size * sample_n, self.max_seq_length, self.vocab_size + 1)\n","        for t in range(self.max_seq_length + 1):\n","            if t == 0:  # input <bos>\n","                it = fc_feats.new_full([batch_size * sample_n], self.bos_idx, dtype=torch.long)\n","\n","            logprobs, state = self.get_logprobs_state(it, p_fc_feats, p_att_feats, pp_att_feats, p_att_masks, state,\n","                                                      output_logsoftmax=output_logsoftmax)\n","\n","            if decoding_constraint and t > 0:\n","                tmp = logprobs.new_zeros(logprobs.size())\n","                tmp.scatter_(1, seq[:, t - 1].data.unsqueeze(1), float('-inf'))\n","                logprobs = logprobs + tmp\n","\n","            # Mess with trigrams\n","            # Copy from https://github.com/lukemelas/image-paragraph-captioning\n","            if block_trigrams and t >= 3:\n","                # Store trigram generated at last step\n","                prev_two_batch = seq[:, t - 3:t - 1]\n","                for i in range(batch_size):  # = seq.size(0)\n","                    prev_two = (prev_two_batch[i][0].item(), prev_two_batch[i][1].item())\n","                    current = seq[i][t - 1]\n","                    if t == 3:  # initialize\n","                        trigrams.append({prev_two: [current]})  # {LongTensor: list containing 1 int}\n","                    elif t > 3:\n","                        if prev_two in trigrams[i]:  # add to list\n","                            trigrams[i][prev_two].append(current)\n","                        else:  # create list\n","                            trigrams[i][prev_two] = [current]\n","                # Block used trigrams at next step\n","                prev_two_batch = seq[:, t - 2:t]\n","                mask = torch.zeros(logprobs.size(), requires_grad=False).cuda()  # batch_size x vocab_size\n","                for i in range(batch_size):\n","                    prev_two = (prev_two_batch[i][0].item(), prev_two_batch[i][1].item())\n","                    if prev_two in trigrams[i]:\n","                        for j in trigrams[i][prev_two]:\n","                            mask[i, j] += 1\n","                # Apply mask to log probs\n","                # logprobs = logprobs - (mask * 1e9)\n","                alpha = 2.0  # = 4\n","                logprobs = logprobs + (mask * -0.693 * alpha)  # ln(1/2) * alpha (alpha -> infty works best)\n","\n","            # sample the next word\n","            if t == self.max_seq_length:  # skip if we achieve maximum length\n","                break\n","            it, sampleLogprobs = self.sample_next_word(logprobs, sample_method, temperature)\n","\n","            # stop when all finished\n","            if t == 0:\n","                unfinished = it != self.eos_idx\n","            else:\n","                it[~unfinished] = self.pad_idx  # This allows eos_idx not being overwritten to 0\n","                logprobs = logprobs * unfinished.unsqueeze(1).float()\n","                unfinished = unfinished * (it != self.eos_idx)\n","            seq[:, t] = it\n","            seqLogprobs[:, t] = logprobs\n","            # quit loop if all sequences have finished\n","            if unfinished.sum() == 0:\n","                break\n","\n","        return seq, seqLogprobs\n","\n","    def _diverse_sample(self, fc_feats, att_feats, att_masks=None, opt={}):\n","\n","        sample_method = opt.get('sample_method', 'greedy')\n","        beam_size = opt.get('beam_size', 1)\n","        temperature = opt.get('temperature', 1.0)\n","        group_size = opt.get('group_size', 1)\n","        diversity_lambda = opt.get('diversity_lambda', 0.5)\n","        decoding_constraint = opt.get('decoding_constraint', 0)\n","        block_trigrams = opt.get('block_trigrams', 0)\n","\n","        batch_size = fc_feats.size(0)\n","        state = self.init_hidden(batch_size)\n","\n","        p_fc_feats, p_att_feats, pp_att_feats, p_att_masks = self._prepare_feature(fc_feats, att_feats, att_masks)\n","\n","        trigrams_table = [[] for _ in range(group_size)]  # will be a list of batch_size dictionaries\n","\n","        seq_table = [fc_feats.new_full((batch_size, self.max_seq_length), self.pad_idx, dtype=torch.long) for _ in\n","                     range(group_size)]\n","        seqLogprobs_table = [fc_feats.new_zeros(batch_size, self.max_seq_length) for _ in range(group_size)]\n","        state_table = [self.init_hidden(batch_size) for _ in range(group_size)]\n","\n","        for tt in range(self.max_seq_length + group_size):\n","            for divm in range(group_size):\n","                t = tt - divm\n","                seq = seq_table[divm]\n","                seqLogprobs = seqLogprobs_table[divm]\n","                trigrams = trigrams_table[divm]\n","                if t >= 0 and t <= self.max_seq_length - 1:\n","                    if t == 0:  # input <bos>\n","                        it = fc_feats.new_full([batch_size], self.bos_idx, dtype=torch.long)\n","                    else:\n","                        it = seq[:, t - 1]  # changed\n","\n","                    logprobs, state_table[divm] = self.get_logprobs_state(it, p_fc_feats, p_att_feats, pp_att_feats,\n","                                                                          p_att_masks, state_table[divm])  # changed\n","                    logprobs = F.log_softmax(logprobs / temperature, dim=-1)\n","\n","                    # Add diversity\n","                    if divm > 0:\n","                        unaug_logprobs = logprobs.clone()\n","                        for prev_choice in range(divm):\n","                            prev_decisions = seq_table[prev_choice][:, t]\n","                            logprobs[:, prev_decisions] = logprobs[:, prev_decisions] - diversity_lambda\n","\n","                    if decoding_constraint and t > 0:\n","                        tmp = logprobs.new_zeros(logprobs.size())\n","                        tmp.scatter_(1, seq[:, t - 1].data.unsqueeze(1), float('-inf'))\n","                        logprobs = logprobs + tmp\n","\n","                    # Mess with trigrams\n","                    if block_trigrams and t >= 3:\n","                        # Store trigram generated at last step\n","                        prev_two_batch = seq[:, t - 3:t - 1]\n","                        for i in range(batch_size):  # = seq.size(0)\n","                            prev_two = (prev_two_batch[i][0].item(), prev_two_batch[i][1].item())\n","                            current = seq[i][t - 1]\n","                            if t == 3:  # initialize\n","                                trigrams.append({prev_two: [current]})  # {LongTensor: list containing 1 int}\n","                            elif t > 3:\n","                                if prev_two in trigrams[i]:  # add to list\n","                                    trigrams[i][prev_two].append(current)\n","                                else:  # create list\n","                                    trigrams[i][prev_two] = [current]\n","                        # Block used trigrams at next step\n","                        prev_two_batch = seq[:, t - 2:t]\n","                        mask = torch.zeros(logprobs.size(), requires_grad=False).cuda()  # batch_size x vocab_size\n","                        for i in range(batch_size):\n","                            prev_two = (prev_two_batch[i][0].item(), prev_two_batch[i][1].item())\n","                            if prev_two in trigrams[i]:\n","                                for j in trigrams[i][prev_two]:\n","                                    mask[i, j] += 1\n","                        # Apply mask to log probs\n","                        # logprobs = logprobs - (mask * 1e9)\n","                        alpha = 2.0  # = 4\n","                        logprobs = logprobs + (mask * -0.693 * alpha)  # ln(1/2) * alpha (alpha -> infty works best)\n","\n","                    it, sampleLogprobs = self.sample_next_word(logprobs, sample_method, 1)\n","\n","                    # stop when all finished\n","                    if t == 0:\n","                        unfinished = it != self.eos_idx\n","                    else:\n","                        unfinished = seq[:, t - 1] != self.pad_idx & seq[:, t - 1] != self.eos_idx\n","                        it[~unfinished] = self.pad_idx\n","                        unfinished = unfinished & (it != self.eos_idx)  # changed\n","                    seq[:, t] = it\n","                    seqLogprobs[:, t] = sampleLogprobs.view(-1)\n","\n","        return torch.stack(seq_table, 1).reshape(batch_size * group_size, -1), torch.stack(seqLogprobs_table,\n","                                                                                           1).reshape(\n","            batch_size * group_size, -1)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"tUADBh2U__OI","executionInfo":{"status":"ok","timestamp":1709126250101,"user_tz":-300,"elapsed":7,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"}}},"outputs":[],"source":["# base_cmn.py\n","\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import copy\n","import math\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# from .att_model import pack_wrapper, AttModel\n","\n","\n","def clones(module, N):\n","    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n","\n","\n","def subsequent_mask(size):\n","    attn_shape = (1, size, size)\n","    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n","    return torch.from_numpy(subsequent_mask) == 0\n","\n","\n","def attention(query, key, value, mask=None, dropout=None):\n","    d_k = query.size(-1)\n","    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n","    if mask is not None:\n","        scores = scores.masked_fill(mask == 0, float('-inf'))\n","    p_attn = F.softmax(scores, dim=-1)\n","    if dropout is not None:\n","        p_attn = dropout(p_attn)\n","    return torch.matmul(p_attn, value), p_attn\n","\n","\n","def memory_querying_responding(query, key, value, mask=None, dropout=None, topk=32):\n","    d_k = query.size(-1)\n","    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n","    if mask is not None:\n","        scores = scores.masked_fill(mask == 0, float('-inf'))\n","    selected_scores, idx = scores.topk(topk)\n","    dummy_value = value.unsqueeze(2).expand(idx.size(0), idx.size(1), idx.size(2), value.size(-2), value.size(-1))\n","    dummy_idx = idx.unsqueeze(-1).expand(idx.size(0), idx.size(1), idx.size(2), idx.size(3), value.size(-1))\n","    selected_value = torch.gather(dummy_value, 3, dummy_idx)\n","    p_attn = F.softmax(selected_scores, dim=-1)\n","    if dropout is not None:\n","        p_attn = dropout(p_attn)\n","    return torch.matmul(p_attn.unsqueeze(3), selected_value).squeeze(3), p_attn\n","\n","\n","class Transformer(nn.Module):\n","    def __init__(self, encoder, decoder, src_embed, tgt_embed, cmn):\n","        super(Transformer, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.src_embed = src_embed\n","        self.tgt_embed = tgt_embed\n","        self.cmn = cmn\n","\n","    def forward(self, src, tgt, src_mask, tgt_mask, memory_matrix):\n","        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask, memory_matrix=memory_matrix)\n","\n","    def encode(self, src, src_mask):\n","        return self.encoder(self.src_embed(src), src_mask)\n","\n","    def decode(self, memory, src_mask, tgt, tgt_mask, past=None, memory_matrix=None):\n","        embeddings = self.tgt_embed(tgt)\n","\n","        # Memory querying and responding for textual features\n","        dummy_memory_matrix = memory_matrix.unsqueeze(0).expand(embeddings.size(0), memory_matrix.size(0), memory_matrix.size(1))\n","        responses = self.cmn(embeddings, dummy_memory_matrix, dummy_memory_matrix)\n","        embeddings = embeddings + responses\n","        # Memory querying and responding for textual features\n","\n","        return self.decoder(embeddings, memory, src_mask, tgt_mask, past=past)\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(self, layer, N):\n","        super(Encoder, self).__init__()\n","        self.layers = clones(layer, N)\n","        self.norm = LayerNorm(layer.size)\n","\n","    def forward(self, x, mask):\n","        for layer in self.layers:\n","            x = layer(x, mask)\n","        return self.norm(x)\n","\n","\n","class LayerNorm(nn.Module):\n","    def __init__(self, features, eps=1e-6):\n","        super(LayerNorm, self).__init__()\n","        self.a_2 = nn.Parameter(torch.ones(features))\n","        self.b_2 = nn.Parameter(torch.zeros(features))\n","        self.eps = eps\n","\n","    def forward(self, x):\n","        mean = x.mean(-1, keepdim=True)\n","        std = x.std(-1, keepdim=True)\n","        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n","\n","\n","class SublayerConnection(nn.Module):\n","    def __init__(self, size, dropout):\n","        super(SublayerConnection, self).__init__()\n","        self.norm = LayerNorm(size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, sublayer):\n","        _x = sublayer(self.norm(x))\n","        if type(_x) is tuple:\n","            return x + self.dropout(_x[0]), _x[1]\n","        return x + self.dropout(_x)\n","\n","\n","class EncoderLayer(nn.Module):\n","    def __init__(self, size, self_attn, feed_forward, dropout):\n","        super(EncoderLayer, self).__init__()\n","        self.self_attn = self_attn\n","        self.feed_forward = feed_forward\n","        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n","        self.size = size\n","\n","    def forward(self, x, mask):\n","        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n","        return self.sublayer[1](x, self.feed_forward)\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(self, layer, N):\n","        super(Decoder, self).__init__()\n","        self.layers = clones(layer, N)\n","        self.norm = LayerNorm(layer.size)\n","\n","    def forward(self, x, memory, src_mask, tgt_mask, past=None):\n","        if past is not None:\n","            present = [[], []]\n","            x = x[:, -1:]\n","            tgt_mask = tgt_mask[:, -1:] if tgt_mask is not None else None\n","            past = list(zip(past[0].split(2, dim=0), past[1].split(2, dim=0)))\n","        else:\n","            past = [None] * len(self.layers)\n","        for i, (layer, layer_past) in enumerate(zip(self.layers, past)):\n","            x = layer(x, memory, src_mask, tgt_mask,\n","                      layer_past)\n","            if layer_past is not None:\n","                present[0].append(x[1][0])\n","                present[1].append(x[1][1])\n","                x = x[0]\n","        if past[0] is None:\n","            return self.norm(x)\n","        else:\n","            return self.norm(x), [torch.cat(present[0], 0), torch.cat(present[1], 0)]\n","\n","\n","class DecoderLayer(nn.Module):\n","    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n","        super(DecoderLayer, self).__init__()\n","        self.size = size\n","        self.self_attn = self_attn\n","        self.src_attn = src_attn\n","        self.feed_forward = feed_forward\n","        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n","\n","    def forward(self, x, memory, src_mask, tgt_mask, layer_past=None):\n","        m = memory\n","        if layer_past is None:\n","            x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n","            x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n","            return self.sublayer[2](x, self.feed_forward)\n","        else:\n","            present = [None, None]\n","            x, present[0] = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask, layer_past[0]))\n","            x, present[1] = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask, layer_past[1]))\n","            return self.sublayer[2](x, self.feed_forward), present\n","\n","\n","class MultiThreadMemory(nn.Module):\n","    def __init__(self, h, d_model, dropout=0.1, topk=32):\n","        super(MultiThreadMemory, self).__init__()\n","        assert d_model % h == 0\n","        self.d_k = d_model // h\n","        self.h = h\n","        self.linears = clones(nn.Linear(d_model, d_model), 4)\n","        self.attn = None\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.topk = topk\n","\n","    def forward(self, query, key, value, mask=None, layer_past=None):\n","        if mask is not None:\n","            mask = mask.unsqueeze(1)\n","        nbatches = query.size(0)\n","\n","        if layer_past is not None and layer_past.shape[2] == key.shape[1] > 1:\n","            query = self.linears[0](query)\n","            key, value = layer_past[0], layer_past[1]\n","            present = torch.stack([key, value])\n","        else:\n","            query, key, value = \\\n","                [l(x) for l, x in zip(self.linears, (query, key, value))]\n","        if layer_past is not None and not (layer_past.shape[2] == key.shape[1] > 1):\n","            past_key, past_value = layer_past[0], layer_past[1]\n","            key = torch.cat((past_key, key), dim=1)\n","            value = torch.cat((past_value, value), dim=1)\n","            present = torch.stack([key, value])\n","\n","        query, key, value = \\\n","            [x.view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n","             for x in [query, key, value]]\n","\n","        x, self.attn = memory_querying_responding(query, key, value, mask=mask, dropout=self.dropout, topk=self.topk)\n","\n","        x = x.transpose(1, 2).contiguous() \\\n","            .view(nbatches, -1, self.h * self.d_k)\n","        if layer_past is not None:\n","            return self.linears[-1](x), present\n","        else:\n","            return self.linears[-1](x)\n","\n","\n","class MultiHeadedAttention(nn.Module):\n","    def __init__(self, h, d_model, dropout=0.1):\n","        super(MultiHeadedAttention, self).__init__()\n","        assert d_model % h == 0\n","        self.d_k = d_model // h\n","        self.h = h\n","        self.linears = clones(nn.Linear(d_model, d_model), 4)\n","        self.attn = None\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","    def forward(self, query, key, value, mask=None, layer_past=None):\n","        if mask is not None:\n","            mask = mask.unsqueeze(1)\n","        nbatches = query.size(0)\n","        if layer_past is not None and layer_past.shape[2] == key.shape[1] > 1:\n","            query = self.linears[0](query)\n","            key, value = layer_past[0], layer_past[1]\n","            present = torch.stack([key, value])\n","        else:\n","            query, key, value = \\\n","                [l(x) for l, x in zip(self.linears, (query, key, value))]\n","\n","        if layer_past is not None and not (layer_past.shape[2] == key.shape[1] > 1):\n","            past_key, past_value = layer_past[0], layer_past[1]\n","            key = torch.cat((past_key, key), dim=1)\n","            value = torch.cat((past_value, value), dim=1)\n","            present = torch.stack([key, value])\n","\n","        query, key, value = \\\n","            [x.view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n","             for x in [query, key, value]]\n","\n","        x, self.attn = attention(query, key, value, mask=mask,\n","                                 dropout=self.dropout)\n","        x = x.transpose(1, 2).contiguous() \\\n","            .view(nbatches, -1, self.h * self.d_k)\n","        if layer_past is not None:\n","            return self.linears[-1](x), present\n","        else:\n","            return self.linears[-1](x)\n","\n","\n","class PositionwiseFeedForward(nn.Module):\n","    def __init__(self, d_model, d_ff, dropout=0.1):\n","        super(PositionwiseFeedForward, self).__init__()\n","        self.w_1 = nn.Linear(d_model, d_ff)\n","        self.w_2 = nn.Linear(d_ff, d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n","\n","\n","class Embeddings(nn.Module):\n","    def __init__(self, d_model, vocab):\n","        super(Embeddings, self).__init__()\n","        self.lut = nn.Embedding(vocab, d_model)\n","        self.d_model = d_model\n","\n","    def forward(self, x):\n","        return self.lut(x) * math.sqrt(self.d_model)\n","\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len).unsqueeze(1).float()\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n","                             -(math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:, :x.size(1)]\n","        return self.dropout(x)\n","\n","\n","class BaseCMN(AttModel):\n","\n","    def make_model(self, tgt_vocab, cmn):\n","        c = copy.deepcopy\n","        attn = MultiHeadedAttention(self.num_heads, self.d_model)\n","        ff = PositionwiseFeedForward(self.d_model, self.d_ff, self.dropout)\n","        position = PositionalEncoding(self.d_model, self.dropout)\n","        model = Transformer(\n","            Encoder(EncoderLayer(self.d_model, c(attn), c(ff), self.dropout), self.num_layers),\n","            Decoder(DecoderLayer(self.d_model, c(attn), c(attn), c(ff), self.dropout), self.num_layers),\n","            nn.Sequential(c(position)),\n","            nn.Sequential(Embeddings(self.d_model, tgt_vocab), c(position)), cmn)\n","        for p in model.parameters():\n","            if p.dim() > 1:\n","                nn.init.xavier_uniform_(p)\n","        return model\n","\n","    def __init__(self, args, tokenizer):\n","        super(BaseCMN, self).__init__(args, tokenizer)\n","        self.args = args\n","        self.num_layers = args['num_layers']\n","        self.d_model = args['d_model']      #the dimension of Transformer\n","        self.d_ff = args['d_ff']            #the dimension of FFN\n","        self.num_heads = args['num_heads']  #the number of heads in Transformer.\n","        self.dropout = args['dropout']      #the dropout rate of Transformer\n","        self.topk = args['topk']            #the number of k\n","\n","        tgt_vocab = self.vocab_size + 1\n","\n","        self.cmn = MultiThreadMemory(args['num_heads'], args['d_model'], topk=args['topk'])\n","\n","        self.model = self.make_model(tgt_vocab, self.cmn)\n","        self.logit = nn.Linear(args['d_model'], tgt_vocab)\n","\n","        self.memory_matrix = nn.Parameter(torch.FloatTensor(args['cmm_size'], args['cmm_dim']))\n","        nn.init.normal_(self.memory_matrix, 0, 1 / args['cmm_dim'])\n","\n","    def init_hidden(self, bsz):\n","        return []\n","\n","    def _prepare_feature(self, fc_feats, att_feats, att_masks):\n","        att_feats, seq, att_masks, seq_mask = self._prepare_feature_forward(att_feats, att_masks)\n","        memory = self.model.encode(att_feats, att_masks)\n","\n","        return fc_feats[..., :1], att_feats[..., :1], memory, att_masks\n","\n","    def _prepare_feature_forward(self, att_feats, att_masks=None, seq=None):\n","        att_feats, att_masks = self.clip_att(att_feats, att_masks)\n","        att_feats = pack_wrapper(self.att_embed, att_feats, att_masks)\n","\n","        if att_masks is None:\n","            att_masks = att_feats.new_ones(att_feats.shape[:2], dtype=torch.long)\n","\n","        # Memory querying and responding for visual features\n","        dummy_memory_matrix = self.memory_matrix.unsqueeze(0).expand(att_feats.size(0), self.memory_matrix.size(0), self.memory_matrix.size(1))\n","        responses = self.cmn(att_feats, dummy_memory_matrix, dummy_memory_matrix)\n","        att_feats = att_feats + responses\n","        # Memory querying and responding for visual features\n","\n","        att_masks = att_masks.unsqueeze(-2)\n","        if seq is not None:\n","            seq = seq[:, :-1]\n","            seq_mask = (seq.data > 0)\n","            seq_mask[:, 0] += True\n","\n","            seq_mask = seq_mask.unsqueeze(-2)\n","            seq_mask = seq_mask & subsequent_mask(seq.size(-1)).to(seq_mask)\n","        else:\n","            seq_mask = None\n","\n","        return att_feats, seq, att_masks, seq_mask\n","\n","    def _forward(self, fc_feats, att_feats, seq, att_masks=None):\n","       att_feats, seq, att_masks, seq_mask = self._prepare_feature_forward(att_feats, att_masks, seq)\n","\n","       # Print the shapes of tensors before and after each operation\n","      #  print(\"Input shapes:\")\n","      #  print(\"att_feats:\", att_feats.shape)\n","      #  print(\"seq:\", seq.shape)\n","      #  print(\"att_masks:\", att_masks.shape)\n","      #  print(\"seq_mask:\", seq_mask.shape)\n","\n","       out = self.model(att_feats, seq, att_masks, seq_mask, memory_matrix=self.memory_matrix)\n","       outputs = F.log_softmax(self.logit(out), dim=-1)\n","\n","      #  # Print the shape of the final output tensor\n","      #  print(\"Output shape:\")\n","      #  print(\"outputs:\", outputs.shape)\n","\n","       return outputs\n","\n","\n","    def _save_attns(self, start=False):\n","        if start:\n","            self.attention_weights = []\n","        self.attention_weights.append([layer.src_attn.attn.cpu().numpy() for layer in self.model.decoder.layers])\n","\n","    def core(self, it, fc_feats_ph, att_feats_ph, memory, state, mask):\n","        if len(state) == 0:\n","            ys = it.unsqueeze(1)\n","            past = [fc_feats_ph.new_zeros(self.num_layers * 2, fc_feats_ph.shape[0], 0, self.d_model),\n","                    fc_feats_ph.new_zeros(self.num_layers * 2, fc_feats_ph.shape[0], 0, self.d_model)]\n","        else:\n","            ys = torch.cat([state[0][0], it.unsqueeze(1)], dim=1)\n","            past = state[1:]\n","        out, past = self.model.decode(memory, mask, ys, subsequent_mask(ys.size(1)).to(memory.device), past=past,\n","                                      memory_matrix=self.memory_matrix)\n","\n","        if not self.training:\n","            self._save_attns(start=len(state) == 0)\n","        return out[:, -1], [ys.unsqueeze(0)] + past\n"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"9S3-tXOAA5nO","executionInfo":{"status":"ok","timestamp":1709127749644,"user_tz":-300,"elapsed":6,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"}}},"outputs":[],"source":["# visual_extractor.py\n","\n","import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","\n","\n","class VisualExtractor(nn.Module):\n","    def __init__(self, args):\n","        super(VisualExtractor, self).__init__()\n","        self.visual_extractor = args['visual_extractor']\n","        self.pretrained = args['visual_extractor_pretrained']\n","        model = getattr(models, self.visual_extractor)(pretrained=self.pretrained)\n","        modules = list(model.children())[:-2]\n","        self.model = nn.Sequential(*modules)\n","        self.avg_fnt = torch.nn.AvgPool2d(kernel_size=7, stride=1, padding=0)\n","\n","    def forward(self, images):\n","        patch_feats = self.model(images)\n","        avg_feats = self.avg_fnt(patch_feats).squeeze().reshape(-1, patch_feats.size(1))\n","        batch_size, feat_size, _, _ = patch_feats.shape\n","        patch_feats = patch_feats.reshape(batch_size, feat_size, -1).permute(0, 2, 1)\n","        return patch_feats, avg_feats\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"Y3bcNkMM9GfN","executionInfo":{"status":"ok","timestamp":1709126251412,"user_tz":-300,"elapsed":17,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"}}},"outputs":[],"source":["# models.py\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","\n","\n","\n","\n","class BaseCMNModel(nn.Module):\n","    def __init__(self, args, tokenizer):\n","        super(BaseCMNModel, self).__init__()\n","        self.args = args\n","        self.tokenizer = tokenizer\n","\n","        # Create the visual extractor\n","        self.visual_extractor = VisualExtractor(args)\n","        print(\"bellow the visual extractor\")\n","        visual_out_features = 2048  # Adjust this based on the actual output size\n","\n","        # Create the linear layer with the correct input and output size\n","        linear_input_size = visual_out_features\n","        linear_output_size = 980  # Adjust this value based on the actual output size\n","        print(f\"Visual output size: {visual_out_features}\")\n","        print(f\"linear_input_size: {linear_input_size}\")\n","        self.linear_layer = nn.Linear(linear_input_size, linear_output_size)\n","        print(\"bellow the linear layer\")\n","\n","        # Create the encoder-decoder module\n","        self.encoder_decoder = BaseCMN(args, tokenizer)\n","        print(\"in basecmnmodel bellow the encode_decoder\")\n","\n","        if args['dataset_name'] == 'iu_xray':\n","            self.forward = self.forward_iu_xray\n","            print(\"in basecmnmodel iu xray forward pass\")\n","        else:\n","            self.forward = self.forward_mimic_cxr\n","\n","    def forward_iu_xray(self, images, targets=None, mode='train', update_opts={}):\n","        # Forward pass through the visual extractor\n","        images = images.view(-1, *images.shape[2:])\n","        print(\"In forward_iu_xray in BaseCMNModel\")\n","\n","        _, avg_feats = self.visual_extractor(images)\n","        print(\"Below the avg_feats in BaseCMNModel\")\n","        print(f\"Shape of avg_feats before flattening: {avg_feats.shape}\")\n","\n","        # Flatten the features\n","        batch_size, num_features = avg_feats.size(0), np.prod(avg_feats.size()[1:])\n","        # Flatten the features\n","        avg_feats = avg_feats.view(batch_size, -1)\n","        print(f\"Shape of avg_feats after flattening: {avg_feats.shape}\")\n","\n","        linear_output_size = 980\n","        avg_feats = avg_feats.view(batch_size, num_features)\n","        print(f\"Shape of avg_feats after flattening: {avg_feats.shape}\")\n","\n","        # Forward pass through the linear layer\n","        linear_output = self.linear_layer(avg_feats)\n","        print(\"Below the linear_output in forward_iu_xray\")\n","\n","        # Continue with the rest of your model...\n","        # Example: Assuming self.encoder_decoder has a forward method\n","        if mode == 'train':\n","            print('in train')\n","            output = self.encoder_decoder(linear_output, targets=targets, mode='forward')  # Adjust arguments\n","            return output\n","        elif mode == 'sample':\n","            output, output_probs = self.encoder_decoder(linear_output, mode='sample', update_opts=update_opts)  # Adjust arguments\n","            return output, output_probs\n","        else:\n","            raise ValueError\n","\n","    def forward_mimic_cxr(self, images, targets=None, mode='train', update_opts={}):\n","        att_feats, fc_feats = self.visual_extractor(images)\n","        if mode == 'train':\n","            output = self.encoder_decoder(fc_feats, att_feats, targets, mode='forward')\n","            return output\n","        elif mode == 'sample':\n","            output, output_probs = self.encoder_decoder(fc_feats, att_feats, mode='sample', update_opts=update_opts)\n","            return output, output_probs\n","        else:\n","            raise ValueError\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"NzH8kZOzBEq_","executionInfo":{"status":"ok","timestamp":1709126251413,"user_tz":-300,"elapsed":14,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"}}},"outputs":[],"source":["# dataset.py\n","\n","import json\n","import os\n","\n","import torch\n","from PIL import Image\n","from torch.utils.data import Dataset\n","\n","\n","class BaseDataset(Dataset):\n","    def __init__(self, args, tokenizer, split, transform=None):\n","        self.image_dir = args['image_dir']\n","        self.ann_path = args['ann_path']\n","        self.max_seq_length = args['max_seq_length']\n","        self.split = split\n","        self.tokenizer = tokenizer\n","        self.transform = transform\n","        self.ann = json.loads(open(self.ann_path, 'r').read())\n","        self.examples = self.ann[self.split]\n","        for i in range(len(self.examples)):\n","            self.examples[i]['ids'] = tokenizer(self.examples[i]['report'])[:self.max_seq_length]\n","            self.examples[i]['mask'] = [1] * len(self.examples[i]['ids'])\n","\n","    def __len__(self):\n","        return len(self.examples)\n","\n","\n","class IuxrayMultiImageDataset(BaseDataset):\n","    def __getitem__(self, idx):\n","        example = self.examples[idx]\n","        image_id = example['id']\n","        image_path = example['image_path']\n","        image_1 = Image.open(os.path.join(self.image_dir, image_path[0])).convert('RGB')\n","        image_2 = Image.open(os.path.join(self.image_dir, image_path[1])).convert('RGB')\n","        if self.transform is not None:\n","            image_1 = self.transform(image_1)\n","            image_2 = self.transform(image_2)\n","        image = torch.stack((image_1, image_2), 0)\n","        report_ids = example['ids']\n","        report_masks = example['mask']\n","        seq_length = len(report_ids)\n","        sample = (image_id, image, report_ids, report_masks, seq_length)\n","        return sample\n","\n","\n","class MimiccxrSingleImageDataset(BaseDataset):\n","    def __getitem__(self, idx):\n","        example = self.examples[idx]\n","        image_id = example['id']\n","        image_path = example['image_path']\n","        image = Image.open(os.path.join(self.image_dir, image_path[0])).convert('RGB')\n","        image_id = os.path.join(self.image_dir, image_path[0])\n","        if self.transform is not None:\n","            image = self.transform(image)\n","        report_ids = example['ids']\n","        report_masks = example['mask']\n","        seq_length = len(report_ids)\n","        sample = (image_id, image, report_ids, report_masks, seq_length)\n","        return sample\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Zy2B_4Oz9SRo","executionInfo":{"status":"ok","timestamp":1709126251413,"user_tz":-300,"elapsed":13,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"}}},"outputs":[],"source":["import numpy as np\n","import torch\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","\n","class R2DataLoader(DataLoader):\n","    def __init__(self, args, tokenizer, split, shuffle):\n","        self.args = args\n","        self.dataset_name = args['dataset_name']\n","        self.batch_size = args['batch_size']\n","        self.shuffle = shuffle\n","        self.num_workers = args['num_workers']\n","        self.tokenizer = tokenizer\n","        self.split = split\n","\n","        if split == 'train':\n","            # Training data transformations\n","            self.transform = transforms.Compose([\n","                transforms.Resize((256, 256)),\n","                transforms.RandomCrop(224),\n","                transforms.RandomHorizontalFlip(),\n","                transforms.ToTensor(),\n","                transforms.Normalize((0.485, 0.456, 0.406),\n","                                     (0.229, 0.224, 0.225))])\n","        else:\n","            # Validation or test data transformations\n","            self.transform = transforms.Compose([\n","                transforms.Resize((224, 224)),\n","                transforms.ToTensor(),\n","                transforms.Normalize((0.485, 0.456, 0.406),\n","                                     (0.229, 0.224, 0.225))])\n","\n","        if self.dataset_name == 'iu_xray':\n","            self.dataset = IuxrayMultiImageDataset(self.args, self.tokenizer, self.split, transform=self.transform)\n","        else:\n","            self.dataset = MimiccxrSingleImageDataset(self.args, self.tokenizer, self.split, transform=self.transform)\n","\n","        self.init_kwargs = {\n","            'dataset': self.dataset,\n","            'batch_size': self.batch_size,\n","            'shuffle': self.shuffle,\n","            'collate_fn': self.collate_fn,\n","            'num_workers': self.num_workers\n","        }\n","        super().__init__(**self.init_kwargs)\n","\n","    @staticmethod\n","    def collate_fn(data):\n","        image_id_batch, image_batch, report_ids_batch, report_masks_batch, seq_lengths_batch = zip(*data)\n","\n","        image_batch = torch.stack([img.squeeze(0) for img in image_batch], 0)\n","\n","        # print(\"Input image shape:\", image_batch.shape)  # Add this line\n","        max_seq_length = max(seq_lengths_batch)\n","\n","        target_batch = np.zeros((len(report_ids_batch), max_seq_length), dtype=int)\n","        target_masks_batch = np.zeros((len(report_ids_batch), max_seq_length), dtype=int)\n","\n","        for i, report_ids in enumerate(report_ids_batch):\n","            target_batch[i, :len(report_ids)] = report_ids\n","\n","        for i, report_masks in enumerate(report_masks_batch):\n","            target_masks_batch[i, :len(report_masks)] = report_masks\n","\n","        print(\"ending the dataloader\")\n","        return image_id_batch, image_batch, torch.LongTensor(target_batch), torch.FloatTensor(target_masks_batch)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"lFzd3ADX9aN_","executionInfo":{"status":"ok","timestamp":1709126252207,"user_tz":-300,"elapsed":4,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"}}},"outputs":[],"source":["\n","#loss.py\n","\n","import torch\n","import torch.nn as nn\n","\n","\n","class LanguageModelCriterion(nn.Module):\n","    def __init__(self):\n","        super(LanguageModelCriterion, self).__init__()\n","\n","    def forward(self, input, target, mask):\n","        # truncate to the same size\n","        target = target[:, :input.size(1)]\n","        mask = mask[:, :input.size(1)]\n","        output = -input.gather(2, target.long().unsqueeze(2)).squeeze(2) * mask\n","        output = torch.sum(output) / torch.sum(mask)\n","        return output\n","\n","\n","def compute_loss(output, reports_ids, reports_masks):\n","    criterion = LanguageModelCriterion()\n","    loss = criterion(output, reports_ids[:, 1:], reports_masks[:, 1:]).mean()\n","    return loss\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"NySNCFk9Bf0M","executionInfo":{"status":"ok","timestamp":1709126252749,"user_tz":-300,"elapsed":7,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"}}},"outputs":[],"source":["# blueScore.py\n","\n","# bleu_scorer.py\n","# David Chiang <chiang@isi.edu>\n","\n","# Copyright (c) 2004-2006 University of Maryland. All rights\n","# reserved. Do not redistribute without permission from the\n","# author. Not for commercial use.\n","\n","# Modified by:\n","# Hao Fang <hfang@uw.edu>\n","# Tsung-Yi Lin <tl483@cornell.edu>\n","\n","# Last modified : Wed 22 May 2019 08:10:00 PM EDT\n","# By Sabarish Sivanath\n","# To support Python 3\n","\n","'''Provides:\n","cook_refs(refs, n=4): Transform a list of reference sentences as strings into a form usable by cook_test().\n","cook_test(test, refs, n=4): Transform a test sentence as a string (together with the cooked reference sentences) into a form usable by score_cooked().\n","'''\n","\n","import copy\n","import sys, math, re\n","from collections import defaultdict\n","\n","def precook(s, n=4, out=False):\n","    \"\"\"Takes a string as input and returns an object that can be given to\n","    either cook_refs or cook_test. This is optional: cook_refs and cook_test\n","    can take string arguments as well.\"\"\"\n","    words = s.split()\n","    counts = defaultdict(int)\n","    for k in range(1,n+1):\n","        for i in range(len(words)-k+1):\n","            ngram = tuple(words[i:i+k])\n","            counts[ngram] += 1\n","    return (len(words), counts)\n","\n","def cook_refs(refs, eff=None, n=4): ## lhuang: oracle will call with \"average\"\n","    '''Takes a list of reference sentences for a single segment\n","    and returns an object that encapsulates everything that BLEU\n","    needs to know about them.'''\n","\n","    reflen = []\n","    maxcounts = {}\n","    for ref in refs:\n","        rl, counts = precook(ref, n)\n","        reflen.append(rl)\n","        for (ngram,count) in counts.items():\n","            maxcounts[ngram] = max(maxcounts.get(ngram,0), count)\n","\n","    # Calculate effective reference sentence length.\n","    if eff == \"shortest\":\n","        reflen = min(reflen)\n","    elif eff == \"average\":\n","        reflen = float(sum(reflen))/len(reflen)\n","\n","    ## lhuang: N.B.: leave reflen computaiton to the very end!!\n","\n","    ## lhuang: N.B.: in case of \"closest\", keep a list of reflens!! (bad design)\n","\n","    return (reflen, maxcounts)\n","\n","def cook_test(test, refs , eff=None, n=4):\n","    '''Takes a test sentence and returns an object that\n","    encapsulates everything that BLEU needs to know about it.'''\n","\n","    reflen = refs[0]\n","    refmaxcounts = refs[1]\n","\n","    testlen, counts = precook(test, n, True)\n","\n","    result = {}\n","\n","    # Calculate effective reference sentence length.\n","\n","    if eff == \"closest\":\n","        result[\"reflen\"] = min((abs(l-testlen), l) for l in reflen)[1]\n","    else: ## i.e., \"average\" or \"shortest\" or None\n","        result[\"reflen\"] = reflen\n","\n","    result[\"testlen\"] = testlen\n","\n","    result[\"guess\"] = [max(0,testlen-k+1) for k in range(1,n+1)]\n","\n","    result['correct'] = [0]*n\n","    for (ngram, count) in counts.items():\n","        result[\"correct\"][len(ngram)-1] += min(refmaxcounts.get(ngram,0), count)\n","\n","    return result\n","\n","class BleuScorer(object):\n","    \"\"\"Bleu scorer.\n","    \"\"\"\n","\n","    __slots__ = \"n\", \"crefs\", \"ctest\", \"_score\", \"_ratio\", \"_testlen\", \"_reflen\", \"special_reflen\"\n","    # special_reflen is used in oracle (proportional effective ref len for a node).\n","\n","    def copy(self):\n","        ''' copy the refs.'''\n","        new = BleuScorer(n=self.n)\n","        new.ctest = copy.copy(self.ctest)\n","        new.crefs = copy.copy(self.crefs)\n","        new._score = None\n","        return new\n","\n","    def __init__(self, test=None, refs=None, n=4, special_reflen=None):\n","        ''' singular instance '''\n","\n","        self.n = n\n","        self.crefs = []\n","        self.ctest = []\n","        self.cook_append(test, refs)\n","        self.special_reflen = special_reflen\n","\n","    def cook_append(self, test, refs):\n","        '''called by constructor and __iadd__ to avoid creating new instances.'''\n","\n","        if refs is not None:\n","            self.crefs.append(cook_refs(refs))\n","            if test is not None:\n","                cooked_test = cook_test(test, self.crefs[-1])\n","                self.ctest.append(cooked_test) ## N.B.: -1\n","            else:\n","                self.ctest.append(None) # lens of crefs and ctest have to match\n","\n","        self._score = None ## need to recompute\n","\n","    def ratio(self, option=None):\n","        self.compute_score(option=option)\n","        return self._ratio\n","\n","    def score_ratio(self, option=None):\n","        '''return (bleu, len_ratio) pair'''\n","        return (self.fscore(option=option), self.ratio(option=option))\n","\n","    def score_ratio_str(self, option=None):\n","        return \"%.4f (%.2f)\" % self.score_ratio(option)\n","\n","    def reflen(self, option=None):\n","        self.compute_score(option=option)\n","        return self._reflen\n","\n","    def testlen(self, option=None):\n","        self.compute_score(option=option)\n","        return self._testlen\n","\n","    def retest(self, new_test):\n","        if type(new_test) is str:\n","            new_test = [new_test]\n","        assert len(new_test) == len(self.crefs), new_test\n","        self.ctest = []\n","        for t, rs in zip(new_test, self.crefs):\n","            self.ctest.append(cook_test(t, rs))\n","        self._score = None\n","\n","        return self\n","\n","    def rescore(self, new_test):\n","        ''' replace test(s) with new test(s), and returns the new score.'''\n","\n","        return self.retest(new_test).compute_score()\n","\n","    def size(self):\n","        assert len(self.crefs) == len(self.ctest), \"refs/test mismatch! %d<>%d\" % (len(self.crefs), len(self.ctest))\n","        return len(self.crefs)\n","\n","    def __iadd__(self, other):\n","        '''add an instance (e.g., from another sentence).'''\n","\n","        if type(other) is tuple:\n","            ## avoid creating new BleuScorer instances\n","            self.cook_append(other[0], other[1])\n","        else:\n","            assert self.compatible(other), \"incompatible BLEUs.\"\n","            self.ctest.extend(other.ctest)\n","            self.crefs.extend(other.crefs)\n","            self._score = None ## need to recompute\n","\n","        return self\n","\n","    def compatible(self, other):\n","        return isinstance(other, BleuScorer) and self.n == other.n\n","\n","    def single_reflen(self, option=\"average\"):\n","        return self._single_reflen(self.crefs[0][0], option)\n","\n","    def _single_reflen(self, reflens, option=None, testlen=None):\n","\n","        if option == \"shortest\":\n","            reflen = min(reflens)\n","        elif option == \"average\":\n","            reflen = float(sum(reflens))/len(reflens)\n","        elif option == \"closest\":\n","            reflen = min((abs(l-testlen), l) for l in reflens)[1]\n","        else:\n","            assert False, \"unsupported reflen option %s\" % option\n","\n","        return reflen\n","\n","    def recompute_score(self, option=None, verbose=0):\n","        self._score = None\n","        return self.compute_score(option, verbose)\n","\n","    def compute_score(self, option=None, verbose=0):\n","        n = self.n\n","        small = 1e-9\n","        tiny = 1e-15 ## so that if guess is 0 still return 0\n","        bleu_list = [[] for _ in range(n)]\n","\n","        if self._score is not None:\n","            return self._score\n","\n","        if option is None:\n","            option = \"average\" if len(self.crefs) == 1 else \"closest\"\n","\n","        self._testlen = 0\n","        self._reflen = 0\n","        totalcomps = {'testlen':0, 'reflen':0, 'guess':[0]*n, 'correct':[0]*n}\n","\n","        # for each sentence\n","        for comps in self.ctest:\n","            testlen = comps['testlen']\n","            self._testlen += testlen\n","\n","            if self.special_reflen is None: ## need computation\n","                reflen = self._single_reflen(comps['reflen'], option, testlen)\n","            else:\n","                reflen = self.special_reflen\n","\n","            self._reflen += reflen\n","\n","            for key in ['guess','correct']:\n","                for k in range(n):\n","                    totalcomps[key][k] += comps[key][k]\n","\n","            # append per image bleu score\n","            bleu = 1.\n","            for k in range(n):\n","                bleu *= (float(comps['correct'][k]) + tiny) \\\n","                        /(float(comps['guess'][k]) + small)\n","                bleu_list[k].append(bleu ** (1./(k+1)))\n","            ratio = (testlen + tiny) / (reflen + small) ## N.B.: avoid zero division\n","            if ratio < 1:\n","                for k in range(n):\n","                    bleu_list[k][-1] *= math.exp(1 - 1/ratio)\n","\n","            if verbose > 1:\n","                print(comps, reflen)\n","\n","        totalcomps['reflen'] = self._reflen\n","        totalcomps['testlen'] = self._testlen\n","\n","        bleus = []\n","        bleu = 1.\n","        for k in range(n):\n","            bleu *= float(totalcomps['correct'][k] + tiny) \\\n","                    / (totalcomps['guess'][k] + small)\n","            bleus.append(bleu ** (1./(k+1)))\n","        ratio = (self._testlen + tiny) / (self._reflen + small) ## N.B.: avoid zero division\n","        if ratio < 1:\n","            for k in range(n):\n","                bleus[k] *= math.exp(1 - 1/ratio)\n","\n","        if verbose > 0:\n","            print(totalcomps)\n","            print(\"ratio:\", ratio)\n","\n","        self._score = bleus\n","        return self._score, bleu_list\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"Uqc-YtQvBQKy","executionInfo":{"status":"ok","timestamp":1709126252751,"user_tz":-300,"elapsed":7,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"}}},"outputs":[],"source":["# blue.py\n","\n","#!/usr/bin/env python\n","#\n","# File Name : bleu.py\n","#\n","# Description : Wrapper for BLEU scorer.\n","#\n","# Creation Date : 06-01-2015\n","# Last Modified : Thu 19 Mar 2015 09:13:28 PM PDT\n","# Authors : Hao Fang <hfang@uw.edu> and Tsung-Yi Lin <tl483@cornell.edu>\n","\n","# Last modified : Wed 22 May 2019 08:10:00 PM EDT\n","# By Sabarish Sivanath\n","# To support Python 3\n","\n","# from .bleu_scorer import BleuScorer\n","\n","\n","class Bleu:\n","    def __init__(self, n=4):\n","        # default compute Blue score up to 4\n","        self._n = n\n","        self._hypo_for_image = {}\n","        self.ref_for_image = {}\n","\n","    def compute_score(self, gts, res, score_option = 'closest', verbose = 1):\n","        '''\n","        Inputs:\n","            gts - ground truths\n","            res - predictions\n","            score_option - {shortest, closest, average}\n","            verbose - 1 or 0\n","        Outputs:\n","            Blue scores\n","        '''\n","        assert(gts.keys() == res.keys())\n","        imgIds = gts.keys()\n","\n","        bleu_scorer = BleuScorer(n=self._n)\n","        for id in imgIds:\n","            hypo = res[id]\n","            ref = gts[id]\n","\n","            # Sanity check.\n","            assert(type(hypo) is list)\n","            assert(len(hypo) == 1)\n","            assert(type(ref) is list)\n","            #assert(len(ref) >= 1)\n","\n","            bleu_scorer += (hypo[0], ref)\n","\n","        score, scores = bleu_scorer.compute_score(option = score_option, verbose =verbose)\n","\n","        # return (bleu, bleu_info)\n","        return score, scores\n","\n","    def method(self):\n","        return \"Bleu\"\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"3yVU05WQNfEo","executionInfo":{"status":"ok","timestamp":1709126253786,"user_tz":-300,"elapsed":7,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"}}},"outputs":[],"source":["# meteor.py\n","import os\n","import subprocess\n","import threading\n","\n","METEOR_JAR = '/content/R2GenCMN/pycocoevalcap/meteor/meteor-1.5.jar'\n","\n","class Meteor:\n","\n","    def __init__(self):\n","        current_directory = '/content/R2GenCMN/pycocoevalcap/meteor'  # replace with the actual directory path\n","        self.meteor_cmd = ['java', '-jar', '-Xmx2G', METEOR_JAR,\n","                            '-', '-', '-stdio', '-l', 'en', '-norm']\n","        self.meteor_p = subprocess.Popen(self.meteor_cmd,\n","                                        cwd=current_directory,\n","                                        stdin=subprocess.PIPE,\n","                                        stdout=subprocess.PIPE,\n","                                        stderr=subprocess.PIPE,\n","                                        universal_newlines=True,\n","                                        bufsize=1)\n","        # Used to guarantee thread safety\n","        self.lock = threading.Lock()\n","\n","    def compute_score(self, gts, res):\n","        assert(gts.keys() == res.keys())\n","        imgIds = gts.keys()\n","        scores = []\n","\n","        eval_line = 'EVAL'\n","        self.lock.acquire()\n","        for i in imgIds:\n","            assert(len(res[i]) == 1)\n","            stat = self._stat(res[i][0], gts[i])\n","            eval_line += ' ||| {}'.format(stat)\n","\n","        self.meteor_p.stdin.write('{}\\n'.format(eval_line))\n","        for i in range(0,len(imgIds)):\n","            scores.append(float(self.meteor_p.stdout.readline().strip()))\n","        score = float(self.meteor_p.stdout.readline().strip())\n","        self.lock.release()\n","\n","        return score, scores\n","\n","    def method(self):\n","        return \"METEOR\"\n","\n","    def _stat(self, hypothesis_str, reference_list):\n","        # SCORE ||| reference 1 words ||| reference n words ||| hypothesis words\n","        hypothesis_str = hypothesis_str.replace('|||','').replace('  ',' ')\n","        score_line = ' ||| '.join(('SCORE', ' ||| '.join(reference_list), hypothesis_str))\n","        self.meteor_p.stdin.write('{}\\n'.format(score_line))\n","        return self.meteor_p.stdout.readline().strip()\n","\n","    def _score(self, hypothesis_str, reference_list):\n","        self.lock.acquire()\n","        # SCORE ||| reference 1 words ||| reference n words ||| hypothesis words\n","        hypothesis_str = hypothesis_str.replace('|||','').replace('  ',' ')\n","        score_line = ' ||| '.join(('SCORE', ' ||| '.join(reference_list), hypothesis_str))\n","        self.meteor_p.stdin.write('{}\\n'.format(score_line))\n","        stats = self.meteor_p.stdout.readline().strip()\n","        eval_line = 'EVAL ||| {}'.format(stats)\n","        # EVAL ||| stats\n","        self.meteor_p.stdin.write('{}\\n'.format(eval_line))\n","        score = float(self.meteor_p.stdout.readline().strip())\n","        # bug fix: there are two values returned by the jar file, one average, and one all, so do it twice\n","        # thanks for Andrej for pointing this out\n","        score = float(self.meteor_p.stdout.readline().strip())\n","        self.lock.release()\n","        return score\n","\n","    def __del__(self):\n","        self.lock.acquire()\n","        self.meteor_p.stdin.close()\n","        self.meteor_p.kill()\n","        self.meteor_p.wait()\n","        self.lock.release()\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"sQ7Mul-BByw4","executionInfo":{"status":"ok","timestamp":1709126254361,"user_tz":-300,"elapsed":5,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"}}},"outputs":[],"source":["# rough.py\n","\n","#!/usr/bin/env python\n","#\n","# File Name : rouge.py\n","#\n","# Description : Computes ROUGE-L metric as described by Lin and Hovey (2004)\n","#\n","# Creation Date : 2015-01-07 06:03\n","# Author : Ramakrishna Vedantam <vrama91@vt.edu>\n","\n","import numpy as np\n","import pdb\n","\n","def my_lcs(string, sub):\n","    \"\"\"\n","    Calculates longest common subsequence for a pair of tokenized strings\n","    :param string : list of str : tokens from a string split using whitespace\n","    :param sub : list of str : shorter string, also split using whitespace\n","    :returns: length (list of int): length of the longest common subsequence between the two strings\n","\n","    Note: my_lcs only gives length of the longest common subsequence, not the actual LCS\n","    \"\"\"\n","    if(len(string)< len(sub)):\n","        sub, string = string, sub\n","\n","    lengths = [[0 for i in range(0,len(sub)+1)] for j in range(0,len(string)+1)]\n","\n","    for j in range(1,len(sub)+1):\n","        for i in range(1,len(string)+1):\n","            if(string[i-1] == sub[j-1]):\n","                lengths[i][j] = lengths[i-1][j-1] + 1\n","            else:\n","                lengths[i][j] = max(lengths[i-1][j] , lengths[i][j-1])\n","\n","    return lengths[len(string)][len(sub)]\n","\n","class Rouge():\n","    '''\n","    Class for computing ROUGE-L score for a set of candidate sentences for the MS COCO test set\n","\n","    '''\n","    def __init__(self):\n","        # vrama91: updated the value below based on discussion with Hovey\n","        self.beta = 1.2\n","\n","    def calc_score(self, candidate, refs):\n","        \"\"\"\n","        Compute ROUGE-L score given one candidate and references for an image\n","        :param candidate: str : candidate sentence to be evaluated\n","        :param refs: list of str : COCO reference sentences for the particular image to be evaluated\n","        :returns score: int (ROUGE-L score for the candidate evaluated against references)\n","        \"\"\"\n","        assert(len(candidate)==1)\n","        assert(len(refs)>0)\n","        prec = []\n","        rec = []\n","\n","        # split into tokens\n","        token_c = candidate[0].split(\" \")\n","\n","        for reference in refs:\n","            # split into tokens\n","            token_r = reference.split(\" \")\n","            # compute the longest common subsequence\n","            lcs = my_lcs(token_r, token_c)\n","            prec.append(lcs/float(len(token_c)))\n","            rec.append(lcs/float(len(token_r)))\n","\n","        prec_max = max(prec)\n","        rec_max = max(rec)\n","\n","        if(prec_max!=0 and rec_max !=0):\n","            score = ((1 + self.beta**2)*prec_max*rec_max)/float(rec_max + self.beta**2*prec_max)\n","        else:\n","            score = 0.0\n","        return score\n","\n","    def compute_score(self, gts, res):\n","        \"\"\"\n","        Computes Rouge-L score given a set of reference and candidate sentences for the dataset\n","        Invoked by evaluate_captions.py\n","        :param hypo_for_image: dict : candidate / test sentences with \"image name\" key and \"tokenized sentences\" as values\n","        :param ref_for_image: dict : reference MS-COCO sentences with \"image name\" key and \"tokenized sentences\" as values\n","        :returns: average_score: float (mean ROUGE-L score computed by averaging scores for all the images)\n","        \"\"\"\n","        assert(gts.keys() == res.keys())\n","        imgIds = gts.keys()\n","\n","        score = []\n","        for id in imgIds:\n","            hypo = res[id]\n","            ref  = gts[id]\n","\n","            score.append(self.calc_score(hypo, ref))\n","\n","            # Sanity check.\n","            assert(type(hypo) is list)\n","            assert(len(hypo) == 1)\n","            assert(type(ref) is list)\n","            assert(len(ref) > 0)\n","\n","        average_score = np.mean(np.array(score))\n","        return average_score, np.array(score)\n","\n","    def method(self):\n","        return \"Rouge\"\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"eKrLIbYz9rxY","executionInfo":{"status":"ok","timestamp":1709126256876,"user_tz":-300,"elapsed":1253,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"}}},"outputs":[],"source":["#metrics.py\n","\n","from sklearn.metrics import roc_auc_score, f1_score, recall_score, precision_score\n","\n","# from pycocoevalcap.bleu.bleu import Bleu\n","# from pycocoevalcap.meteor import Meteor\n","# from pycocoevalcap.rouge import Rouge\n","\n","\n","def compute_scores(gts, res):\n","    \"\"\"\n","    Performs the MS COCO evaluation using the Python 3 implementation (https://github.com/salaniz/pycocoevalcap)\n","\n","    :param gts: Dictionary with the image ids and their gold captions,\n","    :param res: Dictionary with the image ids ant their generated captions\n","    :print: Evaluation score (the mean of the scores of all the instances) for each measure\n","    \"\"\"\n","\n","    # Set up scorers\n","    scorers = [\n","        (Bleu(4), [\"BLEU_1\", \"BLEU_2\", \"BLEU_3\", \"BLEU_4\"]),\n","        (Meteor(), \"METEOR\"),\n","        (Rouge(), \"ROUGE_L\")\n","    ]\n","    eval_res = {}\n","    # Compute score for each metric\n","    for scorer, method in scorers:\n","        try:\n","            score, scores = scorer.compute_score(gts, res, verbose=0)\n","        except TypeError:\n","            score, scores = scorer.compute_score(gts, res)\n","        if type(method) == list:\n","            for sc, m in zip(score, method):\n","                eval_res[m] = sc\n","        else:\n","            eval_res[method] = score\n","    return eval_res\n","\n","\n","def compute_mlc(gt, pred, label_set):\n","    res_mlc = {}\n","    avg_aucroc = 0\n","    for i, label in enumerate(label_set):\n","        res_mlc['AUCROC_' + label] = roc_auc_score(gt[:, i], pred[:, i])\n","        avg_aucroc += res_mlc['AUCROC_' + label]\n","    res_mlc['AVG_AUCROC'] = avg_aucroc / len(label_set)\n","\n","    res_mlc['F1_MACRO'] = f1_score(gt, pred, average=\"macro\")\n","    res_mlc['F1_MICRO'] = f1_score(gt, pred, average=\"micro\")\n","    res_mlc['RECALL_MACRO'] = recall_score(gt, pred, average=\"macro\")\n","    res_mlc['RECALL_MICRO'] = recall_score(gt, pred, average=\"micro\")\n","    res_mlc['PRECISION_MACRO'] = precision_score(gt, pred, average=\"macro\")\n","    res_mlc['PRECISION_MICRO'] = precision_score(gt, pred, average=\"micro\")\n","\n","    return res_mlc\n","\n","\n","class MetricWrapper(object):\n","    def __init__(self, label_set):\n","        self.label_set = label_set\n","\n","    def __call__(self, gts, res, gts_mlc, res_mlc):\n","        eval_res = compute_scores(gts, res)\n","        eval_res_mlc = compute_mlc(gts_mlc, res_mlc, self.label_set)\n","\n","        eval_res.update(**eval_res_mlc)\n","        return eval_res\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"IuiAhRh890Zn","executionInfo":{"status":"ok","timestamp":1709126258061,"user_tz":-300,"elapsed":6,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"}}},"outputs":[],"source":["# optimizers\n","\n","import torch\n","from torch import optim\n","\n","\n","def build_optimizer(args, model):\n","    ve_params = list(map(id, model.visual_extractor.parameters()))\n","    ed_params = filter(lambda x: id(x) not in ve_params, model.parameters())\n","    optimizer = getattr(torch.optim, args['optim'])(\n","        [{'params': model.visual_extractor.parameters(), 'lr': args['lr_ve']},\n","         {'params': ed_params, 'lr': args['lr_ed']}],\n","        betas=args['adam_betas'],\n","        eps=args['adam_eps'],\n","        weight_decay=args['weight_decay'],\n","        amsgrad=args['amsgrad']\n","    )\n","    return optimizer\n","\n","\n","def build_lr_scheduler(args, optimizer):\n","    lr_scheduler = getattr(torch.optim.lr_scheduler, args['lr_scheduler'])(optimizer, args['step_size'], args['gamma'])\n","    return lr_scheduler\n","\n","\n","def set_lr(optimizer, lr):\n","    for group in optimizer.param_groups:\n","        group['lr'] = lr\n","\n","\n","def get_lr(optimizer):\n","    for group in optimizer.param_groups:\n","        return group['lr']\n","\n","\n","class NoamOpt(object):\n","    \"Optim wrapper that implements rate.\"\n","\n","    def __init__(self, model_size, factor, warmup, optimizer):\n","        self.optimizer = optimizer\n","        self._step = 0\n","        self.warmup = warmup\n","        self.factor = factor\n","        self.model_size = model_size\n","        self._rate = 0\n","\n","    def step(self):\n","        \"Update parameters and rate\"\n","        self._step += 1\n","        rate = self.rate()\n","        for p in self.optimizer.param_groups:\n","            p['lr'] = rate\n","        self._rate = rate\n","        self.optimizer.step()\n","\n","    def rate(self, step=None):\n","        \"Implement `lrate` above\"\n","        if step is None:\n","            step = self._step\n","        return self.factor * \\\n","               (self.model_size ** (-0.5) *\n","                min(step ** (-0.5), step * self.warmup ** (-1.5)))\n","\n","    def __getattr__(self, name):\n","        return getattr(self.optimizer, name)\n","\n","    def state_dict(self):\n","        state_dict = self.optimizer.state_dict()\n","        state_dict['_step'] = self._step\n","        return state_dict\n","\n","    def load_state_dict(self, state_dict):\n","        if '_step' in state_dict:\n","            self._step = state_dict['_step']\n","            del state_dict['_step']\n","        self.optimizer.load_state_dict(state_dict)\n","\n","\n","def get_std_opt(model, optim_func='adam', factor=1, warmup=2000):\n","    optim_func = dict(Adam=torch.optim.Adam,\n","                      AdamW=torch.optim.AdamW)[optim_func]\n","    return NoamOpt(model.d_model, factor, warmup,\n","                   optim_func(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n","\n","\n","def build_noamopt_optimizer(args, model):\n","    ve_optimizer = getattr(torch.optim, args['optim'])(\n","        model.visual_extractor.parameters(),\n","        lr=0,\n","        betas=args['adam_betas'],\n","        eps=args['adam_eps'],\n","        weight_decay=args['weight_decay'],\n","        amsgrad=args['amsgrad']\n","    )\n","    ed_optimizer = get_std_opt(model.encoder_decoder, optim_func=args['optim'], factor=args['noamopt_factor'],\n","                               warmup=args['noamopt_warmup'])\n","    return ve_optimizer, ed_optimizer\n","\n","\n","class ReduceLROnPlateau(object):\n","    \"Optim wrapper that implements rate.\"\n","\n","    def __init__(self, optimizer, mode='min', factor=0.1, patience=10, verbose=False, threshold=0.0001,\n","                 threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08):\n","        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode=mode, factor=factor,\n","                                                              patience=patience, verbose=verbose, threshold=threshold,\n","                                                              threshold_mode=threshold_mode, cooldown=cooldown,\n","                                                              min_lr=min_lr, eps=eps)\n","        self.optimizer = optimizer\n","        self.current_lr = get_lr(optimizer)\n","\n","    def step(self):\n","        \"Update parameters and rate\"\n","        self.optimizer.step()\n","\n","    def scheduler_step(self, val):\n","        self.scheduler.step(val)\n","        self.current_lr = get_lr(self.optimizer)\n","\n","    def state_dict(self):\n","        return {'current_lr': self.current_lr,\n","                'scheduler_state_dict': self.scheduler.state_dict(),\n","                'optimizer_state_dict': self.optimizer.state_dict()}\n","\n","    def load_state_dict(self, state_dict):\n","        if 'current_lr' not in state_dict:\n","            # it's normal optimizer\n","            self.optimizer.load_state_dict(state_dict)\n","            set_lr(self.optimizer, self.current_lr)  # use the lr fromt the option\n","        else:\n","            # it's a schduler\n","            self.current_lr = state_dict['current_lr']\n","            self.scheduler.load_state_dict(state_dict['scheduler_state_dict'])\n","            self.optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n","            # current_lr is actually useless in this case\n","\n","    def rate(self, step=None):\n","        \"Implement `lrate` above\"\n","        if step is None:\n","            step = self._step\n","        return self.factor * \\\n","               (self.model_size ** (-0.5) *\n","                min(step ** (-0.5), step * self.warmup ** (-1.5)))\n","\n","    def __getattr__(self, name):\n","        return getattr(self.optimizer, name)\n","\n","\n","def build_plateau_optimizer(args, model):\n","    ve_optimizer = getattr(torch.optim, args['optim'])(\n","        model.visual_extractor.parameters(),\n","        lr=args['lr_ve'],\n","        betas=args['adam_betas'],\n","        eps=args['adam_eps'],\n","        weight_decay=args['weight_decay'],\n","        amsgrad=args['amsgrad']\n","    )\n","    ve_optimizer = ReduceLROnPlateau(ve_optimizer,\n","                                     factor=args['reduce_on_plateau_factor'],\n","                                     patience=args['reduce_on_plateau_patience'])\n","    ed_optimizer = getattr(torch.optim, args['optim'])(\n","        model.encoder_decoder.parameters(),\n","        lr=args['lr_ed'],\n","        betas=args['adam_betas'],\n","        eps=args['adam_eps'],\n","        weight_decay=args['weight_decay'],\n","        amsgrad=args['amsgrad']\n","    )\n","    ed_optimizer = ReduceLROnPlateau(ed_optimizer,\n","                                     factor=args['reduce_on_plateau_factor'],\n","                                     patience=args['reduce_on_plateau_patience'])\n","\n","    return ve_optimizer, ed_optimizer\n"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"73Msdavu981Z","executionInfo":{"status":"ok","timestamp":1709126261782,"user_tz":-300,"elapsed":5,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"}}},"outputs":[],"source":["# tokenizer.py\n","\n","import json\n","import re\n","from collections import Counter\n","\n","\n","class Tokenizer(object):\n","    def __init__(self, args):\n","        self.ann_path = args['ann_path']\n","        self.threshold = args['threshold']\n","        self.dataset_name = args['dataset_name']\n","        if self.dataset_name == 'iu_xray':\n","            self.clean_report = self.clean_report_iu_xray\n","        else:\n","            self.clean_report = self.clean_report_mimic_cxr\n","        self.ann = json.loads(open(self.ann_path, 'r').read())\n","        self.token2idx, self.idx2token = self.create_vocabulary()\n","\n","    def create_vocabulary(self):\n","        total_tokens = []\n","\n","        for example in self.ann['train']:\n","            tokens = self.clean_report(example['report']).split()\n","            for token in tokens:\n","                total_tokens.append(token)\n","\n","        counter = Counter(total_tokens)\n","        vocab = [k for k, v in counter.items() if v >= self.threshold] + ['<unk>']\n","        vocab.sort()\n","        token2idx, idx2token = {}, {}\n","        for idx, token in enumerate(vocab):\n","            token2idx[token] = idx + 1\n","            idx2token[idx + 1] = token\n","        return token2idx, idx2token\n","\n","    def clean_report_iu_xray(self, report):\n","        report_cleaner = lambda t: t.replace('..', '.').replace('..', '.').replace('..', '.').replace('1. ', '') \\\n","            .replace('. 2. ', '. ').replace('. 3. ', '. ').replace('. 4. ', '. ').replace('. 5. ', '. ') \\\n","            .replace(' 2. ', '. ').replace(' 3. ', '. ').replace(' 4. ', '. ').replace(' 5. ', '. ') \\\n","            .strip().lower().split('. ')\n","        sent_cleaner = lambda t: re.sub('[.,?;*!%^&_+():-\\[\\]{}]', '', t.replace('\"', '').replace('/', '').\n","                                        replace('\\\\', '').replace(\"'\", '').strip().lower())\n","        tokens = [sent_cleaner(sent) for sent in report_cleaner(report) if sent_cleaner(sent) != []]\n","        report = ' . '.join(tokens) + ' .'\n","        return report\n","\n","    def clean_report_mimic_cxr(self, report):\n","        report_cleaner = lambda t: t.replace('\\n', ' ').replace('__', '_').replace('__', '_').replace('__', '_') \\\n","            .replace('__', '_').replace('__', '_').replace('__', '_').replace('__', '_').replace('  ', ' ') \\\n","            .replace('  ', ' ').replace('  ', ' ').replace('  ', ' ').replace('  ', ' ').replace('  ', ' ') \\\n","            .replace('..', '.').replace('..', '.').replace('..', '.').replace('..', '.').replace('..', '.') \\\n","            .replace('..', '.').replace('..', '.').replace('..', '.').replace('1. ', '').replace('. 2. ', '. ') \\\n","            .replace('. 3. ', '. ').replace('. 4. ', '. ').replace('. 5. ', '. ').replace(' 2. ', '. ') \\\n","            .replace(' 3. ', '. ').replace(' 4. ', '. ').replace(' 5. ', '. ') \\\n","            .strip().lower().split('. ')\n","        sent_cleaner = lambda t: re.sub('[.,?;*!%^&_+():-\\[\\]{}]', '', t.replace('\"', '').replace('/', '')\n","                                        .replace('\\\\', '').replace(\"'\", '').strip().lower())\n","        tokens = [sent_cleaner(sent) for sent in report_cleaner(report) if sent_cleaner(sent) != []]\n","        report = ' . '.join(tokens) + ' .'\n","        return report\n","\n","    def get_token_by_id(self, id):\n","        return self.idx2token[id]\n","\n","    def get_id_by_token(self, token):\n","        if token not in self.token2idx:\n","            return self.token2idx['<unk>']\n","        return self.token2idx[token]\n","\n","    def get_vocab_size(self):\n","        return len(self.token2idx)\n","\n","    def __call__(self, report):\n","        tokens = self.clean_report(report).split()\n","        ids = []\n","        for token in tokens:\n","            ids.append(self.get_id_by_token(token))\n","        ids = [0] + ids + [0]\n","        return ids\n","\n","    def decode(self, ids):\n","        txt = ''\n","        for i, idx in enumerate(ids):\n","            if idx > 0:\n","                if i >= 1:\n","                    txt += ' '\n","                txt += self.idx2token[idx]\n","            else:\n","                break\n","        return txt\n","\n","    def decode_batch(self, ids_batch):\n","        out = []\n","        for ids in ids_batch:\n","            out.append(self.decode(ids))\n","        return out\n"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"C7HFSZ_0-FeU","executionInfo":{"status":"ok","timestamp":1709126263413,"user_tz":-300,"elapsed":6,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"}}},"outputs":[],"source":["# trainer.py\n","\n","import logging\n","import os\n","from abc import abstractmethod\n","\n","import torch\n","from numpy import inf\n","\n","\n","class BaseTrainer(object):\n","    def __init__(self, model, criterion, metric_ftns, optimizer, args, lr_scheduler):\n","        self.args = args\n","\n","        logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n","                            datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n","        self.logger = logging.getLogger(__name__)\n","\n","        # setup GPU device if available, move model into configured device\n","        self.device, device_ids = self._prepare_device(args['n_gpu'])\n","        self.model = model.to(self.device)\n","        if len(device_ids) > 1:\n","            self.model = torch.nn.DataParallel(model, device_ids=device_ids)\n","\n","        self.criterion = criterion\n","        self.metric_ftns = metric_ftns\n","        self.optimizer = optimizer\n","        self.lr_scheduler = lr_scheduler\n","\n","        self.epochs = self.args['epochs']\n","        self.save_period = self.args['save_period']\n","\n","        self.mnt_mode = args['monitor_mode']\n","        self.mnt_metric = 'val_' + args['monitor_metric']\n","        self.mnt_metric_test = 'test_' + args['monitor_metric']\n","        assert self.mnt_mode in ['min', 'max']\n","\n","        self.mnt_best = inf if self.mnt_mode == 'min' else -inf\n","        self.early_stop = getattr(self.args, 'early_stop', inf)\n","\n","        self.start_epoch = 1\n","        self.checkpoint_dir = args['save_dir']\n","\n","        self.best_recorder = {'val': {self.mnt_metric: self.mnt_best},\n","                              'test': {self.mnt_metric_test: self.mnt_best}}\n","\n","        if not os.path.exists(self.checkpoint_dir):\n","            os.makedirs(self.checkpoint_dir)\n","\n","        if args['resume'] is not None:\n","            self._resume_checkpoint(args['resume'])\n","\n","    @abstractmethod\n","    def _train_epoch(self, epoch):\n","        raise NotImplementedError\n","\n","    def train(self):\n","        not_improved_count = 0\n","\n","        for epoch in range(self.start_epoch, self.epochs + 1):\n","\n","            result = self._train_epoch(epoch)\n","\n","            # save logged informations into log dict\n","            log = {'epoch': epoch}\n","            log.update(result)\n","            self._record_best(log)\n","\n","            # print logged informations to the screen\n","            for key, value in log.items():\n","                self.logger.info('\\t{:15s}: {}'.format(str(key), value))\n","                print('\\t{:15s}: {}'.format(str(key), value))\n","\n","            # evaluate model performance according to configured metric, save best checkpoint as model_best\n","            best = False\n","            if self.mnt_mode != 'off':\n","                try:\n","                    # check whether model performance improved or not, according to specified metric(mnt_metric)\n","                    improved = (self.mnt_mode == 'min' and log[self.mnt_metric] <= self.mnt_best) or \\\n","                               (self.mnt_mode == 'max' and log[self.mnt_metric] >= self.mnt_best)\n","                except KeyError:\n","                    self.logger.warning(\n","                        \"Warning: Metric '{}' is not found. \" \"Model performance monitoring is disabled.\".format(\n","                            self.mnt_metric))\n","                    self.mnt_mode = 'off'\n","                    improved = False\n","\n","                if improved:\n","                    self.mnt_best = log[self.mnt_metric]\n","                    not_improved_count = 0\n","                    best = True\n","                else:\n","                    not_improved_count += 1\n","\n","                if not_improved_count > self.early_stop:\n","                    self.logger.info(\"Validation performance didn\\'t improve for {} epochs. \" \"Training stops.\".format(\n","                        self.early_stop))\n","                    print(\"Validation performance didn\\'t improve for {} epochs. \" \"Training stops.\".format(\n","                        self.early_stop))\n","                    break\n","\n","            if epoch % self.save_period == 0:\n","                self._save_checkpoint(epoch, save_best=best)\n","\n","    def _record_best(self, log):\n","        improved_val = (self.mnt_mode == 'min' and log[self.mnt_metric] <= self.best_recorder['val'][\n","            self.mnt_metric]) or \\\n","                       (self.mnt_mode == 'max' and log[self.mnt_metric] >= self.best_recorder['val'][self.mnt_metric])\n","        if improved_val:\n","            self.best_recorder['val'].update(log)\n","\n","        improved_test = (self.mnt_mode == 'min' and log[self.mnt_metric_test] <= self.best_recorder['test'][\n","            self.mnt_metric_test]) or \\\n","                        (self.mnt_mode == 'max' and log[self.mnt_metric_test] >= self.best_recorder['test'][\n","                            self.mnt_metric_test])\n","        if improved_test:\n","            self.best_recorder['test'].update(log)\n","\n","    def _print_best(self):\n","        self.logger.info('Best results (w.r.t {}) in validation set:'.format(self.args['monitor_metric']))\n","        for key, value in self.best_recorder['val'].items():\n","            self.logger.info('\\t{:15s}: {}'.format(str(key), value))\n","\n","        self.logger.info('Best results (w.r.t {}) in test set:'.format(self.args['monitor_metric']))\n","        for key, value in self.best_recorder['test'].items():\n","            self.logger.info('\\t{:15s}: {}'.format(str(key), value))\n","\n","    def _prepare_device(self, n_gpu_use):\n","        n_gpu = torch.cuda.device_count()\n","        if n_gpu_use > 0 and n_gpu == 0:\n","            self.logger.warning(\n","                \"Warning: There\\'s no GPU available on this machine,\" \"training will be performed on CPU.\")\n","            n_gpu_use = 0\n","        if n_gpu_use > n_gpu:\n","            self.logger.warning(\n","                \"Warning: The number of GPU\\'s configured to use is {}, but only {} are available \" \"on this machine.\".format(\n","                    n_gpu_use, n_gpu))\n","            n_gpu_use = n_gpu\n","        device = torch.device('cuda:0' if n_gpu_use > 0 else 'cpu')\n","        list_ids = list(range(n_gpu_use))\n","        return device, list_ids\n","\n","    def _save_checkpoint(self, epoch, save_best=False):\n","        state = {\n","            'epoch': epoch,\n","            'state_dict': self.model.state_dict(),\n","            'optimizer': self.optimizer.state_dict(),\n","            'monitor_best': self.mnt_best\n","        }\n","        filename = os.path.join(self.checkpoint_dir, 'current_checkpoint.pth')\n","        torch.save(state, filename)\n","        self.logger.info(\"Saving checkpoint: {} ...\".format(filename))\n","        print(\"Saving checkpoint: {} ...\".format(filename))\n","        if save_best:\n","            best_path = os.path.join(self.checkpoint_dir, 'model_best.pth')\n","            torch.save(state, best_path)\n","            self.logger.info(\"Saving current best: model_best.pth ...\")\n","            print(\"Saving current best: model_best.pth ...\")\n","\n","    def _resume_checkpoint(self, resume_path):\n","        resume_path = str(resume_path)\n","        self.logger.info(\"Loading checkpoint: {} ...\".format(resume_path))\n","        print(\"Loading checkpoint: {} ...\".format(resume_path))\n","        checkpoint = torch.load(resume_path)\n","        self.start_epoch = checkpoint['epoch'] + 1\n","        self.mnt_best = checkpoint['monitor_best']\n","        self.model.load_state_dict(checkpoint['state_dict'])\n","        self.optimizer.load_state_dict(checkpoint['optimizer'])\n","\n","        self.logger.info(\"Checkpoint loaded. Resume training from epoch {}\".format(self.start_epoch))\n","        print(\"Checkpoint loaded. Resume training from epoch {}\".format(self.start_epoch))\n","\n","\n","class Trainer(BaseTrainer):\n","    def __init__(self, model, criterion, metric_ftns, optimizer, args, lr_scheduler, train_dataloader,\n","                 val_dataloader, test_dataloader):\n","        super(Trainer, self).__init__(model, criterion, metric_ftns, optimizer, args, lr_scheduler)\n","        self.train_dataloader = train_dataloader\n","        self.val_dataloader = val_dataloader\n","        self.test_dataloader = test_dataloader\n","\n","\n","    def _train_epoch(self, epoch):\n","\n","\n","        self.logger.info('[{}/{}] Start to train in the training set.'.format(epoch, self.epochs))\n","        print('[{}/{}] Start to train in the training set.'.format(epoch, self.epochs))\n","        train_loss = 0\n","        self.model.train()\n","        for batch_idx, (images_id, images, reports_ids, reports_masks) in enumerate(self.train_dataloader):\n","\n","            images, reports_ids, reports_masks = images.to(self.device), reports_ids.to(self.device), \\\n","                                                 reports_masks.to(self.device)\n","            output = self.model(images, reports_ids, mode='train')\n","            loss = self.criterion(output, reports_ids, reports_masks)\n","            train_loss += loss.item()\n","            self.optimizer.zero_grad()\n","            loss.backward()\n","            self.optimizer.step()\n","            if batch_idx % self.args['log_period'] == 0:\n","                self.logger.info('[{}/{}] Step: {}/{}, Training Loss: {:.5f}.'\n","                                 .format(epoch, self.epochs, batch_idx, len(self.train_dataloader),\n","                                         train_loss / (batch_idx + 1)))\n","                print('[{}/{}] Step: {}/{}, Training Loss: {:.5f}.'\n","                                 .format(epoch, self.epochs, batch_idx, len(self.train_dataloader),\n","                                         train_loss / (batch_idx + 1)))\n","\n","        log = {'train_loss': train_loss / len(self.train_dataloader)}\n","\n","        self.logger.info('[{}/{}] Start to evaluate in the validation set.'.format(epoch, self.epochs))\n","        self.model.eval()\n","        with torch.no_grad():\n","            val_gts, val_res = [], []\n","            for batch_idx, (images_id, images, reports_ids, reports_masks) in enumerate(self.val_dataloader):\n","                images, reports_ids, reports_masks = images.to(self.device), reports_ids.to(\n","                    self.device), reports_masks.to(self.device)\n","\n","                output, _ = self.model(images, mode='sample')\n","                reports = self.model.tokenizer.decode_batch(output.cpu().numpy())\n","                ground_truths = self.model.tokenizer.decode_batch(reports_ids[:, 1:].cpu().numpy())\n","                val_res.extend(reports)\n","                val_gts.extend(ground_truths)\n","\n","            val_met = self.metric_ftns({i: [gt] for i, gt in enumerate(val_gts)},\n","                                       {i: [re] for i, re in enumerate(val_res)})\n","            log.update(**{'val_' + k: v for k, v in val_met.items()})\n","\n","        self.logger.info('[{}/{}] Start to evaluate in the test set.'.format(epoch, self.epochs))\n","        self.model.eval()\n","        with torch.no_grad():\n","            test_gts, test_res = [], []\n","            for batch_idx, (images_id, images, reports_ids, reports_masks) in enumerate(self.test_dataloader):\n","                images, reports_ids, reports_masks = images.to(self.device), reports_ids.to(\n","                    self.device), reports_masks.to(self.device)\n","                output, _ = self.model(images, mode='sample')\n","                reports = self.model.tokenizer.decode_batch(output.cpu().numpy())\n","                ground_truths = self.model.tokenizer.decode_batch(reports_ids[:, 1:].cpu().numpy())\n","                test_res.extend(reports)\n","                test_gts.extend(ground_truths)\n","\n","            test_met = self.metric_ftns({i: [gt] for i, gt in enumerate(test_gts)},\n","                                        {i: [re] for i, re in enumerate(test_res)})\n","            log.update(**{'test_' + k: v for k, v in test_met.items()})\n","\n","        self.lr_scheduler.step()\n","\n","        return log\n"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"oYRJovXO31LN","executionInfo":{"status":"ok","timestamp":1709126265960,"user_tz":-300,"elapsed":5,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"}}},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1703920458470,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"},"user_tz":-270},"id":"ba27-xNXzORC","outputId":"9c130b9c-d27b-4c00-e38c-285e7f754dbe"},"outputs":[{"name":"stdout","output_type":"stream","text":["['AlexNet', 'AlexNet_Weights', 'ConvNeXt', 'ConvNeXt_Base_Weights', 'ConvNeXt_Large_Weights', 'ConvNeXt_Small_Weights', 'ConvNeXt_Tiny_Weights', 'DenseNet', 'DenseNet121_Weights', 'DenseNet161_Weights', 'DenseNet169_Weights', 'DenseNet201_Weights', 'EfficientNet', 'EfficientNet_B0_Weights', 'EfficientNet_B1_Weights', 'EfficientNet_B2_Weights', 'EfficientNet_B3_Weights', 'EfficientNet_B4_Weights', 'EfficientNet_B5_Weights', 'EfficientNet_B6_Weights', 'EfficientNet_B7_Weights', 'EfficientNet_V2_L_Weights', 'EfficientNet_V2_M_Weights', 'EfficientNet_V2_S_Weights', 'GoogLeNet', 'GoogLeNetOutputs', 'GoogLeNet_Weights', 'Inception3', 'InceptionOutputs', 'Inception_V3_Weights', 'MNASNet', 'MNASNet0_5_Weights', 'MNASNet0_75_Weights', 'MNASNet1_0_Weights', 'MNASNet1_3_Weights', 'MaxVit', 'MaxVit_T_Weights', 'MobileNetV2', 'MobileNetV3', 'MobileNet_V2_Weights', 'MobileNet_V3_Large_Weights', 'MobileNet_V3_Small_Weights', 'RegNet', 'RegNet_X_16GF_Weights', 'RegNet_X_1_6GF_Weights', 'RegNet_X_32GF_Weights', 'RegNet_X_3_2GF_Weights', 'RegNet_X_400MF_Weights', 'RegNet_X_800MF_Weights', 'RegNet_X_8GF_Weights', 'RegNet_Y_128GF_Weights', 'RegNet_Y_16GF_Weights', 'RegNet_Y_1_6GF_Weights', 'RegNet_Y_32GF_Weights', 'RegNet_Y_3_2GF_Weights', 'RegNet_Y_400MF_Weights', 'RegNet_Y_800MF_Weights', 'RegNet_Y_8GF_Weights', 'ResNeXt101_32X8D_Weights', 'ResNeXt101_64X4D_Weights', 'ResNeXt50_32X4D_Weights', 'ResNet', 'ResNet101_Weights', 'ResNet152_Weights', 'ResNet18_Weights', 'ResNet34_Weights', 'ResNet50_Weights', 'ShuffleNetV2', 'ShuffleNet_V2_X0_5_Weights', 'ShuffleNet_V2_X1_0_Weights', 'ShuffleNet_V2_X1_5_Weights', 'ShuffleNet_V2_X2_0_Weights', 'SqueezeNet', 'SqueezeNet1_0_Weights', 'SqueezeNet1_1_Weights', 'SwinTransformer', 'Swin_B_Weights', 'Swin_S_Weights', 'Swin_T_Weights', 'Swin_V2_B_Weights', 'Swin_V2_S_Weights', 'Swin_V2_T_Weights', 'VGG', 'VGG11_BN_Weights', 'VGG11_Weights', 'VGG13_BN_Weights', 'VGG13_Weights', 'VGG16_BN_Weights', 'VGG16_Weights', 'VGG19_BN_Weights', 'VGG19_Weights', 'ViT_B_16_Weights', 'ViT_B_32_Weights', 'ViT_H_14_Weights', 'ViT_L_16_Weights', 'ViT_L_32_Weights', 'VisionTransformer', 'Weights', 'WeightsEnum', 'Wide_ResNet101_2_Weights', 'Wide_ResNet50_2_Weights', '_GoogLeNetOutputs', '_InceptionOutputs', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_api', '_meta', '_utils', 'alexnet', 'convnext', 'convnext_base', 'convnext_large', 'convnext_small', 'convnext_tiny', 'densenet', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'detection', 'efficientnet', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'efficientnet_v2_l', 'efficientnet_v2_m', 'efficientnet_v2_s', 'get_model', 'get_model_builder', 'get_model_weights', 'get_weight', 'googlenet', 'inception', 'inception_v3', 'list_models', 'maxvit', 'maxvit_t', 'mnasnet', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet', 'mobilenet_v2', 'mobilenet_v3_large', 'mobilenet_v3_small', 'mobilenetv2', 'mobilenetv3', 'optical_flow', 'quantization', 'regnet', 'regnet_x_16gf', 'regnet_x_1_6gf', 'regnet_x_32gf', 'regnet_x_3_2gf', 'regnet_x_400mf', 'regnet_x_800mf', 'regnet_x_8gf', 'regnet_y_128gf', 'regnet_y_16gf', 'regnet_y_1_6gf', 'regnet_y_32gf', 'regnet_y_3_2gf', 'regnet_y_400mf', 'regnet_y_800mf', 'regnet_y_8gf', 'resnet', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext101_64x4d', 'resnext50_32x4d', 'segmentation', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'shufflenetv2', 'squeezenet', 'squeezenet1_0', 'squeezenet1_1', 'swin_b', 'swin_s', 'swin_t', 'swin_transformer', 'swin_v2_b', 'swin_v2_s', 'swin_v2_t', 'vgg', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'video', 'vision_transformer', 'vit_b_16', 'vit_b_32', 'vit_h_14', 'vit_l_16', 'vit_l_32', 'wide_resnet101_2', 'wide_resnet50_2']\n"]}],"source":["# For PyTorch\n","import torchvision.models as models\n","print(dir(models))"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6758,"status":"ok","timestamp":1709126336055,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"},"user_tz":-300},"id":"c2W6R702HtJT","outputId":"30cb9167-001d-455a-d6ea-4e702793a895"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting timm\n","  Downloading timm-0.9.16-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.1.0+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.16.0+cu121)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.3)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.9.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (23.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.3)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.25.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n","Installing collected packages: timm\n","Successfully installed timm-0.9.16\n"]}],"source":["!pip install timm"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16835,"status":"ok","timestamp":1704271406682,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"},"user_tz":-300},"id":"h9LeEHwu6z18","outputId":"45d3bbba-04d7-466d-8c2d-7ea797d63454"},"outputs":[{"name":"stdout","output_type":"stream","text":["['bat_resnext26ts', 'beit_base_patch16_224', 'beit_base_patch16_384', 'beit_large_patch16_224', 'beit_large_patch16_384', 'beit_large_patch16_512', 'beitv2_base_patch16_224', 'beitv2_large_patch16_224', 'botnet26t_256', 'botnet50ts_256', 'caformer_b36', 'caformer_m36', 'caformer_s18', 'caformer_s36', 'cait_m36_384', 'cait_m48_448', 'cait_s24_224', 'cait_s24_384', 'cait_s36_384', 'cait_xs24_384', 'cait_xxs24_224', 'cait_xxs24_384', 'cait_xxs36_224', 'cait_xxs36_384', 'coat_lite_medium', 'coat_lite_medium_384', 'coat_lite_mini', 'coat_lite_small', 'coat_lite_tiny', 'coat_mini', 'coat_small', 'coat_tiny', 'coatnet_0_224', 'coatnet_0_rw_224', 'coatnet_1_224', 'coatnet_1_rw_224', 'coatnet_2_224', 'coatnet_2_rw_224', 'coatnet_3_224', 'coatnet_3_rw_224', 'coatnet_4_224', 'coatnet_5_224', 'coatnet_bn_0_rw_224', 'coatnet_nano_cc_224', 'coatnet_nano_rw_224', 'coatnet_pico_rw_224', 'coatnet_rmlp_0_rw_224', 'coatnet_rmlp_1_rw2_224', 'coatnet_rmlp_1_rw_224', 'coatnet_rmlp_2_rw_224', 'coatnet_rmlp_2_rw_384', 'coatnet_rmlp_3_rw_224', 'coatnet_rmlp_nano_rw_224', 'coatnext_nano_rw_224', 'convformer_b36', 'convformer_m36', 'convformer_s18', 'convformer_s36', 'convit_base', 'convit_small', 'convit_tiny', 'convmixer_768_32', 'convmixer_1024_20_ks9_p14', 'convmixer_1536_20', 'convnext_atto', 'convnext_atto_ols', 'convnext_base', 'convnext_femto', 'convnext_femto_ols', 'convnext_large', 'convnext_large_mlp', 'convnext_nano', 'convnext_nano_ols', 'convnext_pico', 'convnext_pico_ols', 'convnext_small', 'convnext_tiny', 'convnext_tiny_hnf', 'convnext_xlarge', 'convnext_xxlarge', 'convnextv2_atto', 'convnextv2_base', 'convnextv2_femto', 'convnextv2_huge', 'convnextv2_large', 'convnextv2_nano', 'convnextv2_pico', 'convnextv2_small', 'convnextv2_tiny', 'crossvit_9_240', 'crossvit_9_dagger_240', 'crossvit_15_240', 'crossvit_15_dagger_240', 'crossvit_15_dagger_408', 'crossvit_18_240', 'crossvit_18_dagger_240', 'crossvit_18_dagger_408', 'crossvit_base_240', 'crossvit_small_240', 'crossvit_tiny_240', 'cs3darknet_focus_l', 'cs3darknet_focus_m', 'cs3darknet_focus_s', 'cs3darknet_focus_x', 'cs3darknet_l', 'cs3darknet_m', 'cs3darknet_s', 'cs3darknet_x', 'cs3edgenet_x', 'cs3se_edgenet_x', 'cs3sedarknet_l', 'cs3sedarknet_x', 'cs3sedarknet_xdw', 'cspdarknet53', 'cspresnet50', 'cspresnet50d', 'cspresnet50w', 'cspresnext50', 'darknet17', 'darknet21', 'darknet53', 'darknetaa53', 'davit_base', 'davit_giant', 'davit_huge', 'davit_large', 'davit_small', 'davit_tiny', 'deit3_base_patch16_224', 'deit3_base_patch16_384', 'deit3_huge_patch14_224', 'deit3_large_patch16_224', 'deit3_large_patch16_384', 'deit3_medium_patch16_224', 'deit3_small_patch16_224', 'deit3_small_patch16_384', 'deit_base_distilled_patch16_224', 'deit_base_distilled_patch16_384', 'deit_base_patch16_224', 'deit_base_patch16_384', 'deit_small_distilled_patch16_224', 'deit_small_patch16_224', 'deit_tiny_distilled_patch16_224', 'deit_tiny_patch16_224', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'densenet264d', 'densenetblur121d', 'dla34', 'dla46_c', 'dla46x_c', 'dla60', 'dla60_res2net', 'dla60_res2next', 'dla60x', 'dla60x_c', 'dla102', 'dla102x', 'dla102x2', 'dla169', 'dm_nfnet_f0', 'dm_nfnet_f1', 'dm_nfnet_f2', 'dm_nfnet_f3', 'dm_nfnet_f4', 'dm_nfnet_f5', 'dm_nfnet_f6', 'dpn48b', 'dpn68', 'dpn68b', 'dpn92', 'dpn98', 'dpn107', 'dpn131', 'eca_botnext26ts_256', 'eca_halonext26ts', 'eca_nfnet_l0', 'eca_nfnet_l1', 'eca_nfnet_l2', 'eca_nfnet_l3', 'eca_resnet33ts', 'eca_resnext26ts', 'eca_vovnet39b', 'ecaresnet26t', 'ecaresnet50d', 'ecaresnet50d_pruned', 'ecaresnet50t', 'ecaresnet101d', 'ecaresnet101d_pruned', 'ecaresnet200d', 'ecaresnet269d', 'ecaresnetlight', 'ecaresnext26t_32x4d', 'ecaresnext50t_32x4d', 'edgenext_base', 'edgenext_small', 'edgenext_small_rw', 'edgenext_x_small', 'edgenext_xx_small', 'efficientformer_l1', 'efficientformer_l3', 'efficientformer_l7', 'efficientformerv2_l', 'efficientformerv2_s0', 'efficientformerv2_s1', 'efficientformerv2_s2', 'efficientnet_b0', 'efficientnet_b0_g8_gn', 'efficientnet_b0_g16_evos', 'efficientnet_b0_gn', 'efficientnet_b1', 'efficientnet_b1_pruned', 'efficientnet_b2', 'efficientnet_b2_pruned', 'efficientnet_b3', 'efficientnet_b3_g8_gn', 'efficientnet_b3_gn', 'efficientnet_b3_pruned', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'efficientnet_b8', 'efficientnet_cc_b0_4e', 'efficientnet_cc_b0_8e', 'efficientnet_cc_b1_8e', 'efficientnet_el', 'efficientnet_el_pruned', 'efficientnet_em', 'efficientnet_es', 'efficientnet_es_pruned', 'efficientnet_l2', 'efficientnet_lite0', 'efficientnet_lite1', 'efficientnet_lite2', 'efficientnet_lite3', 'efficientnet_lite4', 'efficientnetv2_l', 'efficientnetv2_m', 'efficientnetv2_rw_m', 'efficientnetv2_rw_s', 'efficientnetv2_rw_t', 'efficientnetv2_s', 'efficientnetv2_xl', 'efficientvit_b0', 'efficientvit_b1', 'efficientvit_b2', 'efficientvit_b3', 'efficientvit_l1', 'efficientvit_l2', 'efficientvit_l3', 'efficientvit_m0', 'efficientvit_m1', 'efficientvit_m2', 'efficientvit_m3', 'efficientvit_m4', 'efficientvit_m5', 'ese_vovnet19b_dw', 'ese_vovnet19b_slim', 'ese_vovnet19b_slim_dw', 'ese_vovnet39b', 'ese_vovnet39b_evos', 'ese_vovnet57b', 'ese_vovnet99b', 'eva02_base_patch14_224', 'eva02_base_patch14_448', 'eva02_base_patch16_clip_224', 'eva02_enormous_patch14_clip_224', 'eva02_large_patch14_224', 'eva02_large_patch14_448', 'eva02_large_patch14_clip_224', 'eva02_large_patch14_clip_336', 'eva02_small_patch14_224', 'eva02_small_patch14_336', 'eva02_tiny_patch14_224', 'eva02_tiny_patch14_336', 'eva_giant_patch14_224', 'eva_giant_patch14_336', 'eva_giant_patch14_560', 'eva_giant_patch14_clip_224', 'eva_large_patch14_196', 'eva_large_patch14_336', 'fastvit_ma36', 'fastvit_s12', 'fastvit_sa12', 'fastvit_sa24', 'fastvit_sa36', 'fastvit_t8', 'fastvit_t12', 'fbnetc_100', 'fbnetv3_b', 'fbnetv3_d', 'fbnetv3_g', 'flexivit_base', 'flexivit_large', 'flexivit_small', 'focalnet_base_lrf', 'focalnet_base_srf', 'focalnet_huge_fl3', 'focalnet_huge_fl4', 'focalnet_large_fl3', 'focalnet_large_fl4', 'focalnet_small_lrf', 'focalnet_small_srf', 'focalnet_tiny_lrf', 'focalnet_tiny_srf', 'focalnet_xlarge_fl3', 'focalnet_xlarge_fl4', 'gc_efficientnetv2_rw_t', 'gcresnet33ts', 'gcresnet50t', 'gcresnext26ts', 'gcresnext50ts', 'gcvit_base', 'gcvit_small', 'gcvit_tiny', 'gcvit_xtiny', 'gcvit_xxtiny', 'gernet_l', 'gernet_m', 'gernet_s', 'ghostnet_050', 'ghostnet_100', 'ghostnet_130', 'ghostnetv2_100', 'ghostnetv2_130', 'ghostnetv2_160', 'gmixer_12_224', 'gmixer_24_224', 'gmlp_b16_224', 'gmlp_s16_224', 'gmlp_ti16_224', 'halo2botnet50ts_256', 'halonet26t', 'halonet50ts', 'halonet_h1', 'haloregnetz_b', 'hardcorenas_a', 'hardcorenas_b', 'hardcorenas_c', 'hardcorenas_d', 'hardcorenas_e', 'hardcorenas_f', 'hrnet_w18', 'hrnet_w18_small', 'hrnet_w18_small_v2', 'hrnet_w18_ssld', 'hrnet_w30', 'hrnet_w32', 'hrnet_w40', 'hrnet_w44', 'hrnet_w48', 'hrnet_w48_ssld', 'hrnet_w64', 'inception_next_base', 'inception_next_small', 'inception_next_tiny', 'inception_resnet_v2', 'inception_v3', 'inception_v4', 'lambda_resnet26rpt_256', 'lambda_resnet26t', 'lambda_resnet50ts', 'lamhalobotnet50ts_256', 'lcnet_035', 'lcnet_050', 'lcnet_075', 'lcnet_100', 'lcnet_150', 'legacy_senet154', 'legacy_seresnet18', 'legacy_seresnet34', 'legacy_seresnet50', 'legacy_seresnet101', 'legacy_seresnet152', 'legacy_seresnext26_32x4d', 'legacy_seresnext50_32x4d', 'legacy_seresnext101_32x4d', 'legacy_xception', 'levit_128', 'levit_128s', 'levit_192', 'levit_256', 'levit_256d', 'levit_384', 'levit_384_s8', 'levit_512', 'levit_512_s8', 'levit_512d', 'levit_conv_128', 'levit_conv_128s', 'levit_conv_192', 'levit_conv_256', 'levit_conv_256d', 'levit_conv_384', 'levit_conv_384_s8', 'levit_conv_512', 'levit_conv_512_s8', 'levit_conv_512d', 'maxvit_base_tf_224', 'maxvit_base_tf_384', 'maxvit_base_tf_512', 'maxvit_large_tf_224', 'maxvit_large_tf_384', 'maxvit_large_tf_512', 'maxvit_nano_rw_256', 'maxvit_pico_rw_256', 'maxvit_rmlp_base_rw_224', 'maxvit_rmlp_base_rw_384', 'maxvit_rmlp_nano_rw_256', 'maxvit_rmlp_pico_rw_256', 'maxvit_rmlp_small_rw_224', 'maxvit_rmlp_small_rw_256', 'maxvit_rmlp_tiny_rw_256', 'maxvit_small_tf_224', 'maxvit_small_tf_384', 'maxvit_small_tf_512', 'maxvit_tiny_pm_256', 'maxvit_tiny_rw_224', 'maxvit_tiny_rw_256', 'maxvit_tiny_tf_224', 'maxvit_tiny_tf_384', 'maxvit_tiny_tf_512', 'maxvit_xlarge_tf_224', 'maxvit_xlarge_tf_384', 'maxvit_xlarge_tf_512', 'maxxvit_rmlp_nano_rw_256', 'maxxvit_rmlp_small_rw_256', 'maxxvit_rmlp_tiny_rw_256', 'maxxvitv2_nano_rw_256', 'maxxvitv2_rmlp_base_rw_224', 'maxxvitv2_rmlp_base_rw_384', 'maxxvitv2_rmlp_large_rw_224', 'mixer_b16_224', 'mixer_b32_224', 'mixer_l16_224', 'mixer_l32_224', 'mixer_s16_224', 'mixer_s32_224', 'mixnet_l', 'mixnet_m', 'mixnet_s', 'mixnet_xl', 'mixnet_xxl', 'mnasnet_050', 'mnasnet_075', 'mnasnet_100', 'mnasnet_140', 'mnasnet_small', 'mobilenetv2_035', 'mobilenetv2_050', 'mobilenetv2_075', 'mobilenetv2_100', 'mobilenetv2_110d', 'mobilenetv2_120d', 'mobilenetv2_140', 'mobilenetv3_large_075', 'mobilenetv3_large_100', 'mobilenetv3_rw', 'mobilenetv3_small_050', 'mobilenetv3_small_075', 'mobilenetv3_small_100', 'mobileone_s0', 'mobileone_s1', 'mobileone_s2', 'mobileone_s3', 'mobileone_s4', 'mobilevit_s', 'mobilevit_xs', 'mobilevit_xxs', 'mobilevitv2_050', 'mobilevitv2_075', 'mobilevitv2_100', 'mobilevitv2_125', 'mobilevitv2_150', 'mobilevitv2_175', 'mobilevitv2_200', 'mvitv2_base', 'mvitv2_base_cls', 'mvitv2_huge_cls', 'mvitv2_large', 'mvitv2_large_cls', 'mvitv2_small', 'mvitv2_small_cls', 'mvitv2_tiny', 'nasnetalarge', 'nest_base', 'nest_base_jx', 'nest_small', 'nest_small_jx', 'nest_tiny', 'nest_tiny_jx', 'nf_ecaresnet26', 'nf_ecaresnet50', 'nf_ecaresnet101', 'nf_regnet_b0', 'nf_regnet_b1', 'nf_regnet_b2', 'nf_regnet_b3', 'nf_regnet_b4', 'nf_regnet_b5', 'nf_resnet26', 'nf_resnet50', 'nf_resnet101', 'nf_seresnet26', 'nf_seresnet50', 'nf_seresnet101', 'nfnet_f0', 'nfnet_f1', 'nfnet_f2', 'nfnet_f3', 'nfnet_f4', 'nfnet_f5', 'nfnet_f6', 'nfnet_f7', 'nfnet_l0', 'pit_b_224', 'pit_b_distilled_224', 'pit_s_224', 'pit_s_distilled_224', 'pit_ti_224', 'pit_ti_distilled_224', 'pit_xs_224', 'pit_xs_distilled_224', 'pnasnet5large', 'poolformer_m36', 'poolformer_m48', 'poolformer_s12', 'poolformer_s24', 'poolformer_s36', 'poolformerv2_m36', 'poolformerv2_m48', 'poolformerv2_s12', 'poolformerv2_s24', 'poolformerv2_s36', 'pvt_v2_b0', 'pvt_v2_b1', 'pvt_v2_b2', 'pvt_v2_b2_li', 'pvt_v2_b3', 'pvt_v2_b4', 'pvt_v2_b5', 'regnetv_040', 'regnetv_064', 'regnetx_002', 'regnetx_004', 'regnetx_004_tv', 'regnetx_006', 'regnetx_008', 'regnetx_016', 'regnetx_032', 'regnetx_040', 'regnetx_064', 'regnetx_080', 'regnetx_120', 'regnetx_160', 'regnetx_320', 'regnety_002', 'regnety_004', 'regnety_006', 'regnety_008', 'regnety_008_tv', 'regnety_016', 'regnety_032', 'regnety_040', 'regnety_040_sgn', 'regnety_064', 'regnety_080', 'regnety_080_tv', 'regnety_120', 'regnety_160', 'regnety_320', 'regnety_640', 'regnety_1280', 'regnety_2560', 'regnetz_005', 'regnetz_040', 'regnetz_040_h', 'regnetz_b16', 'regnetz_b16_evos', 'regnetz_c16', 'regnetz_c16_evos', 'regnetz_d8', 'regnetz_d8_evos', 'regnetz_d32', 'regnetz_e8', 'repghostnet_050', 'repghostnet_058', 'repghostnet_080', 'repghostnet_100', 'repghostnet_111', 'repghostnet_130', 'repghostnet_150', 'repghostnet_200', 'repvgg_a0', 'repvgg_a1', 'repvgg_a2', 'repvgg_b0', 'repvgg_b1', 'repvgg_b1g4', 'repvgg_b2', 'repvgg_b2g4', 'repvgg_b3', 'repvgg_b3g4', 'repvgg_d2se', 'repvit_m0_9', 'repvit_m1', 'repvit_m1_0', 'repvit_m1_1', 'repvit_m1_5', 'repvit_m2', 'repvit_m2_3', 'repvit_m3', 'res2net50_14w_8s', 'res2net50_26w_4s', 'res2net50_26w_6s', 'res2net50_26w_8s', 'res2net50_48w_2s', 'res2net50d', 'res2net101_26w_4s', 'res2net101d', 'res2next50', 'resmlp_12_224', 'resmlp_24_224', 'resmlp_36_224', 'resmlp_big_24_224', 'resnest14d', 'resnest26d', 'resnest50d', 'resnest50d_1s4x24d', 'resnest50d_4s2x40d', 'resnest101e', 'resnest200e', 'resnest269e', 'resnet10t', 'resnet14t', 'resnet18', 'resnet18d', 'resnet26', 'resnet26d', 'resnet26t', 'resnet32ts', 'resnet33ts', 'resnet34', 'resnet34d', 'resnet50', 'resnet50_gn', 'resnet50c', 'resnet50d', 'resnet50s', 'resnet50t', 'resnet51q', 'resnet61q', 'resnet101', 'resnet101c', 'resnet101d', 'resnet101s', 'resnet152', 'resnet152c', 'resnet152d', 'resnet152s', 'resnet200', 'resnet200d', 'resnetaa34d', 'resnetaa50', 'resnetaa50d', 'resnetaa101d', 'resnetblur18', 'resnetblur50', 'resnetblur50d', 'resnetblur101d', 'resnetrs50', 'resnetrs101', 'resnetrs152', 'resnetrs200', 'resnetrs270', 'resnetrs350', 'resnetrs420', 'resnetv2_50', 'resnetv2_50d', 'resnetv2_50d_evos', 'resnetv2_50d_frn', 'resnetv2_50d_gn', 'resnetv2_50t', 'resnetv2_50x1_bit', 'resnetv2_50x3_bit', 'resnetv2_101', 'resnetv2_101d', 'resnetv2_101x1_bit', 'resnetv2_101x3_bit', 'resnetv2_152', 'resnetv2_152d', 'resnetv2_152x2_bit', 'resnetv2_152x4_bit', 'resnext26ts', 'resnext50_32x4d', 'resnext50d_32x4d', 'resnext101_32x4d', 'resnext101_32x8d', 'resnext101_32x16d', 'resnext101_32x32d', 'resnext101_64x4d', 'rexnet_100', 'rexnet_130', 'rexnet_150', 'rexnet_200', 'rexnet_300', 'rexnetr_100', 'rexnetr_130', 'rexnetr_150', 'rexnetr_200', 'rexnetr_300', 'samvit_base_patch16', 'samvit_base_patch16_224', 'samvit_huge_patch16', 'samvit_large_patch16', 'sebotnet33ts_256', 'sedarknet21', 'sehalonet33ts', 'selecsls42', 'selecsls42b', 'selecsls60', 'selecsls60b', 'selecsls84', 'semnasnet_050', 'semnasnet_075', 'semnasnet_100', 'semnasnet_140', 'senet154', 'sequencer2d_l', 'sequencer2d_m', 'sequencer2d_s', 'seresnet18', 'seresnet33ts', 'seresnet34', 'seresnet50', 'seresnet50t', 'seresnet101', 'seresnet152', 'seresnet152d', 'seresnet200d', 'seresnet269d', 'seresnetaa50d', 'seresnext26d_32x4d', 'seresnext26t_32x4d', 'seresnext26ts', 'seresnext50_32x4d', 'seresnext101_32x4d', 'seresnext101_32x8d', 'seresnext101_64x4d', 'seresnext101d_32x8d', 'seresnextaa101d_32x8d', 'seresnextaa201d_32x8d', 'skresnet18', 'skresnet34', 'skresnet50', 'skresnet50d', 'skresnext50_32x4d', 'spnasnet_100', 'swin_base_patch4_window7_224', 'swin_base_patch4_window12_384', 'swin_large_patch4_window7_224', 'swin_large_patch4_window12_384', 'swin_s3_base_224', 'swin_s3_small_224', 'swin_s3_tiny_224', 'swin_small_patch4_window7_224', 'swin_tiny_patch4_window7_224', 'swinv2_base_window8_256', 'swinv2_base_window12_192', 'swinv2_base_window12to16_192to256', 'swinv2_base_window12to24_192to384', 'swinv2_base_window16_256', 'swinv2_cr_base_224', 'swinv2_cr_base_384', 'swinv2_cr_base_ns_224', 'swinv2_cr_giant_224', 'swinv2_cr_giant_384', 'swinv2_cr_huge_224', 'swinv2_cr_huge_384', 'swinv2_cr_large_224', 'swinv2_cr_large_384', 'swinv2_cr_small_224', 'swinv2_cr_small_384', 'swinv2_cr_small_ns_224', 'swinv2_cr_small_ns_256', 'swinv2_cr_tiny_224', 'swinv2_cr_tiny_384', 'swinv2_cr_tiny_ns_224', 'swinv2_large_window12_192', 'swinv2_large_window12to16_192to256', 'swinv2_large_window12to24_192to384', 'swinv2_small_window8_256', 'swinv2_small_window16_256', 'swinv2_tiny_window8_256', 'swinv2_tiny_window16_256', 'tf_efficientnet_b0', 'tf_efficientnet_b1', 'tf_efficientnet_b2', 'tf_efficientnet_b3', 'tf_efficientnet_b4', 'tf_efficientnet_b5', 'tf_efficientnet_b6', 'tf_efficientnet_b7', 'tf_efficientnet_b8', 'tf_efficientnet_cc_b0_4e', 'tf_efficientnet_cc_b0_8e', 'tf_efficientnet_cc_b1_8e', 'tf_efficientnet_el', 'tf_efficientnet_em', 'tf_efficientnet_es', 'tf_efficientnet_l2', 'tf_efficientnet_lite0', 'tf_efficientnet_lite1', 'tf_efficientnet_lite2', 'tf_efficientnet_lite3', 'tf_efficientnet_lite4', 'tf_efficientnetv2_b0', 'tf_efficientnetv2_b1', 'tf_efficientnetv2_b2', 'tf_efficientnetv2_b3', 'tf_efficientnetv2_l', 'tf_efficientnetv2_m', 'tf_efficientnetv2_s', 'tf_efficientnetv2_xl', 'tf_mixnet_l', 'tf_mixnet_m', 'tf_mixnet_s', 'tf_mobilenetv3_large_075', 'tf_mobilenetv3_large_100', 'tf_mobilenetv3_large_minimal_100', 'tf_mobilenetv3_small_075', 'tf_mobilenetv3_small_100', 'tf_mobilenetv3_small_minimal_100', 'tiny_vit_5m_224', 'tiny_vit_11m_224', 'tiny_vit_21m_224', 'tiny_vit_21m_384', 'tiny_vit_21m_512', 'tinynet_a', 'tinynet_b', 'tinynet_c', 'tinynet_d', 'tinynet_e', 'tnt_b_patch16_224', 'tnt_s_patch16_224', 'tresnet_l', 'tresnet_m', 'tresnet_v2_l', 'tresnet_xl', 'twins_pcpvt_base', 'twins_pcpvt_large', 'twins_pcpvt_small', 'twins_svt_base', 'twins_svt_large', 'twins_svt_small', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'visformer_small', 'visformer_tiny', 'vit_base_patch8_224', 'vit_base_patch14_dinov2', 'vit_base_patch14_reg4_dinov2', 'vit_base_patch16_18x2_224', 'vit_base_patch16_224', 'vit_base_patch16_224_miil', 'vit_base_patch16_384', 'vit_base_patch16_clip_224', 'vit_base_patch16_clip_384', 'vit_base_patch16_clip_quickgelu_224', 'vit_base_patch16_gap_224', 'vit_base_patch16_plus_240', 'vit_base_patch16_reg8_gap_256', 'vit_base_patch16_rpn_224', 'vit_base_patch16_siglip_224', 'vit_base_patch16_siglip_256', 'vit_base_patch16_siglip_384', 'vit_base_patch16_siglip_512', 'vit_base_patch16_xp_224', 'vit_base_patch32_224', 'vit_base_patch32_384', 'vit_base_patch32_clip_224', 'vit_base_patch32_clip_256', 'vit_base_patch32_clip_384', 'vit_base_patch32_clip_448', 'vit_base_patch32_clip_quickgelu_224', 'vit_base_patch32_plus_256', 'vit_base_r26_s32_224', 'vit_base_r50_s16_224', 'vit_base_r50_s16_384', 'vit_base_resnet26d_224', 'vit_base_resnet50d_224', 'vit_giant_patch14_224', 'vit_giant_patch14_clip_224', 'vit_giant_patch14_dinov2', 'vit_giant_patch14_reg4_dinov2', 'vit_giant_patch16_gap_224', 'vit_gigantic_patch14_224', 'vit_gigantic_patch14_clip_224', 'vit_huge_patch14_224', 'vit_huge_patch14_clip_224', 'vit_huge_patch14_clip_336', 'vit_huge_patch14_clip_378', 'vit_huge_patch14_clip_quickgelu_224', 'vit_huge_patch14_clip_quickgelu_378', 'vit_huge_patch14_gap_224', 'vit_huge_patch14_xp_224', 'vit_huge_patch16_gap_448', 'vit_large_patch14_224', 'vit_large_patch14_clip_224', 'vit_large_patch14_clip_336', 'vit_large_patch14_clip_quickgelu_224', 'vit_large_patch14_clip_quickgelu_336', 'vit_large_patch14_dinov2', 'vit_large_patch14_reg4_dinov2', 'vit_large_patch14_xp_224', 'vit_large_patch16_224', 'vit_large_patch16_384', 'vit_large_patch16_siglip_256', 'vit_large_patch16_siglip_384', 'vit_large_patch32_224', 'vit_large_patch32_384', 'vit_large_r50_s32_224', 'vit_large_r50_s32_384', 'vit_medium_patch16_gap_240', 'vit_medium_patch16_gap_256', 'vit_medium_patch16_gap_384', 'vit_medium_patch16_reg4_256', 'vit_medium_patch16_reg4_gap_256', 'vit_relpos_base_patch16_224', 'vit_relpos_base_patch16_cls_224', 'vit_relpos_base_patch16_clsgap_224', 'vit_relpos_base_patch16_plus_240', 'vit_relpos_base_patch16_rpn_224', 'vit_relpos_base_patch32_plus_rpn_256', 'vit_relpos_medium_patch16_224', 'vit_relpos_medium_patch16_cls_224', 'vit_relpos_medium_patch16_rpn_224', 'vit_relpos_small_patch16_224', 'vit_relpos_small_patch16_rpn_224', 'vit_small_patch8_224', 'vit_small_patch14_dinov2', 'vit_small_patch14_reg4_dinov2', 'vit_small_patch16_18x2_224', 'vit_small_patch16_36x1_224', 'vit_small_patch16_224', 'vit_small_patch16_384', 'vit_small_patch32_224', 'vit_small_patch32_384', 'vit_small_r26_s32_224', 'vit_small_r26_s32_384', 'vit_small_resnet26d_224', 'vit_small_resnet50d_s16_224', 'vit_so400m_patch14_siglip_224', 'vit_so400m_patch14_siglip_384', 'vit_srelpos_medium_patch16_224', 'vit_srelpos_small_patch16_224', 'vit_tiny_patch16_224', 'vit_tiny_patch16_384', 'vit_tiny_r_s16_p8_224', 'vit_tiny_r_s16_p8_384', 'volo_d1_224', 'volo_d1_384', 'volo_d2_224', 'volo_d2_384', 'volo_d3_224', 'volo_d3_448', 'volo_d4_224', 'volo_d4_448', 'volo_d5_224', 'volo_d5_448', 'volo_d5_512', 'vovnet39a', 'vovnet57a', 'wide_resnet50_2', 'wide_resnet101_2', 'xception41', 'xception41p', 'xception65', 'xception65p', 'xception71', 'xcit_large_24_p8_224', 'xcit_large_24_p8_384', 'xcit_large_24_p16_224', 'xcit_large_24_p16_384', 'xcit_medium_24_p8_224', 'xcit_medium_24_p8_384', 'xcit_medium_24_p16_224', 'xcit_medium_24_p16_384', 'xcit_nano_12_p8_224', 'xcit_nano_12_p8_384', 'xcit_nano_12_p16_224', 'xcit_nano_12_p16_384', 'xcit_small_12_p8_224', 'xcit_small_12_p8_384', 'xcit_small_12_p16_224', 'xcit_small_12_p16_384', 'xcit_small_24_p8_224', 'xcit_small_24_p8_384', 'xcit_small_24_p16_224', 'xcit_small_24_p16_384', 'xcit_tiny_12_p8_224', 'xcit_tiny_12_p8_384', 'xcit_tiny_12_p16_224', 'xcit_tiny_12_p16_384', 'xcit_tiny_24_p8_224', 'xcit_tiny_24_p8_384', 'xcit_tiny_24_p16_224', 'xcit_tiny_24_p16_384']\n"]}],"source":["import timm\n","print(timm.list_models())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hXH-foBOoeZQ"},"outputs":[],"source":["#twins_pcpvt_base\n","# twins_pcpvt_large\n","# twins_pcpvt_small\n","# vgg11\n","#vit_tiny_r_s16_p8_384\n","# vit_tiny_r_s16_p8_224\n","# vit_small_r26_s32_224\n","# vit_large_r50_s32_384\n","# vit_large_r50_s32_224\n","# resnest200e\n","# vgg19_bn\n","# seresnext101_64x4d\n","# volo_d1_224\n","\n","# densenet121\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lghhwIuEejsa"},"outputs":[],"source":["from timm import create_model\n","model = create_model('wide_resnet101_2', pretrained=True)\n","model"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"b-cm_GeJG_SL","executionInfo":{"status":"ok","timestamp":1709126337436,"user_tz":-300,"elapsed":12,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from timm.models import create_model\n","\n","class VisualExtractor(nn.Module):\n","    def __init__(self, args):\n","        super(VisualExtractor, self).__init__()\n","        self.visual_extractor = args['visual_extractor']\n","        self.pretrained = args['visual_extractor_pretrained']\n","\n","        # Create a new first layer to handle (3, 64) input channels\n","        new_first_layer = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","\n","        # Load the pretrained Swin Transformer model\n","        model = create_model(self.visual_extractor, pretrained=self.pretrained, first_layer=new_first_layer)\n","\n","        # Replace the initial convolutional layer\n","        model.patch_embed.proj = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","\n","        modules = list(model.children())[:-2]\n","        self.model = nn.Sequential(*modules)\n","        self.avg_fnt = torch.nn.AvgPool2d(kernel_size=7, stride=1, padding=0)\n","\n","        # Determine the input size for layer normalization based on the actual size of the output\n","        _, last_size, _, _ = self.model(torch.randn(1, 3, 224, 224)).shape\n","        self.norm = nn.LayerNorm([last_size, last_size, 64])\n","\n","    def forward(self, images):\n","        patch_feats = self.model(images)\n","        print(\"Below the model(images) in Visual Extractor\")\n","\n","        B, C, H, W = patch_feats.shape\n","        patch_feats = patch_feats.permute(0, 2, 3, 1).contiguous().view(B * H * W, C)\n","\n","        # Apply layer normalization\n","        patch_feats = self.norm(patch_feats.view(B, H, W, C).permute(0, 3, 1, 2)).view(B, C, H, W)\n","\n","        avg_feats = self.avg_fnt(patch_feats).squeeze().reshape(-1, patch_feats.size(1))\n","\n","        return patch_feats, avg_feats\n"]},{"cell_type":"code","source":["!pip install torchviz\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rPCVYpMw4cNw","executionInfo":{"status":"ok","timestamp":1709126587246,"user_tz":-300,"elapsed":14281,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"}},"outputId":"d55e450b-bf45-4fd2-a8bd-76746a53e31e"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchviz\n","  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.1.0+cu121)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchviz) (1.3.0)\n","Building wheels for collected packages: torchviz\n","  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4131 sha256=8cdd5c99d8c63e484c6aeb60bdda6974de14f9ff16a0722f473535bbc0c0cae9\n","  Stored in directory: /root/.cache/pip/wheels/4c/97/88/a02973217949e0db0c9f4346d154085f4725f99c4f15a87094\n","Successfully built torchviz\n","Installing collected packages: torchviz\n","Successfully installed torchviz-0.0.2\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torchviz import make_dot\n","\n","# Define the VisualExtractor class\n","class VisualExtractor(nn.Module):\n","    def __init__(self, args):\n","        super(VisualExtractor, self).__init__()\n","        self.visual_extractor = args['visual_extractor']\n","        self.pretrained = args['visual_extractor_pretrained']\n","        model = create_model(self.visual_extractor, pretrained=self.pretrained)\n","        modules = list(model.children())[:-2]\n","        self.model = nn.Sequential(*modules)\n","        self.avg_fnt = torch.nn.AvgPool2d(kernel_size=7, stride=1, padding=0)\n","\n","    def forward(self, images):\n","        patch_feats = self.model(images)\n","        avg_feats = self.avg_fnt(patch_feats).squeeze().reshape(-1, patch_feats.size(1))\n","        batch_size, feat_size, _, _ = patch_feats.shape\n","        patch_feats = patch_feats.reshape(batch_size, feat_size, -1).permute(0, 2, 1)\n","        return patch_feats, avg_feats\n","\n","\n","# # Save the graph to a file\n","# graph.render(\"visual_extractor_graph\")\n"],"metadata":{"id":"0SNh1fT-4fd9","executionInfo":{"status":"ok","timestamp":1709126696899,"user_tz":-300,"elapsed":533,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","execution_count":24,"metadata":{"id":"rGtriU3m3Yrr","executionInfo":{"status":"ok","timestamp":1709126342999,"user_tz":-300,"elapsed":3,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"}}},"outputs":[],"source":["# visual_extractor.py\n","\n","\n","import torch\n","import torch.nn as nn\n","# import torchvision.models as models\n","from timm import create_model\n","\n","\n","class VisualExtractor(nn.Module):\n","    def __init__(self, args):\n","        super(VisualExtractor, self).__init__()\n","        self.visual_extractor = args['visual_extractor']\n","        self.pretrained = args['visual_extractor_pretrained']\n","        model = create_model(self.visual_extractor, pretrained=self.pretrained)\n","        modules = list(model.children())[:-2]\n","        self.model = nn.Sequential(*modules)\n","        self.avg_fnt = torch.nn.AvgPool2d(kernel_size=7, stride=1, padding=0)\n","\n","    def forward(self, images):\n","        patch_feats = self.model(images)\n","        print(\"in visual extractor below the model(image)\")\n","        avg_feats = self.avg_fnt(patch_feats).squeeze().reshape(-1, patch_feats.size(1))\n","        batch_size, feat_size, _, _ = patch_feats.shape\n","        patch_feats = patch_feats.reshape(batch_size, feat_size, -1).permute(0, 2, 1)\n","        return patch_feats, avg_feats\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RqfCPZK-ruzG"},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":700,"referenced_widgets":["14d970793a7246ebaaab97491994bc90","aefbe6b33d7247419ce1ab51d923c9cd","65a2cd97451e4c87b655b93c4fcad222","1735b1820a8f4621a47c5a26c79b346e","22dbb0b57ebc491ca64a8d4e87d1b034","b2f0cbae28bf4016b3f7db90511be363","ece427073d3c4a11a48637db780b6bfa","f26e5d73b82a4661bfe0b69fee203b53","f7f7fe6f14604b86b16a975a9cfebae8","7b9d4d5966fb47d9b35df3aa0d525f5f","de73920a77fa4a3aa4ee0a1aca7635b2"]},"executionInfo":{"elapsed":26403,"status":"error","timestamp":1703923785386,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"},"user_tz":-270},"id":"y7x79pLG5q5s","outputId":"810fcc0f-4cda-4077-af8c-7a5c68ada846"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"14d970793a7246ebaaab97491994bc90","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["bellow the visual extractor\n","Visual output size: 2048\n","linear_input_size: 2048\n","bellow the linear layer\n","in basecmnmodel bellow the encode_decoder\n","in basecmnmodel iu xray forward pass\n","[1/6] Start to train in the training set.\n","ending the dataloader\n","ending the dataloader\n","In forward_iu_xray in BaseCMNModel\n","in visual extractor below the model(image)\n","Below the avg_feats in BaseCMNModel\n","Shape of avg_feats before flattening: torch.Size([20, 2048])\n","Shape of avg_feats after flattening: torch.Size([20, 2048])\n","Shape of avg_feats after flattening: torch.Size([20, 2048])\n","Below the linear_output in forward_iu_xray\n","in train\n","ending the dataloader\n"]},{"ename":"TypeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-72-df262e3853f8>\u001b[0m in \u001b[0;36m<cell line: 122>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-72-df262e3853f8>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;31m# build trainer and start to train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-188c867601a7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m# save logged informations into log dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-188c867601a7>\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreports_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreports_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreports_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                                                  \u001b[0mreports_masks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreports_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreports_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreports_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-897e2298f221>\u001b[0m in \u001b[0;36mforward_iu_xray\u001b[0;34m(self, images, targets, mode, update_opts)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'in train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'forward'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adjust arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sample'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-a29ebe2fcc7b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'mode'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mode'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbeam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_logprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: BaseCMN._forward() got an unexpected keyword argument 'targets'"]}],"source":["\n","#train.py\n","\n","import argparse\n","import torchvision.models as models\n","\n","import numpy as np\n","import torch\n","import os\n","\n","\n","\n","# from models.models import BaseCMNModel\n","# from modules.dataloaders import R2DataLoader\n","# from modules.loss import compute_loss\n","# from modules.metrics import compute_scores\n","# from modules.optimizers import build_optimizer, build_lr_scheduler\n","# from modules.tokenizers import Tokenizer\n","# from modules.trainer import Trainer\n","# resnet152\n","# swinv2_cr_small_ns_224\n","# swin_s3_small_224\n","# tiny_vit_5m_224\n","# vit_base_patch16_clip_224\n","# densenet121\n","# res2net101_26w_4s\n","# res2net101d\n","# tf_efficientnet_lite4\n","# tf_mixnet_l\n","\n","\n","def main():\n","    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","    args = {\n","        'image_dir': '/content/drive/MyDrive/iu_xray/images',\n","        'ann_path': '/content/drive/MyDrive/iu_xray/annotation.json',\n","        'dataset_name': 'iu_xray',\n","        'max_seq_length': 60,\n","        'num_workers': 2,\n","        'threshold':3,\n","        'batch_size': 10,\n","        'visual_extractor': 'resnet101',                 # vgg11 , vgg13\n","        'visual_extractor_pretrained': True,           # working models (resnetrs101,)\n","        'd_model': 512,\n","        'd_ff': 512,\n","        'd_vf': 2048,\n","        'num_heads': 8,\n","        'num_layers': 3,\n","        'dropout': 0.1,\n","        'logit_layers': 1,\n","        'bos_idx': 0,\n","       'eos_idx': 0,\n","        'pad_idx': 0,\n","        'use_bn': 0,\n","        'drop_prob_lm': 0.5,\n","        'topk': 32,\n","        'cmm_size': 2048,\n","        'cmm_dim': 512,\n","        'sample_method': 'beam_search',\n","        'beam_size': 3,\n","        'temperature': 1.0,\n","        'sample_n': 1,\n","        'group_size': 1,\n","        'output_logsoftmax': 1,\n","        'decoding_constraint': 0,\n","        'block_trigrams': 1,\n","        'n_gpu': 1,\n","        'epochs': 6,\n","        'save_dir': '/content/drive/MyDrive/111',\n","        'record_dir': 'records/',\n","        'log_period': 1000,\n","        'save_period': 1,\n","        'monitor_mode': 'max',\n","        'monitor_metric': 'BLEU_4',\n","        'early_stop': 50,\n","        'optim': 'Adam',\n","        'lr_ve': 5e-5,\n","        'lr_ed': 7e-4,\n","        'weight_decay': 5e-5,\n","        'adam_betas': (0.9, 0.98),\n","        'adam_eps': 1e-9,\n","        'amsgrad': True,\n","        'noamopt_warmup': 5000,\n","        'noamopt_factor': 1,\n","        'lr_scheduler': 'StepLR',\n","        'step_size': 50,\n","        'gamma': 0.1,\n","        'seed': 9233,\n","        'resume': None,\n","\n","    }\n","\n","    torch.manual_seed(args['seed'])\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(args['seed'])\n","\n","    # create tokenizer\n","    tokenizer = Tokenizer(args)\n","\n","    # create data loader\n","    train_dataloader = R2DataLoader(args, tokenizer, split='train', shuffle=True)\n","    val_dataloader = R2DataLoader(args, tokenizer, split='val', shuffle=False)\n","    test_dataloader = R2DataLoader(args, tokenizer, split='test', shuffle=False)\n","\n","    # build model architecture\n","    model = BaseCMNModel(args, tokenizer)\n","\n","    # get function handles of loss and metrics\n","    criterion = compute_loss\n","    metrics = compute_scores\n","\n","    # build optimizer, learning rate scheduler\n","    optimizer = build_optimizer(args, model)\n","    lr_scheduler = build_lr_scheduler(args, optimizer)\n","\n","    # build trainer and start to train\n","    trainer = Trainer(model, criterion, metrics, optimizer, args, lr_scheduler, train_dataloader, val_dataloader, test_dataloader)\n","    trainer.train()\n","\n","\n","\n","if __name__ == '__main__':\n","    main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CTgJADaR_T-E"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C9ffgtb28jOw"},"outputs":[],"source":["############# here we test the model ###############3"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"SgA8og6j8jNn","executionInfo":{"status":"ok","timestamp":1709126745614,"user_tz":-300,"elapsed":3035,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"}}},"outputs":[],"source":["#tester.py\n","import logging\n","import os\n","from abc import abstractmethod\n","\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import spacy\n","import torch\n","from tqdm import tqdm\n","\n","\n","\n","\n","class BaseTester(object):\n","    def __init__(self, model, criterion, metric_ftns, args):\n","        self.args = args\n","\n","        logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n","                            datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n","        self.logger = logging.getLogger(__name__)\n","\n","        # setup GPU device if available, move model into configured device\n","        self.device, device_ids = self._prepare_device(args['n_gpu'])\n","        self.model = model.to(self.device)\n","        if len(device_ids) > 1:\n","            self.model = torch.nn.DataParallel(model, device_ids=device_ids)\n","\n","        self.criterion = criterion\n","        self.metric_ftns = metric_ftns\n","\n","        self.epochs = self.args['epochs']\n","        self.save_dir = self.args['save_dir']\n","        if not os.path.exists(self.save_dir):\n","            os.makedirs(self.save_dir)\n","\n","        self._load_checkpoint(args['load'])\n","\n","    @abstractmethod\n","    def test(self):\n","        raise NotImplementedError\n","\n","    @abstractmethod\n","    def plot(self):\n","        raise NotImplementedError\n","\n","    def _prepare_device(self, n_gpu_use):\n","        n_gpu = torch.cuda.device_count()\n","        if n_gpu_use > 0 and n_gpu == 0:\n","            self.logger.warning(\n","                \"Warning: There\\'s no GPU available on this machine,\" \"training will be performed on CPU.\")\n","            n_gpu_use = 0\n","        if n_gpu_use > n_gpu:\n","            self.logger.warning(\n","                \"Warning: The number of GPU\\'s configured to use is {}, but only {} are available \" \"on this machine.\".format(\n","                    n_gpu_use, n_gpu))\n","            n_gpu_use = n_gpu\n","        device = torch.device('cuda:0' if n_gpu_use > 0 else 'cpu')\n","        list_ids = list(range(n_gpu_use))\n","        return device, list_ids\n","\n","    def _load_checkpoint(self, load_path):\n","        load_path = str(load_path)\n","        self.logger.info(\"Loading checkpoint: {} ...\".format(load_path))\n","        checkpoint = torch.load(load_path)\n","        self.model.load_state_dict(checkpoint['state_dict'])\n","\n","\n","class Tester(BaseTester):\n","    def __init__(self, model, criterion, metric_ftns, args, test_dataloader):\n","        super(Tester, self).__init__(model, criterion, metric_ftns, args)\n","        self.test_dataloader = test_dataloader\n","\n","    def test(self):\n","        self.logger.info('Start to evaluate in the test set.')\n","        self.model.eval()\n","        log = dict()\n","        with torch.no_grad():\n","            test_gts, test_res = [], []\n","            for batch_idx, (images_id, images, reports_ids, reports_masks) in tqdm(enumerate(self.test_dataloader)):\n","                images, reports_ids, reports_masks = images.to(self.device), reports_ids.to(\n","                    self.device), reports_masks.to(self.device)\n","                output, _ = self.model(images, mode='sample')\n","                reports = self.model.tokenizer.decode_batch(output.cpu().numpy())\n","                ground_truths = self.model.tokenizer.decode_batch(reports_ids[:, 1:].cpu().numpy())\n","                test_res.extend(reports)\n","                test_gts.extend(ground_truths)\n","\n","            test_met = self.metric_ftns({i: [gt] for i, gt in enumerate(test_gts)},\n","                                        {i: [re] for i, re in enumerate(test_res)})\n","            log.update(**{'test_' + k: v for k, v in test_met.items()})\n","            print(log)\n","\n","            test_res, test_gts = pd.DataFrame(test_res), pd.DataFrame(test_gts)\n","            test_res.to_csv(os.path.join(self.save_dir, \"res.csv\"), index=False, header=False)\n","            test_gts.to_csv(os.path.join(self.save_dir, \"gts.csv\"), index=False, header=False)\n","\n","        return log\n","\n","    def plot(self):\n","        assert self.args['batch_size'] == 1 and self.args['beam_size'] == 1\n","        self.logger.info('Start to plot attention weights in the test set.')\n","        os.makedirs(os.path.join(self.save_dir, \"attentions\"), exist_ok=True)\n","        os.makedirs(os.path.join(self.save_dir, \"attentions_entities\"), exist_ok=True)\n","        ner = spacy.load(\"en_core_sci_sm\")\n","        mean = torch.tensor((0.485, 0.456, 0.406))\n","        std = torch.tensor((0.229, 0.224, 0.225))\n","        mean = mean[:, None, None]\n","        std = std[:, None, None]\n","\n","        self.model.eval()\n","        with torch.no_grad():\n","            for batch_idx, (images_id, images, reports_ids, reports_masks) in tqdm(enumerate(self.test_dataloader)):\n","                images, reports_ids, reports_masks = images.to(self.device), reports_ids.to(\n","                    self.device), reports_masks.to(self.device)\n","                output, _ = self.model(images, mode='sample')\n","                image = torch.clamp((images[0].cpu() * std + mean) * 255, 0, 255).int().cpu().numpy()\n","                report = self.model.tokenizer.decode_batch(output.cpu().numpy())[0].split()\n","\n","                char2word = [idx for word_idx, word in enumerate(report) for idx in [word_idx] * (len(word) + 1)][:-1]\n","\n","                attention_weights = self.model.encoder_decoder.attention_weights[:-1]\n","                assert len(attention_weights) == len(report)\n","                for word_idx, (attns, word) in enumerate(zip(attention_weights, report)):\n","                    for layer_idx, attn in enumerate(attns):\n","                        os.makedirs(os.path.join(self.save_dir, \"attentions\", \"{:04d}\".format(batch_idx),\n","                                                 \"layer_{}\".format(layer_idx)), exist_ok=True)\n","\n","                        heatmap = generate_heatmap(image, attn.mean(1).squeeze())\n","                        cv2.imwrite(os.path.join(self.save_dir, \"attentions\", \"{:04d}\".format(batch_idx),\n","                                                 \"layer_{}\".format(layer_idx), \"{:04d}_{}.png\".format(word_idx, word)),\n","                                    heatmap)\n","\n","                for ne_idx, ne in enumerate(ner(\" \".join(report)).ents):\n","                    for layer_idx in range(len(attention_weights[0])):\n","                        os.makedirs(os.path.join(self.save_dir, \"attentions_entities\", \"{:04d}\".format(batch_idx),\n","                                                 \"layer_{}\".format(layer_idx)), exist_ok=True)\n","                        attn = [attns[layer_idx] for attns in\n","                                attention_weights[char2word[ne.start_char]:char2word[ne.end_char] + 1]]\n","                        attn = np.concatenate(attn, axis=2)\n","                        heatmap = generate_heatmap(image, attn.mean(1).mean(1).squeeze())\n","                        cv2.imwrite(os.path.join(self.save_dir, \"attentions_entities\", \"{:04d}\".format(batch_idx),\n","                                                 \"layer_{}\".format(layer_idx), \"{:04d}_{}.png\".format(ne_idx, ne)),\n","                                    heatmap)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aM5LWQ3L8jDD"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F1HI91_E8i4T"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"sht2QAOX8k6Y"},"source":["Here we test the model."]},{"cell_type":"code","execution_count":29,"metadata":{"id":"ImU9yepCCKrR","executionInfo":{"status":"ok","timestamp":1709126759253,"user_tz":-300,"elapsed":1519,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"}}},"outputs":[],"source":["#tester.py\n","import logging\n","import os\n","from abc import abstractmethod\n","\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import spacy\n","import torch\n","from tqdm import tqdm\n","\n","\n","\n","\n","class BaseTester(object):\n","    def __init__(self, model, criterion, metric_ftns, args):\n","        self.args = args\n","\n","        logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n","                            datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n","        self.logger = logging.getLogger(__name__)\n","\n","        # setup GPU device if available, move model into configured device\n","        self.device, device_ids = self._prepare_device(args['n_gpu'])\n","        self.model = model.to(self.device)\n","        if len(device_ids) > 1:\n","            self.model = torch.nn.DataParallel(model, device_ids=device_ids)\n","\n","        self.criterion = criterion\n","        self.metric_ftns = metric_ftns\n","\n","        self.epochs = self.args['epochs']\n","        self.save_dir = self.args['save_dir']\n","        if not os.path.exists(self.save_dir):\n","            os.makedirs(self.save_dir)\n","\n","        self._load_checkpoint(args['load'])\n","\n","    @abstractmethod\n","    def test(self):\n","        raise NotImplementedError\n","\n","    @abstractmethod\n","    def plot(self):\n","        raise NotImplementedError\n","\n","    def _prepare_device(self, n_gpu_use):\n","        n_gpu = torch.cuda.device_count()\n","        if n_gpu_use > 0 and n_gpu == 0:\n","            self.logger.warning(\n","                \"Warning: There\\'s no GPU available on this machine,\" \"training will be performed on CPU.\")\n","            n_gpu_use = 0\n","        if n_gpu_use > n_gpu:\n","            self.logger.warning(\n","                \"Warning: The number of GPU\\'s configured to use is {}, but only {} are available \" \"on this machine.\".format(\n","                    n_gpu_use, n_gpu))\n","            n_gpu_use = n_gpu\n","        device = torch.device('cuda:0' if n_gpu_use > 0 else 'cpu')\n","        list_ids = list(range(n_gpu_use))\n","        return device, list_ids\n","\n","    def _load_checkpoint(self, load_path):\n","        load_path = str(load_path)\n","        self.logger.info(\"Loading checkpoint: {} ...\".format(load_path))\n","        checkpoint = torch.load(load_path)\n","        self.model.load_state_dict(checkpoint['state_dict'])\n","\n","\n","class Tester(BaseTester):\n","    def __init__(self, model, criterion, metric_ftns, args, test_dataloader):\n","        super(Tester, self).__init__(model, criterion, metric_ftns, args)\n","        self.test_dataloader = test_dataloader\n","\n","    def test(self):\n","        self.logger.info('Start to evaluate in the test set.')\n","        self.model.eval()\n","        log = dict()\n","        with torch.no_grad():\n","            test_gts, test_res = [], []\n","            for batch_idx, (images_id, images, reports_ids, reports_masks) in tqdm(enumerate(self.test_dataloader)):\n","                images, reports_ids, reports_masks = images.to(self.device), reports_ids.to(\n","                    self.device), reports_masks.to(self.device)\n","                output, _ = self.model(images, mode='sample')\n","                reports = self.model.tokenizer.decode_batch(output.cpu().numpy())\n","                ground_truths = self.model.tokenizer.decode_batch(reports_ids[:, 1:].cpu().numpy())\n","                test_res.extend(reports)\n","                test_gts.extend(ground_truths)\n","\n","            test_met = self.metric_ftns({i: [gt] for i, gt in enumerate(test_gts)},\n","                                        {i: [re] for i, re in enumerate(test_res)})\n","            log.update(**{'test_' + k: v for k, v in test_met.items()})\n","            print(log)\n","\n","            test_res, test_gts = pd.DataFrame(test_res), pd.DataFrame(test_gts)\n","            test_res.to_csv(os.path.join(self.save_dir, \"res.csv\"), index=False, header=False)\n","            test_gts.to_csv(os.path.join(self.save_dir, \"gts.csv\"), index=False, header=False)\n","\n","        return log\n","\n","    def plot(self):\n","        assert self.args['batch_size'] == 1 and self.args['beam_size'] == 1\n","        self.logger.info('Start to plot attention weights in the test set.')\n","        os.makedirs(os.path.join(self.save_dir, \"attentions\"), exist_ok=True)\n","        os.makedirs(os.path.join(self.save_dir, \"attentions_entities\"), exist_ok=True)\n","        ner = spacy.load(\"en_core_sci_sm\")\n","        mean = torch.tensor((0.485, 0.456, 0.406))\n","        std = torch.tensor((0.229, 0.224, 0.225))\n","        mean = mean[:, None, None]\n","        std = std[:, None, None]\n","\n","        self.model.eval()\n","        with torch.no_grad():\n","            for batch_idx, (images_id, images, reports_ids, reports_masks) in tqdm(enumerate(self.test_dataloader)):\n","                images, reports_ids, reports_masks = images.to(self.device), reports_ids.to(\n","                    self.device), reports_masks.to(self.device)\n","                output, _ = self.model(images, mode='sample')\n","                image = torch.clamp((images[0].cpu() * std + mean) * 255, 0, 255).int().cpu().numpy()\n","                report = self.model.tokenizer.decode_batch(output.cpu().numpy())[0].split()\n","\n","                char2word = [idx for word_idx, word in enumerate(report) for idx in [word_idx] * (len(word) + 1)][:-1]\n","\n","                attention_weights = self.model.encoder_decoder.attention_weights[:-1]\n","                assert len(attention_weights) == len(report)\n","                for word_idx, (attns, word) in enumerate(zip(attention_weights, report)):\n","                    for layer_idx, attn in enumerate(attns):\n","                        os.makedirs(os.path.join(self.save_dir, \"attentions\", \"{:04d}\".format(batch_idx),\n","                                                 \"layer_{}\".format(layer_idx)), exist_ok=True)\n","\n","                        heatmap = generate_heatmap(image, attn.mean(1).squeeze())\n","                        cv2.imwrite(os.path.join(self.save_dir, \"attentions\", \"{:04d}\".format(batch_idx),\n","                                                 \"layer_{}\".format(layer_idx), \"{:04d}_{}.png\".format(word_idx, word)),\n","                                    heatmap)\n","\n","                for ne_idx, ne in enumerate(ner(\" \".join(report)).ents):\n","                    for layer_idx in range(len(attention_weights[0])):\n","                        os.makedirs(os.path.join(self.save_dir, \"attentions_entities\", \"{:04d}\".format(batch_idx),\n","                                                 \"layer_{}\".format(layer_idx)), exist_ok=True)\n","                        attn = [attns[layer_idx] for attns in\n","                                attention_weights[char2word[ne.start_char]:char2word[ne.end_char] + 1]]\n","                        attn = np.concatenate(attn, axis=2)\n","                        heatmap = generate_heatmap(image, attn.mean(1).mean(1).squeeze())\n","                        cv2.imwrite(os.path.join(self.save_dir, \"attentions_entities\", \"{:04d}\".format(batch_idx),\n","                                                 \"layer_{}\".format(layer_idx), \"{:04d}_{}.png\".format(ne_idx, ne)),\n","                                    heatmap)\n"]},{"cell_type":"markdown","metadata":{"id":"sVfG7jZieBpO"},"source":["##the below one is working well for saving the report files"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torchviz import make_dot\n","\n","# Define the VisualExtractor class\n","class VisualExtractor(nn.Module):\n","    def __init__(self, args):\n","        super(VisualExtractor, self).__init__()\n","        self.visual_extractor = args['visual_extractor']\n","        self.pretrained = args['visual_extractor_pretrained']\n","        model = create_model(self.visual_extractor, pretrained=self.pretrained)\n","        modules = list(model.children())[:-2]\n","        self.model = nn.Sequential(*modules)\n","        self.avg_fnt = torch.nn.AvgPool2d(kernel_size=7, stride=1, padding=0)\n","\n","    def forward(self, images):\n","        patch_feats = self.model(images)\n","        avg_feats = self.avg_fnt(patch_feats).squeeze().reshape(-1, patch_feats.size(1))\n","        batch_size, feat_size, _, _ = patch_feats.shape\n","        patch_feats = patch_feats.reshape(batch_size, feat_size, -1).permute(0, 2, 1)\n","        return patch_feats, avg_feats\n","\n","\n","# # Save the graph to a file\n","# graph.render(\"visual_extractor_graph\")\n"],"metadata":{"id":"GcZp1L2v7KEh","executionInfo":{"status":"ok","timestamp":1709127284163,"user_tz":-300,"elapsed":599,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torchviz import make_dot\n","from timm import create_model  # Add import statement for create_model\n","\n","# Define the VisualExtractor class\n","class VisualExtractor(nn.Module):\n","    def __init__(self, args):\n","        super(VisualExtractor, self).__init__()\n","        self.visual_extractor = args['visual_extractor']\n","        self.pretrained = args['visual_extractor_pretrained']\n","        model = create_model(self.visual_extractor, pretrained=self.pretrained)\n","        modules = list(model.children())[:-2]\n","        self.model = nn.Sequential(*modules)\n","        self.avg_fnt = torch.nn.AvgPool2d(kernel_size=7, stride=1, padding=0)\n","\n","    def forward(self, images):\n","        patch_feats = self.model(images)\n","        avg_feats = self.avg_fnt(patch_feats).squeeze().reshape(-1, patch_feats.size(1))\n","        batch_size, feat_size, _, _ = patch_feats.shape\n","        patch_feats = patch_feats.reshape(batch_size, feat_size, -1).permute(0, 2, 1)\n","        return patch_feats, avg_feats\n","\n","\n","def main():\n","    args = {\n","        'image_dir': '/content/drive/MyDrive/iu_xray/images',\n","        'ann_path': '/content/drive/MyDrive/iu_xray/annotation.json',\n","        'dataset_name': 'iu_xray',\n","        'max_seq_length': 60,\n","        'threshold': 3,\n","        'num_workers': 2,\n","        'batch_size': 16,\n","        'visual_extractor': 'resnet101',\n","        'visual_extractor_pretrained': True,\n","        'd_model': 512,\n","        'd_ff': 512,\n","        'd_vf': 2048,\n","        'num_heads': 8,\n","        'num_layers': 3,\n","        'dropout': 0.1,\n","        'logit_layers': 1,\n","        'bos_idx': 0,\n","        'eos_idx': 0,\n","        'pad_idx': 0,\n","        'use_bn': 0,\n","        'drop_prob_lm': 0.5,\n","        'topk': 32,\n","        'cmm_size': 2048,\n","        'cmm_dim': 512,\n","        'sample_method': 'beam_search',\n","        'beam_size': 3,\n","        'temperature': 1.0,\n","        'sample_n': 1,\n","        'group_size': 1,\n","        'output_logsoftmax': 1,\n","        'decoding_constraint': 0,\n","        'block_trigrams': 1,\n","        'n_gpu': 1,\n","        'epochs': 100,\n","        'save_dir': 'results/iu_xray',\n","        'record_dir': 'records/',\n","        'log_period': 1000,\n","        'save_period': 1,\n","        'monitor_mode': 'max',\n","        'monitor_metric': 'BLEU_4',\n","        'early_stop': 50,\n","        'optim': 'Adam',\n","        'lr_ve': 5e-5,\n","        'lr_ed': 7e-4,\n","        'weight_decay': 5e-5,\n","        'adam_betas': (0.9, 0.98),\n","        'adam_eps': 1e-9,\n","        'amsgrad': True,\n","        'noamopt_warmup': 5000,\n","        'noamopt_factor': 1,\n","        'lr_scheduler': 'StepLR',\n","        'step_size': 50,\n","        'gamma': 0.1,\n","        'seed': 9233,\n","        'resume': None,\n","        # 'load': \"/content/results/iu_xray/model_best.pth\",\n","        'load': \"/content/drive/MyDrive/resnest200e_save_checkpoints/model_best.pth\",\n","\n","    }\n","\n","     # Instantiate the VisualExtractor model\n","    model = VisualExtractor(args)\n","\n","     # Create a random input tensor\n","    images = torch.randn(1, 3, 224, 224)  # Batch size 1, RGB channels, image size 224x224\n","\n","    # Pass the input through the model\n","    patch_feats, avg_feats = model(images)\n","\n","    # Print the length of the output layer\n","    print(\"Length of the output layer:\", patch_feats.size(1))\n","\n","    # Print the shapes of the output tensors\n","    print(\"Shape of patch_feats:\", patch_feats.shape)\n","    print(\"Shape of avg_feats:\", avg_feats.shape)\n","\n","    #################   input and output tensors ################33\n","\n","    # # Create a random input tensor\n","    # images = torch.randn(1, 3, 224, 224)  # Batch size 1, RGB channels, image size 224x224\n","\n","    # print(\"Input tensor shape:\", images.shape)\n","    # print(\"Input tensor values:\", images)\n","\n","    # # Pass the input tensor through the model\n","    # patch_feats, avg_feats = model(images)\n","\n","    # print(\"Output patch features shape:\", patch_feats.shape)\n","    # print(\"Output patch features:\", patch_feats)\n","\n","    # print(\"Output average features shape:\", avg_feats.shape)\n","    # print(\"Output average features:\", avg_feats)\n","\n","############ Generate Graph of the visual extractor #################\n","    # # Instantiate the VisualExtractor model\n","    # model = VisualExtractor(args)\n","\n","    # # Create a random input tensor\n","    # images = torch.randn(1, 3, 224, 224)  # Batch size 1, RGB channels, image size 224x224\n","\n","    # # Create a graph of the model\n","    # output = model(images)\n","    # graph = make_dot(output)  # Corrected to pass the entire model's output\n","\n","    # # Save the graph to a file\n","    # graph.render(\"visual_extractor_graph\")\n","\n","if __name__ == '__main__':\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1KfJoq2L6OOQ","executionInfo":{"status":"ok","timestamp":1709128161328,"user_tz":-300,"elapsed":2490,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"}},"outputId":"b624fbec-cd58-47e4-83de-b81f7b766d54"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Length of the output layer: 49\n","Shape of patch_feats: torch.Size([1, 49, 2048])\n","Shape of avg_feats: torch.Size([1, 2048])\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5426,"status":"ok","timestamp":1701513158026,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"},"user_tz":480},"id":"D1COW5uSDzCd","outputId":"6fbc822e-d6bb-45d0-f6bf-1c17f1be2eff"},"outputs":[{"name":"stdout","output_type":"stream","text":["Generated Report for Image 1: tortuosity acromioclavicular top acromioclavicular acromioclavicular acromioclavicular acromioclavicular acromioclavicular acromioclavicular acromioclavicular granulomas acromioclavicular subtle shadows wedging acromioclavicular acromioclavicular acromioclavicular acromioclavicular acromioclavicular acromioclavicular acromioclavicular acromioclavicular acromioclavicular acromioclavicular acromioclavicular dual-lumen shift cavoatrial acromioclavicular acromioclavicular posteriorly acromioclavicular acromioclavicular concerning available shift been hyperexpanded osteophytes hyperexpanded underlying shift coarsened underlying shift acromioclavicular acromioclavicular acromioclavicular acromioclavicular acromioclavicular base acromioclavicular base acromioclavicular available amount acromioclavicular in shift\n"]}],"source":["import argparse\n","import numpy as np\n","import torch\n","\n","# from models.models import BaseCMNModel\n","# from modules.dataloaders import R2DataLoader\n","# from modules.loss import compute_loss\n","# from modules.metrics import compute_scores\n","# from modules.tokenizers import Tokenizer\n","# from modules.tester import Tester\n","\n","\n","\n","\n","def main():\n","    args = {\n","         'image_dir': '/content/drive/MyDrive/iu_xray/images',\n","        'ann_path': '/content/drive/MyDrive/iu_xray/annotation.json',\n","        'dataset_name': 'iu_xray',\n","        'max_seq_length': 60,\n","        'threshold': 3,\n","        'num_workers': 2,\n","        'batch_size': 16,\n","        'visual_extractor': 'resnet101',\n","        'visual_extractor_pretrained': True,\n","        'd_model': 512,\n","        'd_ff': 512,\n","        'd_vf': 2048,\n","        'num_heads': 8,\n","        'num_layers': 3,\n","        'dropout': 0.1,\n","        'logit_layers': 1,\n","        'bos_idx': 0,\n","        'eos_idx': 0,\n","        'pad_idx': 0,\n","        'use_bn': 0,\n","        'drop_prob_lm': 0.5,\n","        'topk': 32,\n","        'cmm_size': 2048,\n","        'cmm_dim': 512,\n","        'sample_method': 'beam_search',\n","        'beam_size': 3,\n","        'temperature': 1.0,\n","        'sample_n': 1,\n","        'group_size': 1,\n","        'output_logsoftmax': 1,\n","        'decoding_constraint': 0,\n","        'block_trigrams': 1,\n","        'n_gpu': 1,\n","        'epochs': 100,\n","        'save_dir': 'results/iu_xray',\n","        'record_dir': 'records/',\n","        'log_period': 1000,\n","        'save_period': 1,\n","        'monitor_mode': 'max',\n","        'monitor_metric': 'BLEU_4',\n","        'early_stop': 50,\n","        'optim': 'Adam',\n","        'lr_ve': 5e-5,\n","        'lr_ed': 7e-4,\n","        'weight_decay': 5e-5,\n","        'adam_betas': (0.9, 0.98),\n","        'adam_eps': 1e-9,\n","        'amsgrad': True,\n","        'noamopt_warmup': 5000,\n","        'noamopt_factor': 1,\n","        'lr_scheduler': 'StepLR',\n","        'step_size': 50,\n","        'gamma': 0.1,\n","        'seed': 9233,\n","        'resume': None,\n","        # 'load': \"/content/results/iu_xray/model_best.pth\",\n","        'load': \"/content/drive/MyDrive/resnest200e_save_checkpoints/model_best.pth\",\n","\n","    }\n","\n","     # Create tokenizer\n","    tokenizer = Tokenizer(args)\n","\n","    # Build model architecture\n","    model = BaseCMNModel(args, tokenizer)\n","\n","    return model\n","\n","\n","if __name__ == '__main__':\n","\n","\n","    args = {\n","         'image_dir': '/content/drive/MyDrive/iu_xray/images',\n","        'ann_path': '/content/drive/MyDrive/iu_xray/annotation.json',\n","        'dataset_name': 'iu_xray',\n","        'max_seq_length': 60,\n","        'threshold': 3,\n","        'num_workers': 2,\n","        'batch_size': 16,\n","        'visual_extractor': 'resnet101',\n","        'visual_extractor_pretrained': True,\n","        'd_model': 512,\n","        'd_ff': 512,\n","        'd_vf': 2048,\n","        'num_heads': 8,\n","        'num_layers': 3,\n","        'dropout': 0.1,\n","        'logit_layers': 1,\n","        'bos_idx': 0,\n","        'eos_idx': 0,\n","        'pad_idx': 0,\n","        'use_bn': 0,\n","        'drop_prob_lm': 0.5,\n","        'topk': 32,\n","        'cmm_size': 2048,\n","        'cmm_dim': 512,\n","        'sample_method': 'beam_search',\n","        'beam_size': 3,\n","        'temperature': 1.0,\n","        'sample_n': 1,\n","        'group_size': 1,\n","        'output_logsoftmax': 1,\n","        'decoding_constraint': 0,\n","        'block_trigrams': 1,\n","        'n_gpu': 1,\n","        'epochs': 100,\n","        'save_dir': 'results/iu_xray',\n","        'record_dir': 'records/',\n","        'log_period': 1000,\n","        'save_period': 1,\n","        'monitor_mode': 'max',\n","        'monitor_metric': 'BLEU_4',\n","        'early_stop': 50,\n","        'optim': 'Adam',\n","        'lr_ve': 5e-5,\n","        'lr_ed': 7e-4,\n","        'weight_decay': 5e-5,\n","        'adam_betas': (0.9, 0.98),\n","        'adam_eps': 1e-9,\n","        'amsgrad': True,\n","        'noamopt_warmup': 5000,\n","        'noamopt_factor': 1,\n","        'lr_scheduler': 'StepLR',\n","        'step_size': 50,\n","        'gamma': 0.1,\n","        'seed': 9233,\n","        'resume': None,\n","        # 'load': \"/content/results/iu_xray/model_best.pth\",\n","        'load': \"/content/drive/MyDrive/resnest200e_save_checkpoints/model_best.pth\",\n","\n","    }\n","\n","\n","\n","\n","\n","    tokenizer = Tokenizer(args)\n","    main()\n","    model = main()\n","\n","\n","\n","\n","\n","    # Check if the model is not None\n","    if model is not None:\n","        # Move the model to GPU if it's not already there\n","        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","        model = model.to(device)\n","\n","\n","        # Image paths\n","        # image_path_1 = '/content/drive/MyDrive/iu_xray/images/CXR1000_IM-0003/0.png'\n","        # image_path_2 = '/content/drive/MyDrive/iu_xray/images/CXR1001_IM-0004/1.png'\n","\n","        # Preprocess the images\n","        transform = transforms.Compose([\n","            transforms.Resize((224, 224)),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","        ])\n","\n","        image_1 = Image.open(image_path_1).convert('RGB')\n","        # image_2 = Image.open(image_path_2).convert('RGB')\n","\n","        image_1 = transform(image_1)\n","        # image_2 = transform(image_2)\n","\n","        # Stack the images along a new dimension\n","        image = torch.stack([image_1, image_1], dim=0)\n","\n","        # Add batch dimension\n","        image = image.unsqueeze(0)\n","\n","        # Move the input image to the same device as the model\n","        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","        image = image.to(device)\n","\n","\n","\n","        # Generate the report\n","        with torch.no_grad():\n","            output, _ = model(image, mode='sample')\n","\n","        generated_report = tokenizer.decode_batch(output.cpu().detach().numpy())[0]\n","\n","        # Print the generated report\n","        print(f\"Generated Report for Image 1: {generated_report}\")\n","\n","    else:\n","        print(\"Error: Model is None. Check the main function.\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ggz-1bwuLgW0"},"outputs":[],"source":["import os\n","import torch\n","from PIL import Image\n","from torchvision import transforms\n","\n","def preprocess_images(image_paths, transform=None):\n","    images = []\n","\n","    for image_path in image_paths:\n","        # Load the image\n","        image = Image.open(image_path).convert('RGB')\n","\n","        # Apply transformations if provided\n","        if transform is not None:\n","            image = transform(image)\n","\n","        images.append(image)\n","\n","    # Ensure all images have three channels\n","    images = [img if img.shape[0] == 3 else img.repeat(3, 1, 1) for img in images]\n","\n","    # Stack the images along the 0th dimension to create a tensor\n","    images = torch.stack(images, 0)\n","\n","    # Placeholder values for report-related information (as they are not used during testing)\n","    image_ids = None\n","    report_ids = None\n","    report_masks = None\n","    seq_lengths = None\n","\n","    return image_ids, images, report_ids, report_masks, seq_lengths\n","\n","\n","# Define the generate_reports_for_images function\n","def generate_reports_for_images(model, image_paths, tokenizer, device='cuda'):\n","    # Load and preprocess the images\n","    transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","    ])\n","\n","    image_ids, images, report_ids, report_masks, seq_lengths = preprocess_images(image_paths, transform)\n","    images = images.to(device)  # Move to the specified device\n","\n","    print(f\"Input images size: {images.size()}\")\n","\n","    # Generate reports\n","    model.eval()\n","    with torch.no_grad():\n","        outputs, _ = model(images, mode='sample')\n","        generated_reports = tokenizer.decode_batch(outputs.cpu().numpy())\n","\n","    return generated_reports\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":387,"referenced_widgets":["6ade2a183b544a45b08950a3dc15613e","afc3e4f55dd24949abbe12633af14dc3","9e8f00f15bd74447aa4f625f96706d5f","cfa2bd3294f543db817545af28dad178","cb3eb04700b743669598d58f5452e1d1","b4690b2bc25c465eaa52fcf656b7a75b","f94da2e5732641d7ab09145180cc5ff2","89605e3a3bc24fa198a8f6cb36945f1f","b4db07d76d764a7582931afa9bbfcb06","2d71ea3884744791849057e239ac7adc","0f60d773abf940adbf969fd0859957e3"]},"executionInfo":{"elapsed":2707,"status":"error","timestamp":1703755325600,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"},"user_tz":-270},"id":"em1BN9VTTQis","outputId":"90f079ef-5f0a-4c6e-ecf7-aaec49989e9a"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6ade2a183b544a45b08950a3dc15613e","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/179M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"RuntimeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-70-55cfd56323f7>\u001b[0m in \u001b[0;36m<cell line: 105>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-70-55cfd56323f7>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# Build model architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaseCMNModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Move model to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-55-6376464c47dc>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, tokenizer)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# self.visual_extractor = VisualExtractor(args)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mlinear_out_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisual_extractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVisualExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_out_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'd_vf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaseCMN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'iu_xray'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-65-2972f59741c6>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, linear_out_features)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Calculate the linear layer's input size dynamically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mdummy_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m196\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mdummy_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_fnt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mlinear_input_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdummy_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 7, 7], expected input[1, 1, 196, 768] to have 3 channels, but got 1 channels instead"]}],"source":["\n","import argparse\n","import numpy as np\n","import torch\n","\n","# from models.models import BaseCMNModel\n","# from modules.dataloaders import R2DataLoader\n","# from modules.loss import compute_loss\n","# from modules.metrics import compute_scores\n","# from modules.tokenizers import Tokenizer\n","# from modules.tester import Tester\n","\n","\n","\n","\n","def main():\n","    args = {\n","         'image_dir': '/content/drive/MyDrive/iu_xray/images',\n","        'ann_path': '/content/drive/MyDrive/iu_xray/annotation.json',\n","        'dataset_name': 'iu_xray',\n","        'max_seq_length': 60,\n","        'threshold': 3,\n","        'num_workers': 2,\n","        'batch_size': 16,\n","        'visual_extractor': 'resnet101',\n","        'visual_extractor_pretrained': True,\n","        'd_model': 512,\n","        'd_ff': 512,\n","        'd_vf': 2048,\n","        'num_heads': 8,\n","        'num_layers': 3,\n","        'dropout': 0.1,\n","        'logit_layers': 1,\n","        'bos_idx': 0,\n","        'eos_idx': 0,\n","        'pad_idx': 0,\n","        'use_bn': 0,\n","        'drop_prob_lm': 0.5,\n","        'topk': 32,\n","        'cmm_size': 2048,\n","        'cmm_dim': 512,\n","        'sample_method': 'beam_search',\n","        'beam_size': 3,\n","        'temperature': 1.0,\n","        'sample_n': 1,\n","        'group_size': 1,\n","        'output_logsoftmax': 1,\n","        'decoding_constraint': 0,\n","        'block_trigrams': 1,\n","        'n_gpu': 1,\n","        'epochs': 100,\n","        'save_dir': 'results/iu_xray',\n","        'record_dir': 'records/',\n","        'log_period': 1000,\n","        'save_period': 1,\n","        'monitor_mode': 'max',\n","        'monitor_metric': 'BLEU_4',\n","        'early_stop': 50,\n","        'optim': 'Adam',\n","        'lr_ve': 5e-5,\n","        'lr_ed': 7e-4,\n","        'weight_decay': 5e-5,\n","        'adam_betas': (0.9, 0.98),\n","        'adam_eps': 1e-9,\n","        'amsgrad': True,\n","        'noamopt_warmup': 5000,\n","        'noamopt_factor': 1,\n","        'lr_scheduler': 'StepLR',\n","        'step_size': 50,\n","        'gamma': 0.1,\n","        'seed': 9233,\n","        'resume': None,\n","        # 'load': \"/content/results/iu_xray/model_best.pth\",\n","        'load': \"/content/drive/MyDrive/CMN_saved_checkpoints/current_checkpoint.pth\",\n","\n","    }\n","\n","     # Create tokenizer\n","    tokenizer = Tokenizer(args)\n","\n","    # Build model architecture\n","    model = BaseCMNModel(args, tokenizer)\n","\n","    model = model.to('cuda')  # Move model to GPU\n","\n","    # Load the trained model checkpoint\n","    checkpoint = torch.load(args['load'], map_location='cuda' if torch.cuda.is_available() else 'cpu')\n","    model.load_state_dict(checkpoint['state_dict'])\n","    model.eval()\n","\n","    # Specify the paths of the two images\n","    image_paths = [\n","        '/content/drive/MyDrive/iu_xray/images/CXR1000_IM-0003/0.png',\n","        '/content/drive/MyDrive/iu_xray/images/CXR1001_IM-0004/1.png'\n","    ]\n","\n","    # Generate reports for the two images\n","    generated_reports = generate_reports_for_images(model, image_paths, tokenizer)\n","\n","    # Print the generated reports\n","    for i, report in enumerate(generated_reports):\n","        print(f\"Generated Report for Image {i + 1}: {report}\")\n","\n","\n","\n","if __name__ == '__main__':\n","    main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZX5XH4RbTQhi"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jZzfEk0oYtP1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SRv_E89-Wbpl"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NQp40uY9UAdC"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"BOr8SKH1JMI2"},"source":["here we generate report"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4053,"status":"ok","timestamp":1701430236471,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"},"user_tz":480},"id":"8rR1Ncy6JLrn","outputId":"9da701c3-7fef-4a3a-b139-3f77495fe96a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Generated Report for Image 1: clips probable probable probable probable infrahilar carina probable infrahilar granulomatous infrahilar carina probable granulomatous probable probable infrahilar granulomatous probable granulomatous probable probable infrahilar granulomatous infrahilar infrahilar infrahilar granulomatous infrahilar 6th projected 6th material 6th cardiopulmonary probable infrahilar right opacity lung demonstrates probable infrahilar infrahilar granulomatous infrahilar granulomatous infrahilar infrahilar infrahilar enlarged lung cardiopulmonary probable suspicious granulomatous infrahilar granulomatous infrahilar carina\n"]}],"source":["import argparse\n","import numpy as np\n","import torch\n","\n","# from models.models import BaseCMNModel\n","# from modules.dataloaders import R2DataLoader\n","# from modules.loss import compute_loss\n","# from modules.metrics import compute_scores\n","# from modules.tokenizers import Tokenizer\n","# from modules.tester import Tester\n","\n","\n","\n","\n","def main():\n","    args = {\n","         'image_dir': '/content/drive/MyDrive/iu_xray/images',\n","        'ann_path': '/content/drive/MyDrive/iu_xray/annotation.json',\n","        'dataset_name': 'iu_xray',\n","        'max_seq_length': 60,\n","        'threshold': 3,\n","        'num_workers': 2,\n","        'batch_size': 16,\n","        'visual_extractor': 'resnet101',\n","        'visual_extractor_pretrained': True,\n","        'd_model': 512,\n","        'd_ff': 512,\n","        'd_vf': 2048,\n","        'num_heads': 8,\n","        'num_layers': 3,\n","        'dropout': 0.1,\n","        'logit_layers': 1,\n","        'bos_idx': 0,\n","        'eos_idx': 0,\n","        'pad_idx': 0,\n","        'use_bn': 0,\n","        'drop_prob_lm': 0.5,\n","        'topk': 32,\n","        'cmm_size': 2048,\n","        'cmm_dim': 512,\n","        'sample_method': 'beam_search',\n","        'beam_size': 3,\n","        'temperature': 1.0,\n","        'sample_n': 1,\n","        'group_size': 1,\n","        'output_logsoftmax': 1,\n","        'decoding_constraint': 0,\n","        'block_trigrams': 1,\n","        'n_gpu': 1,\n","        'epochs': 100,\n","        'save_dir': 'results/iu_xray',\n","        'record_dir': 'records/',\n","        'log_period': 1000,\n","        'save_period': 1,\n","        'monitor_mode': 'max',\n","        'monitor_metric': 'BLEU_4',\n","        'early_stop': 50,\n","        'optim': 'Adam',\n","        'lr_ve': 5e-5,\n","        'lr_ed': 7e-4,\n","        'weight_decay': 5e-5,\n","        'adam_betas': (0.9, 0.98),\n","        'adam_eps': 1e-9,\n","        'amsgrad': True,\n","        'noamopt_warmup': 5000,\n","        'noamopt_factor': 1,\n","        'lr_scheduler': 'StepLR',\n","        'step_size': 50,\n","        'gamma': 0.1,\n","        'seed': 9233,\n","        'resume': None,\n","        # 'load': \"/content/results/iu_xray/model_best.pth\",\n","        'load': \"/content/drive/MyDrive/CMN_saved_checkpoints/current_checkpoint.pth\",\n","\n","    }\n","\n","     # Create tokenizer\n","    tokenizer = Tokenizer(args)\n","\n","    # Build model architecture\n","    model = BaseCMNModel(args, tokenizer)\n","\n","    return model\n","\n","\n","if __name__ == '__main__':\n","\n","\n","    args = {\n","         'image_dir': '/content/drive/MyDrive/iu_xray/images',\n","        'ann_path': '/content/drive/MyDrive/iu_xray/annotation.json',\n","        'dataset_name': 'iu_xray',\n","        'max_seq_length': 60,\n","        'threshold': 3,\n","        'num_workers': 2,\n","        'batch_size': 16,\n","        'visual_extractor': 'resnet101',\n","        'visual_extractor_pretrained': True,\n","        'd_model': 512,\n","        'd_ff': 512,\n","        'd_vf': 2048,\n","        'num_heads': 8,\n","        'num_layers': 3,\n","        'dropout': 0.1,\n","        'logit_layers': 1,\n","        'bos_idx': 0,\n","        'eos_idx': 0,\n","        'pad_idx': 0,\n","        'use_bn': 0,\n","        'drop_prob_lm': 0.5,\n","        'topk': 32,\n","        'cmm_size': 2048,\n","        'cmm_dim': 512,\n","        'sample_method': 'beam_search',\n","        'beam_size': 3,\n","        'temperature': 1.0,\n","        'sample_n': 1,\n","        'group_size': 1,\n","        'output_logsoftmax': 1,\n","        'decoding_constraint': 0,\n","        'block_trigrams': 1,\n","        'n_gpu': 1,\n","        'epochs': 100,\n","        'save_dir': 'results/iu_xray',\n","        'record_dir': 'records/',\n","        'log_period': 1000,\n","        'save_period': 1,\n","        'monitor_mode': 'max',\n","        'monitor_metric': 'BLEU_4',\n","        'early_stop': 50,\n","        'optim': 'Adam',\n","        'lr_ve': 5e-5,\n","        'lr_ed': 7e-4,\n","        'weight_decay': 5e-5,\n","        'adam_betas': (0.9, 0.98),\n","        'adam_eps': 1e-9,\n","        'amsgrad': True,\n","        'noamopt_warmup': 5000,\n","        'noamopt_factor': 1,\n","        'lr_scheduler': 'StepLR',\n","        'step_size': 50,\n","        'gamma': 0.1,\n","        'seed': 9233,\n","        'resume': None,\n","        # 'load': \"/content/results/iu_xray/model_best.pth\",\n","        'load': \"/content/drive/MyDrive/CMN_saved_checkpoints/current_checkpoint.pth\",\n","\n","    }\n","\n","\n","\n","\n","\n","    tokenizer = Tokenizer(args)\n","    main()\n","    model = main()\n","\n","\n","\n","\n","\n","    # Check if the model is not None\n","    if model is not None:\n","        # Move the model to GPU if it's not already there\n","        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","        model = model.to(device)\n","\n","        # Image paths\n","        image_path_1 = '/content/drive/MyDrive/iu_xray/images/CXR1000_IM-0003/0.png'\n","        image_path_2 = '/content/drive/MyDrive/iu_xray/images/CXR1001_IM-0004/1.png'\n","\n","        # ... (rest of your code)\n","\n","        # Move the input image to the same device as the model\n","        image = image.to(device)\n","\n","        # Generate the report\n","        with torch.no_grad():\n","            output, _ = model(image, mode='sample')\n","\n","        generated_report = tokenizer.decode_batch(output.cpu().detach().numpy())[0]\n","\n","        # Print the generated report\n","        print(f\"Generated Report for Image 1: {generated_report}\")\n","\n","    else:\n","        print(\"Error: Model is None. Check the main function.\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"F6Y7G69N2a7u"},"source":["here we print report corresponding an image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9iFymwPr2HOI"},"outputs":[],"source":["import logging\n","import os\n","from abc import abstractmethod\n","\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import spacy\n","import torch\n","from tqdm import tqdm\n","\n","# from modules.utils import generate_heatmap\n","\n","\n","class BaseTester(object):\n","    def __init__(self, model, criterion, metric_ftns, args):\n","        self.args = args\n","\n","        logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n","                            datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n","        self.logger = logging.getLogger(__name__)\n","\n","        # setup GPU device if available, move model into configured device\n","        self.device, device_ids = self._prepare_device(args['n_gpu'])\n","        self.model = model.to(self.device)\n","        if len(device_ids) > 1:\n","            self.model = torch.nn.DataParallel(model, device_ids=device_ids)\n","\n","        self.criterion = criterion\n","        self.metric_ftns = metric_ftns\n","\n","        self.epochs = self.args[\"epochs\"]\n","        self.save_dir = self.args['save_dir']\n","        if not os.path.exists(self.save_dir):\n","            os.makedirs(self.save_dir)\n","\n","        self._load_checkpoint(args['load'])\n","\n","    @abstractmethod\n","    def test(self):\n","        raise NotImplementedError\n","\n","    @abstractmethod\n","    def plot(self):\n","        raise NotImplementedError\n","\n","    def _prepare_device(self, n_gpu_use):\n","        n_gpu = torch.cuda.device_count()\n","        if n_gpu_use > 0 and n_gpu == 0:\n","            self.logger.warning(\n","                \"Warning: There\\'s no GPU available on this machine,\" \"training will be performed on CPU.\")\n","            n_gpu_use = 0\n","        if n_gpu_use > n_gpu:\n","            self.logger.warning(\n","                \"Warning: The number of GPU\\'s configured to use is {}, but only {} are available \" \"on this machine.\".format(\n","                    n_gpu_use, n_gpu))\n","            n_gpu_use = n_gpu\n","        device = torch.device('cuda:0' if n_gpu_use > 0 else 'cpu')\n","        list_ids = list(range(n_gpu_use))\n","        return device, list_ids\n","\n","    def _load_checkpoint(self, load_path):\n","        load_path = str(load_path)\n","        self.logger.info(\"Loading checkpoint: {} ...\".format(load_path))\n","        checkpoint = torch.load(load_path)\n","        self.model.load_state_dict(checkpoint['state_dict'])\n","\n","\n","class Tester(BaseTester):\n","    def __init__(self, model, criterion, metric_ftns, args, test_dataloader):\n","        super(Tester, self).__init__(model, criterion, metric_ftns, args)\n","        self.test_dataloader = test_dataloader\n","\n","    def test(self):\n","        self.logger.info('Start to evaluate in the test set.')\n","        self.model.eval()\n","        log = dict()\n","        with torch.no_grad():\n","            test_gts, test_res = [], []  # here we are storing the reports\n","\n","            print(\"Generating Reports:\")\n","            for batch_idx, (images_id, images, reports_ids, reports_masks) in tqdm(enumerate(self.test_dataloader)):\n","               # Assuming you want to process the first image in the batch\n","               single_image = images[4]  # Change this line to select a different image if needed\n","               single_image = single_image.unsqueeze(0)  # Add batch dimension\n","\n","            # Print image ID\n","            image_id = images_id[4]\n","            print(f\"Image ID: {image_id}\")\n","\n","            print(f\"Image batch size: {images.size()}\")\n","            print(f\"Reports IDs batch size: {reports_ids.size()}\")\n","            print(f\"Reports Masks batch size: {reports_masks.size()}\")\n","\n","            # Process the image and generate the report\n","            images, reports_ids, reports_masks = single_image.to(self.device), reports_ids.to(self.device), reports_masks.to(self.device)\n","            output, _ = self.model(images, mode='sample')\n","            generated_report = self.model.tokenizer.decode_batch(output.cpu().numpy())[0]\n","\n","            # Print the generated report\n","            print(f\"Generated Report: {generated_report}\\n\")\n","\n","            # Additional print statement to show progress\n","            if (batch_idx + 1) % 10 == 0:\n","               print(f\"Processed {batch_idx + 1} batches\")\n","\n","            print(\"Reports Generated Successfully!\")\n","\n","            # ... (rest of the method remains unchanged)\n","\n","            return log\n","\n","\n","    def plot(self):\n","        assert self.args['batch_size'] == 1 and self.args['beam_size'] == 1\n","        self.logger.info('Start to plot attention weights in the test set.')\n","        os.makedirs(os.path.join(self.save_dir, \"attentions\"), exist_ok=True)\n","        os.makedirs(os.path.join(self.save_dir, \"attentions_entities\"), exist_ok=True)\n","        ner = spacy.load(\"en_core_sci_sm\")\n","        mean = torch.tensor((0.485, 0.456, 0.406))\n","        std = torch.tensor((0.229, 0.224, 0.225))\n","        mean = mean[:, None, None]\n","        std = std[:, None, None]\n","\n","        self.model.eval()\n","        with torch.no_grad():\n","            for batch_idx, (images_id, images, reports_ids, reports_masks) in tqdm(enumerate(self.test_dataloader)):\n","                images, reports_ids, reports_masks = images.to(self.device), reports_ids.to(\n","                    self.device), reports_masks.to(self.device)\n","                output, _ = self.model(images, mode='sample')\n","                image = torch.clamp((images[0].cpu() * std + mean) * 255, 0, 255).int().cpu().numpy()\n","                report = self.model.tokenizer.decode_batch(output.cpu().numpy())[0].split()\n","                print(report)\n","\n","                char2word = [idx for word_idx, word in enumerate(report) for idx in [word_idx] * (len(word) + 1)][:-1]\n","\n","                attention_weights = self.model.encoder_decoder.attention_weights[:-1]\n","                assert len(attention_weights) == len(report)\n","                for word_idx, (attns, word) in enumerate(zip(attention_weights, report)):\n","                    for layer_idx, attn in enumerate(attns):\n","                        os.makedirs(os.path.join(self.save_dir, \"attentions\", \"{:04d}\".format(batch_idx),\n","                                                 \"layer_{}\".format(layer_idx)), exist_ok=True)\n","\n","                        heatmap = generate_heatmap(image, attn.mean(1).squeeze())\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":662,"status":"ok","timestamp":1701430245492,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"},"user_tz":480},"id":"jW5jp-I3cSzE","outputId":"6b3072f4-36ca-4e82-ad08-cdb814023755"},"outputs":[{"name":"stdout","output_type":"stream","text":["Image ID 1: 0.png\n","Image ID 2: 1.png\n","Generated Report for Image ID 1: granulomatous infrahilar probable right granulomatous probable infrahilar granulomatous overall granulomatous infrahilar granulomatous azygos 6th granulomatous probable 6th bilaterallythere granulomatous 6th granulomatous infrahilar granulomatous infrahilar 6th infrahilar overlie 6th granulomatous probable right right 6th residual infrahilar skin granulomatous inferior one demonstrates tissues demonstrates demonstrates probable granulomatous probable demonstrates tissues cardiopulmonary granulomatous probable lung residual obscured residual tissues granulomatous infrahilar granulomatous infrahilar\n"]}],"source":["from PIL import Image\n","import os\n","import torch\n","from torchvision import transforms\n","\n","# Load the image\n","image_path_1 = \"/content/drive/MyDrive/iu_xray/images/CXR1728_IM-0479/0.png\"  # Replace with the path to your first image\n","image_path_2 = \"/content/drive/MyDrive/iu_xray/images/CXR1728_IM-0479/1.png\"  # Replace with the path to your second image\n","\n","# Extract image IDs from file paths\n","image_id_1 = os.path.basename(image_path_1)\n","image_id_2 = os.path.basename(image_path_2)\n","\n","image_1 = Image.open(image_path_1).convert('RGB')\n","image_2 = Image.open(image_path_2).convert('RGB')\n","\n","# Preprocess the images\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","])\n","\n","image_1 = transform(image_1)\n","image_2 = transform(image_2)\n","\n","# Stack the images along a new dimension\n","image = torch.stack([image_1, image_2], dim=0)\n","\n","# Add batch dimension\n","image = image.unsqueeze(0)\n","\n","# Move the input image to the same device as the model\n","device = next(model.parameters()).device\n","image = image.to(device)\n","\n","# Print the image IDs\n","print(f\"Image ID 1: {image_id_1}\")\n","print(f\"Image ID 2: {image_id_2}\")\n","\n","# Generate the report\n","with torch.no_grad():\n","    output, _ = model(image, mode='sample')\n","\n","generated_report = model.tokenizer.decode_batch(output.cpu().detach().numpy())[0]\n","\n","# Print the generated report\n","print(f\"Generated Report for Image ID 1: {generated_report}\")\n"]},{"cell_type":"markdown","metadata":{"id":"b5lUkJivxEXT"},"source":["here we are generating report from a single image"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5268,"status":"ok","timestamp":1701430312809,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"},"user_tz":480},"id":"sHMvYqvy2HK0","outputId":"3ab8fb0d-7cd6-4109-cdbc-07700510c710"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["Original Image ID: 1.png\n","Generated Report for Original Image: irregularity granulomatous probable infrahilar cardiopulmonary probable infrahilar granulomatous probable infrahilar segment overall granulomatous inferior granulomatous probable infrahilar limits rib significantly rib cardiopulmonary 6th infrahilar infrahilar granulomatous infrahilar granulomatous infrahilar limits several obscured granulomatous caval both infrahilar granulomatous demonstrates elevation granulomatous demonstrates tissues demonstrates overall overlie granulomatous infrahilar pneumothoraces limits granulomatous probable lung typical fractures 6th carina demonstrates tissues 6th infrahilar\n"]}],"source":["\n","import argparse\n","import numpy as np\n","import torch\n","from PIL import Image\n","from torchvision import transforms\n","\n","# from models.models import BaseCMNModel\n","# from modules.dataloaders import R2DataLoader\n","# from modules.loss import compute_loss\n","# from modules.metrics import compute_scores\n","# from modules.tokenizers import Tokenizer\n","# from modules.tester import Tester\n","\n","def main():\n","    args = {\n","         'image_dir': '/content/drive/MyDrive/iu_xray/images',\n","        'ann_path': '/content/drive/MyDrive/iu_xray/annotation.json',\n","        'dataset_name': 'iu_xray',\n","        'max_seq_length': 60,\n","        'threshold': 3,\n","        'num_workers': 2,\n","        'batch_size': 16,\n","        'visual_extractor': 'resnet101',\n","        'visual_extractor_pretrained': True,\n","        'd_model': 512,\n","        'd_ff': 512,\n","        'd_vf': 2048,\n","        'num_heads': 8,\n","        'num_layers': 3,\n","        'dropout': 0.1,\n","        'logit_layers': 1,\n","        'bos_idx': 0,\n","        'eos_idx': 0,\n","        'pad_idx': 0,\n","        'use_bn': 0,\n","        'drop_prob_lm': 0.5,\n","        'topk': 32,\n","        'cmm_size': 2048,\n","        'cmm_dim': 512,\n","        'sample_method': 'beam_search',\n","        'beam_size': 3,\n","        'temperature': 1.0,\n","        'sample_n': 1,\n","        'group_size': 1,\n","        'output_logsoftmax': 1,\n","        'decoding_constraint': 0,\n","        'block_trigrams': 1,\n","        'n_gpu': 1,\n","        'epochs': 100,\n","        'save_dir': 'results/iu_xray',\n","        'record_dir': 'records/',\n","        'log_period': 1000,\n","        'save_period': 1,\n","        'monitor_mode': 'max',\n","        'monitor_metric': 'BLEU_4',\n","        'early_stop': 50,\n","        'optim': 'Adam',\n","        'lr_ve': 5e-5,\n","        'lr_ed': 7e-4,\n","        'weight_decay': 5e-5,\n","        'adam_betas': (0.9, 0.98),\n","        'adam_eps': 1e-9,\n","        'amsgrad': True,\n","        'noamopt_warmup': 5000,\n","        'noamopt_factor': 1,\n","        'lr_scheduler': 'StepLR',\n","        'step_size': 50,\n","        'gamma': 0.1,\n","        'seed': 9233,\n","        'resume': None,\n","        # 'load': \"/content/results/iu_xray/model_best.pth\",\n","        'load': \"/content/drive/MyDrive/CMN_saved_checkpoints/current_checkpoint.pth\",\n","\n","    }\n","\n","    # Fix random seeds\n","    torch.manual_seed(args['seed'])\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(args['seed'])\n","\n","    # Create tokenizer\n","    tokenizer = Tokenizer(args)\n","\n","    # Create data loader\n","    test_dataloader = R2DataLoader(args, tokenizer, split='test', shuffle=False)\n","\n","    # Build model architecture\n","    model = BaseCMNModel(args, tokenizer)\n","\n","\n","if __name__ == '__main__':\n","    main()\n","\n","from PIL import Image\n","import os\n","import shutil\n","import torch\n","from torchvision import transforms\n","\n","# Original image path\n","original_image_path = \"/content/drive/MyDrive/iu_xray/images/CXR1001_IM-0004/1.png\"  # Replace with the path to your image\n","\n","# Specify the directory where you want to create a copy\n","copy_directory = \"/content/drive/MyDrive/iu_xray/images/CXR1728_IM-0479/\"\n","copy_image_path = os.path.join(copy_directory, \"0_copy.png\")\n","\n","# Make a copy of the image\n","shutil.copy(original_image_path, copy_image_path)\n","\n","# Extract image IDs from file paths\n","original_image_id = os.path.basename(original_image_path)\n","copy_image_id = os.path.basename(copy_image_path)\n","\n","# Open the images and convert to RGB\n","original_image = Image.open(original_image_path).convert('RGB')\n","copy_image = Image.open(copy_image_path).convert('RGB')\n","\n","# Preprocess the images\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","])\n","\n","original_image = transform(original_image)\n","copy_image = transform(copy_image)\n","\n","# Stack the images along a new dimension\n","image = torch.stack([original_image, original_image], dim=0)\n","\n","# Add batch dimension\n","image = image.unsqueeze(0)\n","\n","# Move the input image to the same device as the model\n","device = next(model.parameters()).device\n","image = image.to(device)\n","\n","# Print the image IDs\n","print(f\"Original Image ID: {original_image_id}\")\n","# print(f\"Copy Image ID: {copy_image_id}\")\n","\n","# Generate the report\n","with torch.no_grad():\n","    output, _ = model(image, mode='sample')\n","\n","generated_report = model.tokenizer.decode_batch(output.cpu().detach().numpy())[0]\n","\n","# Print the generated report\n","print(f\"Generated Report for Original Image: {generated_report}\")\n"]},{"cell_type":"markdown","metadata":{"id":"qdRyU5Z42HIn"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2CykrsmUMjUU"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HxgrlgkiMjQ2"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"-RrBbpvRO4bO"},"source":["here we generate report from two images"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":442},"executionInfo":{"elapsed":2639,"status":"error","timestamp":1701428113762,"user":{"displayName":"Sana Ullah","userId":"07281496503381877384"},"user_tz":480},"id":"WmWWrEu5MjOn","outputId":"70967198-da19-4bf8-baf3-676723db74fa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Image ID 1: 0.png\n","Image ID 2: 1.png\n"]},{"ename":"RuntimeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-57-56d8003ce5d9>\u001b[0m in \u001b[0;36m<cell line: 139>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-57-56d8003ce5d9>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;31m# Generate the report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sample'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0mgenerated_report\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-8c21a6ba22fb>\u001b[0m in \u001b[0;36mforward_iu_xray\u001b[0;34m(self, images, targets, mode, update_opts)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sample'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sample'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_opts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_probs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-a29ebe2fcc7b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'mode'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mode'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbeam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_logprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-c6f4dc6544d1>\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, fc_feats, att_feats, att_masks, update_opts)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mblock_trigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'block_trigrams'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbeam_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msample_method\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'greedy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'beam_search'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample_beam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgroup_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_diverse_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-c6f4dc6544d1>\u001b[0m in \u001b[0;36m_sample_beam\u001b[0;34m(self, fc_feats, att_feats, att_masks, opt)\u001b[0m\n\u001b[1;32m    124\u001b[0m                                                                                    pp_att_feats, p_att_masks]\n\u001b[1;32m    125\u001b[0m                                                                                   )\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone_beams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_fc_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_att_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpp_att_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_att_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msample_n\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-a29ebe2fcc7b>\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, init_state, init_logprobs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeam_seq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdivm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdivm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                     logprobs_table[divm], state_table[divm] = self.get_logprobs_state(it.cuda(), *(\n\u001b[0m\u001b[1;32m    197\u001b[0m                             args[divm] + [state_table[divm]]))\n\u001b[1;32m    198\u001b[0m                     \u001b[0mlogprobs_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdivm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogprobs_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdivm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-c6f4dc6544d1>\u001b[0m in \u001b[0;36mget_logprobs_state\u001b[0;34m(self, it, fc_feats, att_feats, p_att_feats, att_masks, state, output_logsoftmax)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_att_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_logsoftmax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mlogprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-7ab7b1822b09>\u001b[0m in \u001b[0;36mcore\u001b[0;34m(self, it, fc_feats_ph, att_feats_ph, memory, state, mask)\u001b[0m\n\u001b[1;32m    394\u001b[0m                     fc_feats_ph.new_zeros(self.num_layers * 2, fc_feats_ph.shape[0], 0, self.d_model)]\n\u001b[1;32m    395\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m             \u001b[0mpast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         out, past = self.model.decode(memory, mask, ys, subsequent_mask(ys.size(1)).to(memory.device), past=past,\n","\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument tensors in method wrapper_CUDA_cat)"]}],"source":["\n","import argparse\n","import numpy as np\n","import torch\n","from PIL import Image\n","from torchvision import transforms\n","import os\n","\n","# from models.models import BaseCMNModel\n","# from modules.dataloaders import R2DataLoader\n","# from modules.loss import compute_loss\n","# from modules.metrics import compute_scores\n","# from modules.tokenizers import Tokenizer\n","# from modules.tester import Tester\n","\n","def main():\n","    args = {\n","         'image_dir': '/content/drive/MyDrive/iu_xray/images',\n","        'ann_path': '/content/drive/MyDrive/iu_xray/annotation.json',\n","        'dataset_name': 'iu_xray',\n","        'max_seq_length': 60,\n","        'threshold': 3,\n","        'num_workers': 2,\n","        'batch_size': 16,\n","        'visual_extractor': 'resnet101',\n","        'visual_extractor_pretrained': True,\n","        'd_model': 512,\n","        'd_ff': 512,\n","        'd_vf': 2048,\n","        'num_heads': 8,\n","        'num_layers': 3,\n","        'dropout': 0.1,\n","        'logit_layers': 1,\n","        'bos_idx': 0,\n","        'eos_idx': 0,\n","        'pad_idx': 0,\n","        'use_bn': 0,\n","        'drop_prob_lm': 0.5,\n","        'topk': 32,\n","        'cmm_size': 2048,\n","        'cmm_dim': 512,\n","        'sample_method': 'beam_search',\n","        'beam_size': 3,\n","        'temperature': 1.0,\n","        'sample_n': 1,\n","        'group_size': 1,\n","        'output_logsoftmax': 1,\n","        'decoding_constraint': 0,\n","        'block_trigrams': 1,\n","        'n_gpu': 1,\n","        'epochs': 100,\n","        'save_dir': 'results/iu_xray',\n","        'record_dir': 'records/',\n","        'log_period': 1000,\n","        'save_period': 1,\n","        'monitor_mode': 'max',\n","        'monitor_metric': 'BLEU_4',\n","        'early_stop': 50,\n","        'optim': 'Adam',\n","        'lr_ve': 5e-5,\n","        'lr_ed': 7e-4,\n","        'weight_decay': 5e-5,\n","        'adam_betas': (0.9, 0.98),\n","        'adam_eps': 1e-9,\n","        'amsgrad': True,\n","        'noamopt_warmup': 5000,\n","        'noamopt_factor': 1,\n","        'lr_scheduler': 'StepLR',\n","        'step_size': 50,\n","        'gamma': 0.1,\n","        'seed': 9233,\n","        'resume': None,\n","        # 'load': \"/content/results/iu_xray/model_best.pth\",\n","        'load': \"/content/drive/MyDrive/CMN_saved_checkpoints/current_checkpoint.pth\",\n","\n","    }\n","\n","    # Fix random seeds\n","    torch.manual_seed(args['seed'])\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(args['seed'])\n","\n","    # Create tokenizer\n","    tokenizer = Tokenizer(args)\n","\n","    # Create data loader\n","    # test_dataloader = R2DataLoader(args, tokenizer, split='test', shuffle=False)\n","\n","    # Build model architecture\n","    model = BaseCMNModel(args, tokenizer)\n","\n","\n","\n","    # Load the image\n","    image_path_1 = \"/content/drive/MyDrive/iu_xray/images/CXR1728_IM-0479/0.png\"  # Replace with the path to your first image\n","    image_path_2 = \"/content/drive/MyDrive/iu_xray/images/CXR1728_IM-0479/1.png\"  # Replace with the path to your second image\n","\n","    # Extract image IDs from file paths\n","    image_id_1 = os.path.basename(image_path_1)\n","    image_id_2 = os.path.basename(image_path_2)\n","\n","    image_1 = Image.open(image_path_1).convert('RGB')\n","    image_2 = Image.open(image_path_2).convert('RGB')\n","\n","    # Preprocess the images\n","    transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","    ])\n","\n","    image_1 = transform(image_1)\n","    image_2 = transform(image_2)\n","\n","    # Stack the images along a new dimension\n","    image = torch.stack([image_1, image_2], dim=0)\n","\n","    # Add batch dimension\n","    image = image.unsqueeze(0)\n","\n","    # Move the input image to the same device as the model\n","    device = next(model.parameters()).device\n","    image = image.to(device)\n","\n","    # Print the image IDs\n","    print(f\"Image ID 1: {image_id_1}\")\n","    print(f\"Image ID 2: {image_id_2}\")\n","\n","    # Generate the report\n","    with torch.no_grad():\n","        output, _ = model(image, mode='sample')\n","\n","    generated_report = model.tokenizer.decode_batch(output.cpu().detach().numpy())[0]\n","\n","    # Print the generated report\n","    print(f\"Generated Report for Image ID 1: {generated_report}\")\n","\n","\n","if __name__ == '__main__':\n","    main()\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EVAC5PZBMjD2"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xJ85E90R2HDg"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t6eboeLm2HBE"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"oBKdgqu32KCn"},"source":["here we check the pre-trained model accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h6LjKhH_YtJB"},"outputs":[],"source":["#tester.py\n","import logging\n","import os\n","from abc import abstractmethod\n","\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import spacy\n","import torch\n","from tqdm import tqdm\n","\n","\n","\n","\n","class BaseTester(object):\n","    def __init__(self, model, criterion, metric_ftns, args):\n","        self.args = args\n","\n","        logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n","                            datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n","        self.logger = logging.getLogger(__name__)\n","\n","        # setup GPU device if available, move model into configured device\n","        self.device, device_ids = self._prepare_device(args['n_gpu'])\n","        self.model = model.to(self.device)\n","        if len(device_ids) > 1:\n","            self.model = torch.nn.DataParallel(model, device_ids=device_ids)\n","\n","        self.criterion = criterion\n","        self.metric_ftns = metric_ftns\n","\n","        self.epochs = self.args['epochs']\n","        self.save_dir = self.args['save_dir']\n","        if not os.path.exists(self.save_dir):\n","            os.makedirs(self.save_dir)\n","\n","        self._load_checkpoint(args['load'])\n","\n","    @abstractmethod\n","    def test(self):\n","        raise NotImplementedError\n","\n","    @abstractmethod\n","    def plot(self):\n","        raise NotImplementedError\n","\n","    def _prepare_device(self, n_gpu_use):\n","        n_gpu = torch.cuda.device_count()\n","        if n_gpu_use > 0 and n_gpu == 0:\n","            self.logger.warning(\n","                \"Warning: There\\'s no GPU available on this machine,\" \"training will be performed on CPU.\")\n","            n_gpu_use = 0\n","        if n_gpu_use > n_gpu:\n","            self.logger.warning(\n","                \"Warning: The number of GPU\\'s configured to use is {}, but only {} are available \" \"on this machine.\".format(\n","                    n_gpu_use, n_gpu))\n","            n_gpu_use = n_gpu\n","        device = torch.device('cuda:0' if n_gpu_use > 0 else 'cpu')\n","        list_ids = list(range(n_gpu_use))\n","        return device, list_ids\n","\n","    def _load_checkpoint(self, load_path):\n","        load_path = str(load_path)\n","        self.logger.info(\"Loading checkpoint: {} ...\".format(load_path))\n","        checkpoint = torch.load(load_path)\n","        self.model.load_state_dict(checkpoint['state_dict'])\n","\n","\n","class Tester(BaseTester):\n","    def __init__(self, model, criterion, metric_ftns, args, test_dataloader):\n","        super(Tester, self).__init__(model, criterion, metric_ftns, args)\n","        self.test_dataloader = test_dataloader\n","\n","    def test(self):\n","        self.logger.info('Start to evaluate in the test set.')\n","        self.model.eval()\n","        log = dict()\n","        with torch.no_grad():\n","            test_gts, test_res = [], []\n","            for batch_idx, (images_id, images, reports_ids, reports_masks) in tqdm(enumerate(self.test_dataloader)):\n","                images, reports_ids, reports_masks = images.to(self.device), reports_ids.to(\n","                    self.device), reports_masks.to(self.device)\n","                output, _ = self.model(images, mode='sample')\n","                reports = self.model.tokenizer.decode_batch(output.cpu().numpy())\n","                ground_truths = self.model.tokenizer.decode_batch(reports_ids[:, 1:].cpu().numpy())\n","                test_res.extend(reports)\n","                test_gts.extend(ground_truths)\n","\n","            test_met = self.metric_ftns({i: [gt] for i, gt in enumerate(test_gts)},\n","                                        {i: [re] for i, re in enumerate(test_res)})\n","            log.update(**{'test_' + k: v for k, v in test_met.items()})\n","            print(log)\n","\n","            test_res, test_gts = pd.DataFrame(test_res), pd.DataFrame(test_gts)\n","            test_res.to_csv(os.path.join(self.save_dir, \"res.csv\"), index=False, header=False)\n","            test_gts.to_csv(os.path.join(self.save_dir, \"gts.csv\"), index=False, header=False)\n","\n","        return log\n","\n","    def plot(self):\n","        assert self.args['batch_size'] == 1 and self.args['beam_size'] == 1\n","        self.logger.info('Start to plot attention weights in the test set.')\n","        os.makedirs(os.path.join(self.save_dir, \"attentions\"), exist_ok=True)\n","        os.makedirs(os.path.join(self.save_dir, \"attentions_entities\"), exist_ok=True)\n","        ner = spacy.load(\"en_core_sci_sm\")\n","        mean = torch.tensor((0.485, 0.456, 0.406))\n","        std = torch.tensor((0.229, 0.224, 0.225))\n","        mean = mean[:, None, None]\n","        std = std[:, None, None]\n","\n","        self.model.eval()\n","        with torch.no_grad():\n","            for batch_idx, (images_id, images, reports_ids, reports_masks) in tqdm(enumerate(self.test_dataloader)):\n","                images, reports_ids, reports_masks = images.to(self.device), reports_ids.to(\n","                    self.device), reports_masks.to(self.device)\n","                output, _ = self.model(images, mode='sample')\n","                image = torch.clamp((images[0].cpu() * std + mean) * 255, 0, 255).int().cpu().numpy()\n","                report = self.model.tokenizer.decode_batch(output.cpu().numpy())[0].split()\n","\n","                char2word = [idx for word_idx, word in enumerate(report) for idx in [word_idx] * (len(word) + 1)][:-1]\n","\n","                attention_weights = self.model.encoder_decoder.attention_weights[:-1]\n","                assert len(attention_weights) == len(report)\n","                for word_idx, (attns, word) in enumerate(zip(attention_weights, report)):\n","                    for layer_idx, attn in enumerate(attns):\n","                        os.makedirs(os.path.join(self.save_dir, \"attentions\", \"{:04d}\".format(batch_idx),\n","                                                 \"layer_{}\".format(layer_idx)), exist_ok=True)\n","\n","                        heatmap = generate_heatmap(image, attn.mean(1).squeeze())\n","                        cv2.imwrite(os.path.join(self.save_dir, \"attentions\", \"{:04d}\".format(batch_idx),\n","                                                 \"layer_{}\".format(layer_idx), \"{:04d}_{}.png\".format(word_idx, word)),\n","                                    heatmap)\n","\n","                for ne_idx, ne in enumerate(ner(\" \".join(report)).ents):\n","                    for layer_idx in range(len(attention_weights[0])):\n","                        os.makedirs(os.path.join(self.save_dir, \"attentions_entities\", \"{:04d}\".format(batch_idx),\n","                                                 \"layer_{}\".format(layer_idx)), exist_ok=True)\n","                        attn = [attns[layer_idx] for attns in\n","                                attention_weights[char2word[ne.start_char]:char2word[ne.end_char] + 1]]\n","                        attn = np.concatenate(attn, axis=2)\n","                        heatmap = generate_heatmap(image, attn.mean(1).mean(1).squeeze())\n","                        cv2.imwrite(os.path.join(self.save_dir, \"attentions_entities\", \"{:04d}\".format(batch_idx),\n","                                                 \"layer_{}\".format(layer_idx), \"{:04d}_{}.png\".format(ne_idx, ne)),\n","                                    heatmap)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7sFP52LleM4v"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"V7imebO8TQeY","outputId":"f5044693-82d4-4bfe-a71d-98e5618d7209"},"outputs":[{"name":"stdout","output_type":"stream","text":["bellow the visual extractor\n","Visual output size: 2048\n","linear_input_size: 2048\n","bellow the linear layer\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:__main__:Warning: There's no GPU available on this machine,training will be performed on CPU.\n"]},{"name":"stdout","output_type":"stream","text":["in basecmnmodel bellow the encode_decoder\n","in basecmnmodel iu xray forward pass\n"]},{"ename":"RuntimeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-1dc567433e76>\u001b[0m in \u001b[0;36m<cell line: 92>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-28-1dc567433e76>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;31m# build trainer and start to train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mtester\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTester\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mtester\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-26-94045ce00a15>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, criterion, metric_ftns, args, test_dataloader)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTester\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseTester\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_ftns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTester\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_ftns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-26-94045ce00a15>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, criterion, metric_ftns, args)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'load'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-26-94045ce00a15>\u001b[0m in \u001b[0;36m_load_checkpoint\u001b[0;34m(self, load_path)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mload_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading checkpoint: {} ...\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1012\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m                 return _load(opened_zipfile,\n\u001b[0m\u001b[1;32m   1015\u001b[0m                              \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m                              \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1422\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1390\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m             \u001b[0mtyped_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtyped_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1364\u001b[0m         \u001b[0;31m# stop wrapping with TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m         typed_storage = torch.storage.TypedStorage(\n\u001b[0;32m-> 1366\u001b[0;31m             \u001b[0mwrap_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1367\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m             _internal=True)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cuda_deserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_load_uninitialized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[1;32m    259\u001b[0m                            \u001b[0;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                            \u001b[0;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."]}],"source":["import argparse\n","import numpy as np\n","import torch\n","\n","\n","def main():\n","    args = {\n","        'image_dir': '/content/drive/MyDrive/iu_xray/images',\n","        'ann_path': '/content/drive/MyDrive/iu_xray/annotation.json',\n","        'dataset_name': 'iu_xray',\n","        'max_seq_length': 60,\n","        'threshold': 3,\n","        'num_workers': 2,\n","        'batch_size': 16,\n","        'visual_extractor': 'resnest200e',\n","        'visual_extractor_pretrained': True,\n","        'd_model': 512,\n","        'd_ff': 512,\n","        'd_vf': 2048,\n","        'num_heads': 8,\n","        'num_layers': 3,\n","        'dropout': 0.1,\n","        'logit_layers': 1,\n","        'bos_idx': 0,\n","        'eos_idx': 0,\n","        'pad_idx': 0,\n","        'use_bn': 0,\n","        'drop_prob_lm': 0.5,\n","        'topk': 32,\n","        'cmm_size': 2048,\n","        'cmm_dim': 512,\n","        'sample_method': 'beam_search',\n","        'beam_size': 3,\n","        'temperature': 1.0,\n","        'sample_n': 1,\n","        'group_size': 1,\n","        'output_logsoftmax': 1,\n","        'decoding_constraint': 0,\n","        'block_trigrams': 1,\n","        'n_gpu': 1,\n","        'epochs': 100,\n","        'save_dir': 'results/iu_xray',\n","        'record_dir': 'records/',\n","        'log_period': 1000,\n","        'save_period': 1,\n","        'monitor_mode': 'max',\n","        'monitor_metric': 'BLEU_4',\n","        'early_stop': 50,\n","        'optim': 'Adam',\n","        'lr_ve': 5e-5,\n","        'lr_ed': 7e-4,\n","        'weight_decay': 5e-5,\n","        'adam_betas': (0.9, 0.98),\n","        'adam_eps': 1e-9,\n","        'amsgrad': True,\n","        'noamopt_warmup': 5000,\n","        'noamopt_factor': 1,\n","        'lr_scheduler': 'StepLR',\n","        'step_size': 50,\n","        'gamma': 0.1,\n","        'seed': 9233,\n","        'resume': None,\n","        # 'load': \"/content/results/iu_xray/model_best.pth\",\n","        'load': \"/content/drive/MyDrive/resnest200e_save_checkpoints/model_best.pth\",\n","\n","    }\n","\n","    # fix random seeds\n","    torch.manual_seed(args['seed'])\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(args['seed'])\n","\n","    # create tokenizer\n","    tokenizer = Tokenizer(args)\n","\n","    # create data loader\n","    test_dataloader = R2DataLoader(args, tokenizer, split='test', shuffle=False)\n","\n","    # build model architecture\n","    model = BaseCMNModel(args, tokenizer)\n","\n","    # get function handles of loss and metrics\n","    criterion = compute_loss\n","    metrics = compute_scores\n","\n","    # build trainer and start to train\n","    tester = Tester(model, criterion, metrics, args, test_dataloader)\n","    tester.test()\n","\n","\n","if __name__ == '__main__':\n","    main()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u475_yb7TQdK"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0f60d773abf940adbf969fd0859957e3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"14d970793a7246ebaaab97491994bc90":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aefbe6b33d7247419ce1ab51d923c9cd","IPY_MODEL_65a2cd97451e4c87b655b93c4fcad222","IPY_MODEL_1735b1820a8f4621a47c5a26c79b346e"],"layout":"IPY_MODEL_22dbb0b57ebc491ca64a8d4e87d1b034"}},"1735b1820a8f4621a47c5a26c79b346e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b9d4d5966fb47d9b35df3aa0d525f5f","placeholder":"","style":"IPY_MODEL_de73920a77fa4a3aa4ee0a1aca7635b2","value":" 241M/241M [00:03&lt;00:00, 52.5MB/s]"}},"22dbb0b57ebc491ca64a8d4e87d1b034":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d71ea3884744791849057e239ac7adc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65a2cd97451e4c87b655b93c4fcad222":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f26e5d73b82a4661bfe0b69fee203b53","max":241463512,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f7f7fe6f14604b86b16a975a9cfebae8","value":241463512}},"6ade2a183b544a45b08950a3dc15613e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_afc3e4f55dd24949abbe12633af14dc3","IPY_MODEL_9e8f00f15bd74447aa4f625f96706d5f","IPY_MODEL_cfa2bd3294f543db817545af28dad178"],"layout":"IPY_MODEL_cb3eb04700b743669598d58f5452e1d1"}},"7b9d4d5966fb47d9b35df3aa0d525f5f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89605e3a3bc24fa198a8f6cb36945f1f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e8f00f15bd74447aa4f625f96706d5f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_89605e3a3bc24fa198a8f6cb36945f1f","max":178675806,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b4db07d76d764a7582931afa9bbfcb06","value":178675806}},"aefbe6b33d7247419ce1ab51d923c9cd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b2f0cbae28bf4016b3f7db90511be363","placeholder":"","style":"IPY_MODEL_ece427073d3c4a11a48637db780b6bfa","value":"model.safetensors: 100%"}},"afc3e4f55dd24949abbe12633af14dc3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4690b2bc25c465eaa52fcf656b7a75b","placeholder":"","style":"IPY_MODEL_f94da2e5732641d7ab09145180cc5ff2","value":"model.safetensors: 100%"}},"b2f0cbae28bf4016b3f7db90511be363":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4690b2bc25c465eaa52fcf656b7a75b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4db07d76d764a7582931afa9bbfcb06":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cb3eb04700b743669598d58f5452e1d1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cfa2bd3294f543db817545af28dad178":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d71ea3884744791849057e239ac7adc","placeholder":"","style":"IPY_MODEL_0f60d773abf940adbf969fd0859957e3","value":" 179M/179M [00:00&lt;00:00, 184MB/s]"}},"de73920a77fa4a3aa4ee0a1aca7635b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ece427073d3c4a11a48637db780b6bfa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f26e5d73b82a4661bfe0b69fee203b53":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7f7fe6f14604b86b16a975a9cfebae8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f94da2e5732641d7ab09145180cc5ff2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}